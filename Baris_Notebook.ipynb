{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM and How Adverserial Examples affects its Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of this Notebook: <br>\n",
    "#### 1. Import of Python libraries and the dataset<br>\n",
    "#### 2. Tuning of the Hyperparameters of the Mini-Batch Optimiser<br>\n",
    "#### 3. Training a Naive Model<br>\n",
    "#### 4. Evaluate the Attack against the Naive Model<br>\n",
    "#### 5. Training a Robust Model <br>\n",
    "#### 6. Evaluate the Attack against the Robust Model <br>\n",
    "#### 7. Comparision of the two performances <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin by importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KUTAHYA PORSELEN\n",
    "# Optimisation Algorithms in Non-Convex Classification Problems with Adversarial Training\n",
    "import json\n",
    "from adversary import attack, protect\n",
    "from net import Net\n",
    "import numpy as np\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "from training import training, testing, accuracy, tune_optimizer\n",
    "from adam import AdamOptimizer\n",
    "from adam_run import adam_run, adam_tune, adam_build\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import get_mnist, build_data_loaders\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND BEST PARAMETERS FOR ADAM\n",
    "def find_best_params_adam(adam_tuning):\n",
    "    best_params = dict()\n",
    "    best_params['loss_test'] = float('inf')\n",
    "    best_params['acc_test'] = -float('inf')\n",
    "\n",
    "    for item in adam_tuning:\n",
    "        if best_params['acc_test'] < item['metric_test'] or best_params['loss_test'] > item['loss_test']:\n",
    "            best_params['loss_train'] = item['loss_train']\n",
    "            best_params['acc_train'] = item['metric_train']\n",
    "            best_params['acc_test'] = item['metric_test']\n",
    "            best_params['loss_test'] = item['loss_test']\n",
    "            best_params['lr'] = item['lr']\n",
    "            best_params['beta1'] = item['beta1']\n",
    "            best_params['beta2'] = item['beta2']\n",
    "            best_params['weight_decay'] = item['weight_decay']\n",
    "            best_params['epsilon'] = item['epsilon']\n",
    "            \n",
    "    return best_params\n",
    "\n",
    "#PRINT BEST PARAMETER SET OF ADAM AS WELL AS LOSS AND ACCURACY VALUES\n",
    "def print_adam_stats(best_adam_params):\n",
    "    print(\"Mean train loss value:\",sum(best_adam_params['loss_train'])/len(best_adam_params['loss_train']))\n",
    "    print(\"Lowest test loss value:\",best_adam_params['loss_test'])\n",
    "    print(\"Mean train accuracy value: {:.2f}%\".format(100 * sum(best_adam_params['acc_train'])/len(best_adam_params['acc_train'])))\n",
    "    print(\"Best test accuracy value: {:.2f}%\".format(100 * best_adam_params['acc_test']))\n",
    "    print(\"Optimal Learning rate: \",best_adam_params['lr'])\n",
    "    print(\"Optimal Beta1: \",best_adam_params['beta1'])\n",
    "    print(\"Optimal Beta2: \",best_adam_params['beta2'])\n",
    "    print(\"Optimal Weight Decay: \",best_adam_params['weight_decay'])\n",
    "    print(\"Optimal Epsilon: \",best_adam_params['epsilon'])\n",
    "    \n",
    "def get_device():\n",
    "    device = torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        use_cuda = input(\"CUDA is available, would you like to switch to cuda?(Y/N)\")\n",
    "        if use_cuda == 'Y' or use_cuda == 'y':\n",
    "            print(\"Device is set to CUDA!\")\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            print(\"Device is set to CPU!\")\n",
    "            device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        print(\"Device is set to CPU!\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train loss value: 0.08408223207314121\n",
      "Lowest test loss value: 0.03568430640809596\n",
      "Mean train accuracy value: 97.58%\n",
      "Best test accuracy value: 99.16%\n",
      "Optimal Learning rate:  0.001\n",
      "Optimal Beta1:  0.1\n",
      "Optimal Beta2:  0.999\n",
      "Optimal Weight Decay:  0.01\n",
      "Optimal Epsilon:  1e-09\n"
     ]
    }
   ],
   "source": [
    "#Best parameters\n",
    "file = open('adam_tuning.json',)\n",
    "adam_tuning = json.load(file)\n",
    "best_adam_params = find_best_params_adam(adam_tuning)\n",
    "print_adam_stats(best_adam_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready data and choose device(CPU/GPU)(GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_mnist(normalize=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders with requested batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to CPU!\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "net_naive = Net().to(device)\n",
    "train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on cpu\n",
      "batch 100\tloss = 1.293\tacc = 0.7266\n",
      "batch 200\tloss = 0.3408\tacc = 0.8906\n",
      "batch 300\tloss = 0.2902\tacc = 0.9062\n",
      "batch 400\tloss = 0.0969\tacc = 0.9688\n",
      "epoch 0\tavg epoch loss = 0.6588\tavg epoch acc = 0.8335\n",
      "batch 100\tloss = 0.135\tacc = 0.9609\n",
      "batch 200\tloss = 0.1628\tacc = 0.9766\n",
      "batch 300\tloss = 0.07345\tacc = 0.9844\n",
      "batch 400\tloss = 0.01535\tacc = 1.0\n",
      "epoch 1\tavg epoch loss = 0.09639\tavg epoch acc = 0.9702\n",
      "batch 100\tloss = 0.07025\tacc = 0.9688\n",
      "batch 200\tloss = 0.119\tacc = 0.9688\n",
      "batch 300\tloss = 0.04029\tacc = 0.9922\n",
      "batch 400\tloss = 0.005238\tacc = 1.0\n",
      "epoch 2\tavg epoch loss = 0.0565\tavg epoch acc = 0.9819\n",
      "batch 100\tloss = 0.01636\tacc = 0.9922\n",
      "batch 200\tloss = 0.08295\tacc = 0.9844\n",
      "batch 300\tloss = 0.04021\tacc = 0.9844\n",
      "batch 400\tloss = 0.004928\tacc = 1.0\n",
      "epoch 3\tavg epoch loss = 0.04078\tavg epoch acc = 0.9873\n",
      "batch 100\tloss = 0.007227\tacc = 1.0\n",
      "batch 200\tloss = 0.07279\tacc = 0.9844\n",
      "batch 300\tloss = 0.02256\tacc = 0.9922\n",
      "batch 400\tloss = 0.002037\tacc = 1.0\n",
      "epoch 4\tavg epoch loss = 0.03757\tavg epoch acc = 0.9891\n",
      "batch 100\tloss = 0.01658\tacc = 0.9922\n",
      "batch 200\tloss = 0.05536\tacc = 0.9844\n",
      "batch 300\tloss = 0.03421\tacc = 0.9922\n",
      "batch 400\tloss = 0.0003919\tacc = 1.0\n",
      "epoch 5\tavg epoch loss = 0.0254\tavg epoch acc = 0.9922\n",
      "batch 100\tloss = 0.02341\tacc = 0.9922\n",
      "batch 200\tloss = 0.03518\tacc = 0.9922\n",
      "batch 300\tloss = 0.04858\tacc = 0.9844\n",
      "batch 400\tloss = 0.001299\tacc = 1.0\n",
      "epoch 6\tavg epoch loss = 0.0487\tavg epoch acc = 0.9875\n",
      "batch 100\tloss = 0.00109\tacc = 1.0\n",
      "batch 200\tloss = 0.06897\tacc = 0.9844\n",
      "batch 300\tloss = 0.03065\tacc = 0.9922\n",
      "batch 400\tloss = 0.0002594\tacc = 1.0\n",
      "epoch 7\tavg epoch loss = 0.01882\tavg epoch acc = 0.9941\n",
      "batch 100\tloss = 0.02395\tacc = 0.9922\n",
      "batch 200\tloss = 0.02416\tacc = 0.9922\n",
      "batch 300\tloss = 0.0144\tacc = 0.9922\n",
      "batch 400\tloss = 0.0003786\tacc = 1.0\n",
      "epoch 8\tavg epoch loss = 0.01554\tavg epoch acc = 0.9951\n",
      "batch 100\tloss = 0.09683\tacc = 0.9844\n",
      "batch 200\tloss = 0.04006\tacc = 0.9844\n",
      "batch 300\tloss = 0.04136\tacc = 0.9844\n",
      "batch 400\tloss = 0.0003614\tacc = 1.0\n",
      "epoch 9\tavg epoch loss = 0.01419\tavg epoch acc = 0.9956\n",
      "training took 340.5 s\n",
      "Avg test loss = 0.0503\tAvg test acc = 0.986\n"
     ]
    }
   ],
   "source": [
    "adam_naive = AdamOptimizer(net_naive.parameters(), lr=best_adam_params['lr'], beta1=best_adam_params['beta1'],beta2=best_adam_params['beta2'],weight_decay=best_adam_params['weight_decay'],epsilon=best_adam_params['epsilon'])\n",
    "loss_train, acc_train = training(net_naive, train_loader, adam_naive, criterion, accuracy, epochs=epochs, device=device)\n",
    "loss_test, acc_test = testing(net_naive, test_loader, criterion, accuracy, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.00\tTest Accuracy = 0.970\n",
      "Epsilon: 0.05\tTest Accuracy = 0.966\n",
      "Epsilon: 0.10\tTest Accuracy = 0.960\n",
      "Epsilon: 0.15\tTest Accuracy = 0.950\n",
      "Epsilon: 0.20\tTest Accuracy = 0.939\n",
      "Epsilon: 0.25\tTest Accuracy = 0.923\n",
      "Epsilon: 0.30\tTest Accuracy = 0.898\n",
      "Epsilon: 0.35\tTest Accuracy = 0.864\n",
      "Epsilon: 0.40\tTest Accuracy = 0.820\n",
      "Epsilon: 0.45\tTest Accuracy = 0.762\n"
     ]
    }
   ],
   "source": [
    "noise = np.arange(0, 0.5, 0.05)\n",
    "\n",
    "accuracy_naive= []\n",
    "losses_naive= []\n",
    "\n",
    "for ns in noise:\n",
    "    loss_attack, acc_attack  = attack(net_naive, criterion, test_loader, epsilon=ns, device=device)\n",
    "    accuracy_naive.append(acc_attack)\n",
    "    losses_naive.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train robust model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "robust_net = Net().to(device)\n",
    "protect_epochs = epochs\n",
    "protect_bz = 128\n",
    "prot_train_loader, prot_test_loader = build_data_loaders(train_dataset, test_dataset, protect_bz)\n",
    "adam_proc = AdamOptimizer(robust_net.parameters(), lr=best_adam_params['lr'], beta1=best_adam_params['beta1'],beta2=best_adam_params['beta2'],weight_decay=best_adam_params['weight_decay'],epsilon=best_adam_params['epsilon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the protect function and make the model robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0.00 | Test accuracy: 0.88637\n",
      "Epoch 1.00 | Test accuracy: 0.93839\n",
      "Epoch 2.00 | Test accuracy: 0.93275\n",
      "Epoch 3.00 | Test accuracy: 0.92969\n",
      "Epoch 4.00 | Test accuracy: 0.92613\n",
      "Epoch 5.00 | Test accuracy: 0.92820\n",
      "Epoch 6.00 | Test accuracy: 0.92445\n",
      "Epoch 7.00 | Test accuracy: 0.93829\n",
      "Epoch 8.00 | Test accuracy: 0.93493\n",
      "Epoch 9.00 | Test accuracy: 0.93384\n",
      "training took 910.4 s\n"
     ]
    }
   ],
   "source": [
    "robust_net = protect(robust_net, adam_proc, criterion, prot_train_loader, prot_test_loader, device=device, epochs=protect_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack  the Robust Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.00\tTest Accuracy = 0.991\n",
      "Epsilon: 0.05\tTest Accuracy = 0.990\n",
      "Epsilon: 0.10\tTest Accuracy = 0.988\n",
      "Epsilon: 0.15\tTest Accuracy = 0.986\n",
      "Epsilon: 0.20\tTest Accuracy = 0.984\n",
      "Epsilon: 0.25\tTest Accuracy = 0.981\n",
      "Epsilon: 0.30\tTest Accuracy = 0.976\n",
      "Epsilon: 0.35\tTest Accuracy = 0.969\n",
      "Epsilon: 0.40\tTest Accuracy = 0.957\n",
      "Epsilon: 0.45\tTest Accuracy = 0.940\n"
     ]
    }
   ],
   "source": [
    "noise = np.arange(0, 0.5, 0.05)\n",
    "accuracy_robust = []\n",
    "losses_robust = []\n",
    "\n",
    "for ns in noise:\n",
    "    loss_attack, acc_attack = attack(robust_net, criterion, prot_train_loader, ns, device=device)\n",
    "    accuracy_robust.append(acc_attack)\n",
    "    losses_robust.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis of the Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracy_naive, \"*-\", c='blue', label='Naive Model')\n",
    "plt.plot(epsilons, accuracy_robust, \"*-\", c='orange', label='Robust Model')\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, 0.5, step=0.05))\n",
    "\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
