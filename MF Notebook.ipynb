{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22486822",
   "metadata": {},
   "source": [
    "# Mini-Batch GD and How Adverserial Examples affects its Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c38f0f",
   "metadata": {},
   "source": [
    "### Overview of this Notebook: <br>\n",
    "#### 1. Import of Python libraries and the dataset<br>\n",
    "#### 2. Tuning of the Hyperparameters of the Mini-Batch Optimiser<br>\n",
    "#### 3. Training a Naive Model<br>\n",
    "#### 4. Evaluate the Attack against the Naive Model<br>\n",
    "#### 5. Training a Robust Model <br>\n",
    "#### 6. Evaluate the Attack against the Robust Model <br>\n",
    "#### 7. Comparision of the two performances <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5101acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversary import attack, protect\n",
    "from net import Net\n",
    "import numpy as np\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "from training import training, testing, accuracy, tune_optimizer\n",
    "from minibatch import MiniBatchOptimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import get_mnist, build_data_loaders\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460983e",
   "metadata": {},
   "source": [
    "## Get Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d4508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device chosen is cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device chosen is {}\".format(device))\n",
    "train_dataset, test_dataset = get_mnist(normalize=True)\n",
    "epsilons = np.arange(0, 0.5, 0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a7e912",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "### To test the tuning, just set hyperparamter_tune to True. Otherwise, there is a JSON file with previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b5b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamter_tune = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28633f",
   "metadata": {},
   "source": [
    "This is the tuning setup. It initialises the Optimiser, a new Neural net and then generates a the True/False options for the flag for a Decreasing Learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1314b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tune = Net().to(device)\n",
    "mini_opt_tune = MiniBatchOptimizer(net_tune.parameters()) # Just using defaults\n",
    "dec_lr_set =  [0]*1 + [1]*1\n",
    "random.shuffle(dec_lr_set)\n",
    "fp = 'mini_tuning.json'\n",
    "if not hyperparamter_tune:\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960d347",
   "metadata": {},
   "source": [
    "If tuning is desired, then the following will use a custom function that takes only the training dataset, and uses it as the basic for new training/testing sets to prevent overfitting on the test data. The algorithm tries out every combination of elements in its search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615fa305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 1e-05, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.207\tavg epoch acc = 0.3338\n",
      "epoch 1\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "epoch 2\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "epoch 3\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "epoch 4\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "epoch 5\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "epoch 6\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "epoch 7\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "epoch 8\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "epoch 9\tavg epoch loss = 2.206\tavg epoch acc = 0.3338\n",
      "training took 60.04 s\n",
      "Avg test loss = 2.21\tAvg test acc = 0.334\n",
      "{'lr': 1e-05, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.201\tavg epoch acc = 0.3423\n",
      "epoch 1\tavg epoch loss = 2.189\tavg epoch acc = 0.358\n",
      "epoch 2\tavg epoch loss = 2.177\tavg epoch acc = 0.3729\n",
      "epoch 3\tavg epoch loss = 2.164\tavg epoch acc = 0.3899\n",
      "epoch 4\tavg epoch loss = 2.15\tavg epoch acc = 0.4047\n",
      "epoch 5\tavg epoch loss = 2.135\tavg epoch acc = 0.4205\n",
      "epoch 6\tavg epoch loss = 2.119\tavg epoch acc = 0.4364\n",
      "epoch 7\tavg epoch loss = 2.102\tavg epoch acc = 0.4542\n",
      "epoch 8\tavg epoch loss = 2.084\tavg epoch acc = 0.4723\n",
      "epoch 9\tavg epoch loss = 2.064\tavg epoch acc = 0.4923\n",
      "training took 58.85 s\n",
      "Avg test loss = 2.06\tAvg test acc = 0.508\n",
      "{'lr': 0.0025075, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.199\tavg epoch acc = 0.3369\n",
      "epoch 1\tavg epoch loss = 2.198\tavg epoch acc = 0.3387\n",
      "epoch 2\tavg epoch loss = 2.197\tavg epoch acc = 0.3397\n",
      "epoch 3\tavg epoch loss = 2.197\tavg epoch acc = 0.3401\n",
      "epoch 4\tavg epoch loss = 2.197\tavg epoch acc = 0.3404\n",
      "epoch 5\tavg epoch loss = 2.197\tavg epoch acc = 0.3407\n",
      "epoch 6\tavg epoch loss = 2.197\tavg epoch acc = 0.3409\n",
      "epoch 7\tavg epoch loss = 2.197\tavg epoch acc = 0.341\n",
      "epoch 8\tavg epoch loss = 2.196\tavg epoch acc = 0.3411\n",
      "epoch 9\tavg epoch loss = 2.196\tavg epoch acc = 0.3412\n",
      "training took 59.24 s\n",
      "Avg test loss = 2.2\tAvg test acc = 0.343\n",
      "{'lr': 0.0025075, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.5683\tavg epoch acc = 0.8595\n",
      "epoch 1\tavg epoch loss = 0.1591\tavg epoch acc = 0.9536\n",
      "epoch 2\tavg epoch loss = 0.1092\tavg epoch acc = 0.9681\n",
      "epoch 3\tavg epoch loss = 0.0858\tavg epoch acc = 0.9746\n",
      "epoch 4\tavg epoch loss = 0.07147\tavg epoch acc = 0.9786\n",
      "epoch 5\tavg epoch loss = 0.06149\tavg epoch acc = 0.9815\n",
      "epoch 6\tavg epoch loss = 0.05409\tavg epoch acc = 0.9839\n",
      "epoch 7\tavg epoch loss = 0.04827\tavg epoch acc = 0.9854\n",
      "epoch 8\tavg epoch loss = 0.04359\tavg epoch acc = 0.9869\n",
      "epoch 9\tavg epoch loss = 0.03968\tavg epoch acc = 0.9878\n",
      "training took 59.27 s\n",
      "Avg test loss = 0.0517\tAvg test acc = 0.985\n",
      "{'lr': 0.005005, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.192\tavg epoch acc = 0.3382\n",
      "epoch 1\tavg epoch loss = 2.189\tavg epoch acc = 0.343\n",
      "epoch 2\tavg epoch loss = 2.188\tavg epoch acc = 0.3445\n",
      "epoch 3\tavg epoch loss = 2.187\tavg epoch acc = 0.3456\n",
      "epoch 4\tavg epoch loss = 2.187\tavg epoch acc = 0.3464\n",
      "epoch 5\tavg epoch loss = 2.186\tavg epoch acc = 0.3472\n",
      "epoch 6\tavg epoch loss = 2.186\tavg epoch acc = 0.3476\n",
      "epoch 7\tavg epoch loss = 2.186\tavg epoch acc = 0.348\n",
      "epoch 8\tavg epoch loss = 2.186\tavg epoch acc = 0.3485\n",
      "epoch 9\tavg epoch loss = 2.185\tavg epoch acc = 0.3487\n",
      "training took 66.54 s\n",
      "Avg test loss = 2.19\tAvg test acc = 0.35\n",
      "{'lr': 0.005005, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.3783\tavg epoch acc = 0.9006\n",
      "epoch 1\tavg epoch loss = 0.1046\tavg epoch acc = 0.9689\n",
      "epoch 2\tavg epoch loss = 0.07259\tavg epoch acc = 0.9785\n",
      "epoch 3\tavg epoch loss = 0.05636\tavg epoch acc = 0.9827\n",
      "epoch 4\tavg epoch loss = 0.04617\tavg epoch acc = 0.9861\n",
      "epoch 5\tavg epoch loss = 0.03903\tavg epoch acc = 0.9882\n",
      "epoch 6\tavg epoch loss = 0.03364\tavg epoch acc = 0.99\n",
      "epoch 7\tavg epoch loss = 0.0294\tavg epoch acc = 0.9913\n",
      "epoch 8\tavg epoch loss = 0.02584\tavg epoch acc = 0.9925\n",
      "epoch 9\tavg epoch loss = 0.02291\tavg epoch acc = 0.9935\n",
      "training took 65.5 s\n",
      "Avg test loss = 0.0453\tAvg test acc = 0.986\n",
      "{'lr': 0.0075025000000000005, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.184\tavg epoch acc = 0.3407\n",
      "epoch 1\tavg epoch loss = 2.18\tavg epoch acc = 0.3487\n",
      "epoch 2\tavg epoch loss = 2.178\tavg epoch acc = 0.3516\n",
      "epoch 3\tavg epoch loss = 2.177\tavg epoch acc = 0.3533\n",
      "epoch 4\tavg epoch loss = 2.176\tavg epoch acc = 0.3544\n",
      "epoch 5\tavg epoch loss = 2.175\tavg epoch acc = 0.3555\n",
      "epoch 6\tavg epoch loss = 2.175\tavg epoch acc = 0.3564\n",
      "epoch 7\tavg epoch loss = 2.175\tavg epoch acc = 0.357\n",
      "epoch 8\tavg epoch loss = 2.174\tavg epoch acc = 0.3576\n",
      "epoch 9\tavg epoch loss = 2.174\tavg epoch acc = 0.3582\n",
      "training took 66.55 s\n",
      "Avg test loss = 2.17\tAvg test acc = 0.36\n",
      "{'lr': 0.0075025000000000005, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.3008\tavg epoch acc = 0.9175\n",
      "epoch 1\tavg epoch loss = 0.08364\tavg epoch acc = 0.9746\n",
      "epoch 2\tavg epoch loss = 0.05732\tavg epoch acc = 0.9823\n",
      "epoch 3\tavg epoch loss = 0.04401\tavg epoch acc = 0.9866\n",
      "epoch 4\tavg epoch loss = 0.03549\tavg epoch acc = 0.9892\n",
      "epoch 5\tavg epoch loss = 0.02934\tavg epoch acc = 0.9911\n",
      "epoch 6\tavg epoch loss = 0.02461\tavg epoch acc = 0.9929\n",
      "epoch 7\tavg epoch loss = 0.02087\tavg epoch acc = 0.994\n",
      "epoch 8\tavg epoch loss = 0.01773\tavg epoch acc = 0.9949\n",
      "epoch 9\tavg epoch loss = 0.015\tavg epoch acc = 0.9959\n",
      "training took 65.09 s\n",
      "Avg test loss = 0.0441\tAvg test acc = 0.988\n",
      "{'lr': 0.01, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.176\tavg epoch acc = 0.3438\n",
      "epoch 1\tavg epoch loss = 2.17\tavg epoch acc = 0.3553\n",
      "epoch 2\tavg epoch loss = 2.168\tavg epoch acc = 0.359\n",
      "epoch 3\tavg epoch loss = 2.166\tavg epoch acc = 0.3615\n",
      "epoch 4\tavg epoch loss = 2.165\tavg epoch acc = 0.3634\n",
      "epoch 5\tavg epoch loss = 2.164\tavg epoch acc = 0.3647\n",
      "epoch 6\tavg epoch loss = 2.163\tavg epoch acc = 0.3656\n",
      "epoch 7\tavg epoch loss = 2.163\tavg epoch acc = 0.3667\n",
      "epoch 8\tavg epoch loss = 2.162\tavg epoch acc = 0.3677\n",
      "epoch 9\tavg epoch loss = 2.162\tavg epoch acc = 0.3682\n",
      "training took 68.77 s\n",
      "Avg test loss = 2.16\tAvg test acc = 0.373\n",
      "{'lr': 0.01, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.2567\tavg epoch acc = 0.9284\n",
      "epoch 1\tavg epoch loss = 0.07256\tavg epoch acc = 0.9775\n",
      "epoch 2\tavg epoch loss = 0.04948\tavg epoch acc = 0.9849\n",
      "epoch 3\tavg epoch loss = 0.03761\tavg epoch acc = 0.9886\n",
      "epoch 4\tavg epoch loss = 0.02969\tavg epoch acc = 0.9911\n",
      "epoch 5\tavg epoch loss = 0.02397\tavg epoch acc = 0.993\n",
      "epoch 6\tavg epoch loss = 0.01946\tavg epoch acc = 0.9943\n",
      "epoch 7\tavg epoch loss = 0.01577\tavg epoch acc = 0.9956\n",
      "epoch 8\tavg epoch loss = 0.01283\tavg epoch acc = 0.9968\n",
      "epoch 9\tavg epoch loss = 0.01041\tavg epoch acc = 0.9975\n",
      "training took 70.73 s\n",
      "Avg test loss = 0.0447\tAvg test acc = 0.988\n"
     ]
    }
   ],
   "source": [
    "if hyperparamter_tune:\n",
    "    results = tune_optimizer(\n",
    "    net_tune,\n",
    "    train_dataset.data,\n",
    "    train_dataset.targets,\n",
    "    criterion,\n",
    "    accuracy,\n",
    "    device,\n",
    "    MiniBatchOptimizer,\n",
    "    epochs=10,\n",
    "    search_grid={\n",
    "        'lr': np.linspace(0.00001, 0.01, 5),\n",
    "        'decreasing_lr': dec_lr_set,\n",
    "    }, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c14316",
   "metadata": {},
   "source": [
    "We then append any new results into the json file for the specific Optimiser so that we do not need to retread previous configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f92ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(fp).exists():\n",
    "    with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)\n",
    "\n",
    "    results = old_results + results\n",
    "\n",
    "with open(fp, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Select Best Hyperparamters\n",
    "with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae54822",
   "metadata": {},
   "source": [
    "### Here we read out the best configuration, as determined by the Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc0b0f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy was 99.175% with Learning Rate 0.050005 and Decreasing LR: 0\n"
     ]
    }
   ],
   "source": [
    "df_analysis = pd.DataFrame(results)\n",
    "best_acc = 0.0\n",
    "for index, row in df_analysis.iterrows():    \n",
    "        trial_acc = row[\"metric_test\"]\n",
    "        if trial_acc > best_acc:\n",
    "            best_acc = trial_acc\n",
    "            learning_rate = round(row[\"lr\"], 6)\n",
    "            decreasing_lr = row[\"decreasing_lr\"]\n",
    "\n",
    "print(\"Best Accuracy was {}% with Learning Rate {} and Decreasing LR: {}\".format(100*best_acc, learning_rate, decreasing_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c77fd",
   "metadata": {},
   "source": [
    "## Train the Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc3da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_naive = Net().to(device)\n",
    "train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35d31c",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b99ff4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on cuda\n",
      "batch 100\tloss = 0.6192\tacc = 0.875\n",
      "batch 200\tloss = 0.1253\tacc = 1.0\n",
      "batch 300\tloss = 0.4042\tacc = 0.9375\n",
      "batch 400\tloss = 0.0211\tacc = 1.0\n",
      "batch 500\tloss = 0.5945\tacc = 0.875\n",
      "batch 600\tloss = 0.1318\tacc = 0.9375\n",
      "batch 700\tloss = 0.1055\tacc = 1.0\n",
      "batch 800\tloss = 0.2217\tacc = 0.875\n",
      "batch 900\tloss = 0.1533\tacc = 0.9375\n",
      "batch 1000\tloss = 0.06149\tacc = 1.0\n",
      "batch 1100\tloss = 0.2182\tacc = 0.9375\n",
      "batch 1200\tloss = 0.06249\tacc = 1.0\n",
      "batch 1300\tloss = 0.0297\tacc = 1.0\n",
      "batch 1400\tloss = 0.007738\tacc = 1.0\n",
      "batch 1500\tloss = 0.003958\tacc = 1.0\n",
      "batch 1600\tloss = 0.04703\tacc = 1.0\n",
      "batch 1700\tloss = 0.06399\tacc = 1.0\n",
      "batch 1800\tloss = 0.07012\tacc = 0.9375\n",
      "batch 1900\tloss = 0.0006208\tacc = 1.0\n",
      "batch 2000\tloss = 0.1352\tacc = 0.875\n",
      "batch 2100\tloss = 0.02405\tacc = 1.0\n",
      "batch 2200\tloss = 0.01038\tacc = 1.0\n",
      "batch 2300\tloss = 0.0007347\tacc = 1.0\n",
      "batch 2400\tloss = 0.06695\tacc = 1.0\n",
      "batch 2500\tloss = 0.03668\tacc = 1.0\n",
      "batch 2600\tloss = 0.003128\tacc = 1.0\n",
      "batch 2700\tloss = 0.0182\tacc = 1.0\n",
      "batch 2800\tloss = 0.02166\tacc = 1.0\n",
      "batch 2900\tloss = 0.007288\tacc = 1.0\n",
      "batch 3000\tloss = 0.01452\tacc = 1.0\n",
      "batch 3100\tloss = 0.04011\tacc = 1.0\n",
      "batch 3200\tloss = 0.002193\tacc = 1.0\n",
      "batch 3300\tloss = 0.0007475\tacc = 1.0\n",
      "batch 3400\tloss = 0.02334\tacc = 1.0\n",
      "batch 3500\tloss = 0.001889\tacc = 1.0\n",
      "batch 3600\tloss = 0.001201\tacc = 1.0\n",
      "batch 3700\tloss = 0.0001156\tacc = 1.0\n",
      "epoch 0\tavg epoch loss = 0.126\tavg epoch acc = 0.9618\n",
      "batch 100\tloss = 0.2212\tacc = 0.9375\n",
      "batch 200\tloss = 0.002221\tacc = 1.0\n",
      "batch 300\tloss = 0.000831\tacc = 1.0\n",
      "batch 400\tloss = 0.0115\tacc = 1.0\n",
      "batch 500\tloss = 0.2173\tacc = 0.9375\n",
      "batch 600\tloss = 0.02044\tacc = 1.0\n",
      "batch 700\tloss = 0.006791\tacc = 1.0\n",
      "batch 800\tloss = 0.00355\tacc = 1.0\n",
      "batch 900\tloss = 0.04564\tacc = 1.0\n",
      "batch 1000\tloss = 0.004307\tacc = 1.0\n",
      "batch 1100\tloss = 0.007564\tacc = 1.0\n",
      "batch 1200\tloss = 0.03532\tacc = 1.0\n",
      "batch 1300\tloss = 0.00174\tacc = 1.0\n",
      "batch 1400\tloss = 0.08092\tacc = 0.9375\n",
      "batch 1500\tloss = 0.00084\tacc = 1.0\n",
      "batch 1600\tloss = 0.001426\tacc = 1.0\n",
      "batch 1700\tloss = 0.01861\tacc = 1.0\n",
      "batch 1800\tloss = 0.0214\tacc = 1.0\n",
      "batch 1900\tloss = 3.919e-05\tacc = 1.0\n",
      "batch 2000\tloss = 0.05696\tacc = 1.0\n",
      "batch 2100\tloss = 0.000898\tacc = 1.0\n",
      "batch 2200\tloss = 0.004283\tacc = 1.0\n",
      "batch 2300\tloss = 0.0001611\tacc = 1.0\n",
      "batch 2400\tloss = 0.1769\tacc = 0.9375\n",
      "batch 2500\tloss = 0.1361\tacc = 0.9375\n",
      "batch 2600\tloss = 0.003023\tacc = 1.0\n",
      "batch 2700\tloss = 0.01129\tacc = 1.0\n",
      "batch 2800\tloss = 0.004847\tacc = 1.0\n",
      "batch 2900\tloss = 0.007378\tacc = 1.0\n",
      "batch 3000\tloss = 0.01164\tacc = 1.0\n",
      "batch 3100\tloss = 0.01176\tacc = 1.0\n",
      "batch 3200\tloss = 0.00237\tacc = 1.0\n",
      "batch 3300\tloss = 0.0004522\tacc = 1.0\n",
      "batch 3400\tloss = 0.01309\tacc = 1.0\n",
      "batch 3500\tloss = 0.000433\tacc = 1.0\n",
      "batch 3600\tloss = 0.0004544\tacc = 1.0\n",
      "batch 3700\tloss = 1.762e-05\tacc = 1.0\n",
      "epoch 1\tavg epoch loss = 0.03941\tavg epoch acc = 0.9881\n",
      "batch 100\tloss = 0.01659\tacc = 1.0\n",
      "batch 200\tloss = 0.000798\tacc = 1.0\n",
      "batch 300\tloss = 0.0001574\tacc = 1.0\n",
      "batch 400\tloss = 0.01669\tacc = 1.0\n",
      "batch 500\tloss = 0.02515\tacc = 1.0\n",
      "batch 600\tloss = 0.003372\tacc = 1.0\n",
      "batch 700\tloss = 0.00215\tacc = 1.0\n",
      "batch 800\tloss = 0.001557\tacc = 1.0\n",
      "batch 900\tloss = 0.01406\tacc = 1.0\n",
      "batch 1000\tloss = 0.002505\tacc = 1.0\n",
      "batch 1100\tloss = 0.02308\tacc = 1.0\n",
      "batch 1200\tloss = 0.00342\tacc = 1.0\n",
      "batch 1300\tloss = 0.001628\tacc = 1.0\n",
      "batch 1400\tloss = 0.001192\tacc = 1.0\n",
      "batch 1500\tloss = 0.0003789\tacc = 1.0\n",
      "batch 1600\tloss = 0.0005974\tacc = 1.0\n",
      "batch 1700\tloss = 0.0009684\tacc = 1.0\n",
      "batch 1800\tloss = 0.009452\tacc = 1.0\n",
      "batch 1900\tloss = 1.118e-05\tacc = 1.0\n",
      "batch 2000\tloss = 0.02548\tacc = 1.0\n",
      "batch 2100\tloss = 0.002915\tacc = 1.0\n",
      "batch 2200\tloss = 0.006425\tacc = 1.0\n",
      "batch 2300\tloss = 0.000245\tacc = 1.0\n",
      "batch 2400\tloss = 0.07632\tacc = 1.0\n",
      "batch 2500\tloss = 0.03664\tacc = 1.0\n",
      "batch 2600\tloss = 0.001409\tacc = 1.0\n",
      "batch 2700\tloss = 0.00347\tacc = 1.0\n",
      "batch 2800\tloss = 0.003617\tacc = 1.0\n",
      "batch 2900\tloss = 0.01146\tacc = 1.0\n",
      "batch 3000\tloss = 0.01741\tacc = 1.0\n",
      "batch 3100\tloss = 0.001534\tacc = 1.0\n",
      "batch 3200\tloss = 0.0002135\tacc = 1.0\n",
      "batch 3300\tloss = 0.0002469\tacc = 1.0\n",
      "batch 3400\tloss = 0.006218\tacc = 1.0\n",
      "batch 3500\tloss = 7.115e-05\tacc = 1.0\n",
      "batch 3600\tloss = 0.0001232\tacc = 1.0\n",
      "batch 3700\tloss = 1.205e-05\tacc = 1.0\n",
      "epoch 2\tavg epoch loss = 0.02372\tavg epoch acc = 0.9929\n",
      "batch 100\tloss = 0.00233\tacc = 1.0\n",
      "batch 200\tloss = 0.0001098\tacc = 1.0\n",
      "batch 300\tloss = 6.872e-05\tacc = 1.0\n",
      "batch 400\tloss = 0.0005203\tacc = 1.0\n",
      "batch 500\tloss = 0.02506\tacc = 1.0\n",
      "batch 600\tloss = 0.002176\tacc = 1.0\n",
      "batch 700\tloss = 0.00589\tacc = 1.0\n",
      "batch 800\tloss = 0.003505\tacc = 1.0\n",
      "batch 900\tloss = 0.0009703\tacc = 1.0\n",
      "batch 1000\tloss = 0.01471\tacc = 1.0\n",
      "batch 1100\tloss = 0.03748\tacc = 1.0\n",
      "batch 1200\tloss = 0.0001567\tacc = 1.0\n",
      "batch 1300\tloss = 0.002306\tacc = 1.0\n",
      "batch 1400\tloss = 0.0001897\tacc = 1.0\n",
      "batch 1500\tloss = 8.905e-05\tacc = 1.0\n",
      "batch 1600\tloss = 0.0002235\tacc = 1.0\n",
      "batch 1700\tloss = 0.0004039\tacc = 1.0\n",
      "batch 1800\tloss = 0.01983\tacc = 1.0\n",
      "batch 1900\tloss = 2.044e-05\tacc = 1.0\n",
      "batch 2000\tloss = 8.199e-05\tacc = 1.0\n",
      "batch 2100\tloss = 7.346e-05\tacc = 1.0\n",
      "batch 2200\tloss = 0.001274\tacc = 1.0\n",
      "batch 2300\tloss = 0.0001207\tacc = 1.0\n",
      "batch 2400\tloss = 0.03526\tacc = 1.0\n",
      "batch 2500\tloss = 0.01327\tacc = 1.0\n",
      "batch 2600\tloss = 0.003025\tacc = 1.0\n",
      "batch 2700\tloss = 0.001112\tacc = 1.0\n",
      "batch 2800\tloss = 0.001251\tacc = 1.0\n",
      "batch 2900\tloss = 0.004925\tacc = 1.0\n",
      "batch 3000\tloss = 0.008243\tacc = 1.0\n",
      "batch 3100\tloss = 0.0009028\tacc = 1.0\n",
      "batch 3200\tloss = 0.0005372\tacc = 1.0\n",
      "batch 3300\tloss = 0.0001589\tacc = 1.0\n",
      "batch 3400\tloss = 0.003207\tacc = 1.0\n",
      "batch 3500\tloss = 5.484e-05\tacc = 1.0\n",
      "batch 3600\tloss = 1.389e-05\tacc = 1.0\n",
      "batch 3700\tloss = 3.397e-06\tacc = 1.0\n",
      "epoch 3\tavg epoch loss = 0.01562\tavg epoch acc = 0.9957\n",
      "batch 100\tloss = 0.000351\tacc = 1.0\n",
      "batch 200\tloss = 5.863e-05\tacc = 1.0\n",
      "batch 300\tloss = 3.125e-05\tacc = 1.0\n",
      "batch 400\tloss = 0.0005584\tacc = 1.0\n",
      "batch 500\tloss = 0.009455\tacc = 1.0\n",
      "batch 600\tloss = 0.005386\tacc = 1.0\n",
      "batch 700\tloss = 0.001809\tacc = 1.0\n",
      "batch 800\tloss = 0.03175\tacc = 1.0\n",
      "batch 900\tloss = 0.0005967\tacc = 1.0\n",
      "batch 1000\tloss = 0.001909\tacc = 1.0\n",
      "batch 1100\tloss = 0.004879\tacc = 1.0\n",
      "batch 1200\tloss = 0.0006717\tacc = 1.0\n",
      "batch 1300\tloss = 0.0001687\tacc = 1.0\n",
      "batch 1400\tloss = 6.635e-05\tacc = 1.0\n",
      "batch 1500\tloss = 2.143e-05\tacc = 1.0\n",
      "batch 1600\tloss = 3.532e-05\tacc = 1.0\n",
      "batch 1700\tloss = 0.0001955\tacc = 1.0\n",
      "batch 1800\tloss = 0.1292\tacc = 0.9375\n",
      "batch 1900\tloss = 7.815e-06\tacc = 1.0\n",
      "batch 2000\tloss = 9.599e-05\tacc = 1.0\n",
      "batch 2100\tloss = 2.508e-05\tacc = 1.0\n",
      "batch 2200\tloss = 0.0006361\tacc = 1.0\n",
      "batch 2300\tloss = 2.315e-05\tacc = 1.0\n",
      "batch 2400\tloss = 0.0946\tacc = 0.9375\n",
      "batch 2500\tloss = 0.002238\tacc = 1.0\n",
      "batch 2600\tloss = 0.001735\tacc = 1.0\n",
      "batch 2700\tloss = 0.007443\tacc = 1.0\n",
      "batch 2800\tloss = 0.0003113\tacc = 1.0\n",
      "batch 2900\tloss = 0.0426\tacc = 1.0\n",
      "batch 3000\tloss = 0.006959\tacc = 1.0\n",
      "batch 3100\tloss = 0.002268\tacc = 1.0\n",
      "batch 3200\tloss = 4.776e-05\tacc = 1.0\n",
      "batch 3300\tloss = 4.3e-05\tacc = 1.0\n",
      "batch 3400\tloss = 0.00187\tacc = 1.0\n",
      "batch 3500\tloss = 2.511e-05\tacc = 1.0\n",
      "batch 3600\tloss = 0.0001143\tacc = 1.0\n",
      "batch 3700\tloss = 3.196e-06\tacc = 1.0\n",
      "epoch 4\tavg epoch loss = 0.01212\tavg epoch acc = 0.9964\n",
      "batch 100\tloss = 0.002917\tacc = 1.0\n",
      "batch 200\tloss = 5.049e-05\tacc = 1.0\n",
      "batch 300\tloss = 0.0001045\tacc = 1.0\n",
      "batch 400\tloss = 0.0001003\tacc = 1.0\n",
      "batch 500\tloss = 0.007257\tacc = 1.0\n",
      "batch 600\tloss = 0.01033\tacc = 1.0\n",
      "batch 700\tloss = 0.001385\tacc = 1.0\n",
      "batch 800\tloss = 0.008406\tacc = 1.0\n",
      "batch 900\tloss = 0.001168\tacc = 1.0\n",
      "batch 1000\tloss = 0.001287\tacc = 1.0\n",
      "batch 1100\tloss = 0.0001639\tacc = 1.0\n",
      "batch 1200\tloss = 5.551e-05\tacc = 1.0\n",
      "batch 1300\tloss = 0.04692\tacc = 0.9375\n",
      "batch 1400\tloss = 0.0002585\tacc = 1.0\n",
      "batch 1500\tloss = 6.944e-06\tacc = 1.0\n",
      "batch 1600\tloss = 7.518e-05\tacc = 1.0\n",
      "batch 1700\tloss = 0.0001201\tacc = 1.0\n",
      "batch 1800\tloss = 5.555e-05\tacc = 1.0\n",
      "batch 1900\tloss = 5.439e-06\tacc = 1.0\n",
      "batch 2000\tloss = 5.543e-06\tacc = 1.0\n",
      "batch 2100\tloss = 0.00011\tacc = 1.0\n",
      "batch 2200\tloss = 2.095e-05\tacc = 1.0\n",
      "batch 2300\tloss = 1.366e-05\tacc = 1.0\n",
      "batch 2400\tloss = 0.01479\tacc = 1.0\n",
      "batch 2500\tloss = 0.001047\tacc = 1.0\n",
      "batch 2600\tloss = 0.00608\tacc = 1.0\n",
      "batch 2700\tloss = 6.419e-05\tacc = 1.0\n",
      "batch 2800\tloss = 0.000182\tacc = 1.0\n",
      "batch 2900\tloss = 0.003214\tacc = 1.0\n",
      "batch 3000\tloss = 0.0004006\tacc = 1.0\n",
      "batch 3100\tloss = 0.001526\tacc = 1.0\n",
      "batch 3200\tloss = 4.581e-05\tacc = 1.0\n",
      "batch 3300\tloss = 3.505e-05\tacc = 1.0\n",
      "batch 3400\tloss = 0.0001472\tacc = 1.0\n",
      "batch 3500\tloss = 1.265e-05\tacc = 1.0\n",
      "batch 3600\tloss = 1.465e-05\tacc = 1.0\n",
      "batch 3700\tloss = 1.192e-07\tacc = 1.0\n",
      "epoch 5\tavg epoch loss = 0.00798\tavg epoch acc = 0.9976\n",
      "batch 100\tloss = 0.0001182\tacc = 1.0\n",
      "batch 200\tloss = 2.492e-05\tacc = 1.0\n",
      "batch 300\tloss = 4.28e-05\tacc = 1.0\n",
      "batch 400\tloss = 5.907e-05\tacc = 1.0\n",
      "batch 500\tloss = 0.001222\tacc = 1.0\n",
      "batch 600\tloss = 0.001368\tacc = 1.0\n",
      "batch 700\tloss = 0.0008285\tacc = 1.0\n",
      "batch 800\tloss = 0.01743\tacc = 1.0\n",
      "batch 900\tloss = 0.002655\tacc = 1.0\n",
      "batch 1000\tloss = 0.006366\tacc = 1.0\n",
      "batch 1100\tloss = 0.004553\tacc = 1.0\n",
      "batch 1200\tloss = 6.735e-06\tacc = 1.0\n",
      "batch 1300\tloss = 1.599e-05\tacc = 1.0\n",
      "batch 1400\tloss = 1.073e-05\tacc = 1.0\n",
      "batch 1500\tloss = 2.056e-06\tacc = 1.0\n",
      "batch 1600\tloss = 9.745e-05\tacc = 1.0\n",
      "batch 1700\tloss = 2.813e-05\tacc = 1.0\n",
      "batch 1800\tloss = 0.001346\tacc = 1.0\n",
      "batch 1900\tloss = 1.021e-06\tacc = 1.0\n",
      "batch 2000\tloss = 4.022e-05\tacc = 1.0\n",
      "batch 2100\tloss = 4.193e-05\tacc = 1.0\n",
      "batch 2200\tloss = 1.83e-05\tacc = 1.0\n",
      "batch 2300\tloss = 2.779e-06\tacc = 1.0\n",
      "batch 2400\tloss = 0.02178\tacc = 1.0\n",
      "batch 2500\tloss = 0.0005452\tacc = 1.0\n",
      "batch 2600\tloss = 0.01056\tacc = 1.0\n",
      "batch 2700\tloss = 0.000413\tacc = 1.0\n",
      "batch 2800\tloss = 3.377e-05\tacc = 1.0\n",
      "batch 2900\tloss = 0.004554\tacc = 1.0\n",
      "batch 3000\tloss = 0.0009935\tacc = 1.0\n",
      "batch 3100\tloss = 0.0008055\tacc = 1.0\n",
      "batch 3200\tloss = 0.0002604\tacc = 1.0\n",
      "batch 3300\tloss = 1.156e-05\tacc = 1.0\n",
      "batch 3400\tloss = 6.91e-05\tacc = 1.0\n",
      "batch 3500\tloss = 1.085e-05\tacc = 1.0\n",
      "batch 3600\tloss = 0.001158\tacc = 1.0\n",
      "batch 3700\tloss = 1.416e-07\tacc = 1.0\n",
      "epoch 6\tavg epoch loss = 0.006126\tavg epoch acc = 0.9982\n",
      "batch 100\tloss = 0.01552\tacc = 1.0\n",
      "batch 200\tloss = 1.482e-05\tacc = 1.0\n",
      "batch 300\tloss = 7.979e-06\tacc = 1.0\n",
      "batch 400\tloss = 0.07467\tacc = 0.9375\n",
      "batch 500\tloss = 0.0006814\tacc = 1.0\n",
      "batch 600\tloss = 0.0001389\tacc = 1.0\n",
      "batch 700\tloss = 0.003118\tacc = 1.0\n",
      "batch 800\tloss = 0.04963\tacc = 1.0\n",
      "batch 900\tloss = 0.001039\tacc = 1.0\n",
      "batch 1000\tloss = 0.03193\tacc = 1.0\n",
      "batch 1100\tloss = 0.00294\tacc = 1.0\n",
      "batch 1200\tloss = 6.183e-05\tacc = 1.0\n",
      "batch 1300\tloss = 1.148e-05\tacc = 1.0\n",
      "batch 1400\tloss = 1.259e-06\tacc = 1.0\n",
      "batch 1500\tloss = 1.431e-06\tacc = 1.0\n",
      "batch 1600\tloss = 0.0006055\tacc = 1.0\n",
      "batch 1700\tloss = 0.0003091\tacc = 1.0\n",
      "batch 1800\tloss = 0.0005638\tacc = 1.0\n",
      "batch 1900\tloss = 1.237e-06\tacc = 1.0\n",
      "batch 2000\tloss = 0.000919\tacc = 1.0\n",
      "batch 2100\tloss = 1.932e-05\tacc = 1.0\n",
      "batch 2200\tloss = 0.000524\tacc = 1.0\n",
      "batch 2300\tloss = 0.0002595\tacc = 1.0\n",
      "batch 2400\tloss = 0.08518\tacc = 0.9375\n",
      "batch 2500\tloss = 0.0002266\tacc = 1.0\n",
      "batch 2600\tloss = 0.003135\tacc = 1.0\n",
      "batch 2700\tloss = 8.099e-06\tacc = 1.0\n",
      "batch 2800\tloss = 1.51e-05\tacc = 1.0\n",
      "batch 2900\tloss = 0.003875\tacc = 1.0\n",
      "batch 3000\tloss = 0.0004634\tacc = 1.0\n",
      "batch 3100\tloss = 0.08523\tacc = 0.9375\n",
      "batch 3200\tloss = 6.973e-05\tacc = 1.0\n",
      "batch 3300\tloss = 5.291e-05\tacc = 1.0\n",
      "batch 3400\tloss = 5.8e-05\tacc = 1.0\n",
      "batch 3500\tloss = 4.575e-06\tacc = 1.0\n",
      "batch 3600\tloss = 7.854e-05\tacc = 1.0\n",
      "batch 3700\tloss = 5.96e-08\tacc = 1.0\n",
      "epoch 7\tavg epoch loss = 0.007522\tavg epoch acc = 0.9976\n",
      "batch 100\tloss = 0.0095\tacc = 1.0\n",
      "batch 200\tloss = 2.545e-05\tacc = 1.0\n",
      "batch 300\tloss = 7.063e-06\tacc = 1.0\n",
      "batch 400\tloss = 8.046e-06\tacc = 1.0\n",
      "batch 500\tloss = 0.0001102\tacc = 1.0\n",
      "batch 600\tloss = 6.615e-05\tacc = 1.0\n",
      "batch 700\tloss = 0.0004846\tacc = 1.0\n",
      "batch 800\tloss = 0.00107\tacc = 1.0\n",
      "batch 900\tloss = 9.643e-05\tacc = 1.0\n",
      "batch 1000\tloss = 0.005396\tacc = 1.0\n",
      "batch 1100\tloss = 0.007873\tacc = 1.0\n",
      "batch 1200\tloss = 0.000134\tacc = 1.0\n",
      "batch 1300\tloss = 0.01392\tacc = 1.0\n",
      "batch 1400\tloss = 6.631e-06\tacc = 1.0\n",
      "batch 1500\tloss = 3.502e-07\tacc = 1.0\n",
      "batch 1600\tloss = 2.624e-05\tacc = 1.0\n",
      "batch 1700\tloss = 0.0001145\tacc = 1.0\n",
      "batch 1800\tloss = 0.1404\tacc = 0.9375\n",
      "batch 1900\tloss = 6.556e-07\tacc = 1.0\n",
      "batch 2000\tloss = 2.488e-06\tacc = 1.0\n",
      "batch 2100\tloss = 1.255e-05\tacc = 1.0\n",
      "batch 2200\tloss = 2.875e-05\tacc = 1.0\n",
      "batch 2300\tloss = 4.613e-05\tacc = 1.0\n",
      "batch 2400\tloss = 0.01021\tacc = 1.0\n",
      "batch 2500\tloss = 0.0007313\tacc = 1.0\n",
      "batch 2600\tloss = 0.002107\tacc = 1.0\n",
      "batch 2700\tloss = 2.7e-05\tacc = 1.0\n",
      "batch 2800\tloss = 2.819e-05\tacc = 1.0\n",
      "batch 2900\tloss = 0.002086\tacc = 1.0\n",
      "batch 3000\tloss = 0.0001266\tacc = 1.0\n",
      "batch 3100\tloss = 0.007127\tacc = 1.0\n",
      "batch 3200\tloss = 0.01533\tacc = 1.0\n",
      "batch 3300\tloss = 2.682e-07\tacc = 1.0\n",
      "batch 3400\tloss = 3.404e-05\tacc = 1.0\n",
      "batch 3500\tloss = 1.498e-06\tacc = 1.0\n",
      "batch 3600\tloss = 3.464e-05\tacc = 1.0\n",
      "batch 3700\tloss = 3.725e-08\tacc = 1.0\n",
      "epoch 8\tavg epoch loss = 0.005544\tavg epoch acc = 0.9983\n",
      "batch 100\tloss = 6.333e-06\tacc = 1.0\n",
      "batch 200\tloss = 1.14e-06\tacc = 1.0\n",
      "batch 300\tloss = 8.59e-06\tacc = 1.0\n",
      "batch 400\tloss = 1.245e-05\tacc = 1.0\n",
      "batch 500\tloss = 0.0001093\tacc = 1.0\n",
      "batch 600\tloss = 2.486e-05\tacc = 1.0\n",
      "batch 700\tloss = 0.0001105\tacc = 1.0\n",
      "batch 800\tloss = 0.0001884\tacc = 1.0\n",
      "batch 900\tloss = 0.0001671\tacc = 1.0\n",
      "batch 1000\tloss = 0.02459\tacc = 1.0\n",
      "batch 1100\tloss = 0.0005468\tacc = 1.0\n",
      "batch 1200\tloss = 5.178e-06\tacc = 1.0\n",
      "batch 1300\tloss = 0.0002267\tacc = 1.0\n",
      "batch 1400\tloss = 1.021e-06\tacc = 1.0\n",
      "batch 1500\tloss = 1.118e-07\tacc = 1.0\n",
      "batch 1600\tloss = 3.725e-06\tacc = 1.0\n",
      "batch 1700\tloss = 4.38e-05\tacc = 1.0\n",
      "batch 1800\tloss = 0.0001623\tacc = 1.0\n",
      "batch 1900\tloss = 8.903e-06\tacc = 1.0\n",
      "batch 2000\tloss = 5.893e-06\tacc = 1.0\n",
      "batch 2100\tloss = 1.442e-05\tacc = 1.0\n",
      "batch 2200\tloss = 6.675e-06\tacc = 1.0\n",
      "batch 2300\tloss = 6.109e-06\tacc = 1.0\n",
      "batch 2400\tloss = 0.03319\tacc = 1.0\n",
      "batch 2500\tloss = 0.0001699\tacc = 1.0\n",
      "batch 2600\tloss = 0.0003664\tacc = 1.0\n",
      "batch 2700\tloss = 2.19e-06\tacc = 1.0\n",
      "batch 2800\tloss = 7.808e-06\tacc = 1.0\n",
      "batch 2900\tloss = 0.009652\tacc = 1.0\n",
      "batch 3000\tloss = 2.976e-05\tacc = 1.0\n",
      "batch 3100\tloss = 0.0009815\tacc = 1.0\n",
      "batch 3200\tloss = 4.442e-05\tacc = 1.0\n",
      "batch 3300\tloss = 1.49e-07\tacc = 1.0\n",
      "batch 3400\tloss = 2.583e-05\tacc = 1.0\n",
      "batch 3500\tloss = 2.98e-08\tacc = 1.0\n",
      "batch 3600\tloss = 3.204e-07\tacc = 1.0\n",
      "batch 3700\tloss = 0.0\tacc = 1.0\n",
      "epoch 9\tavg epoch loss = 0.002901\tavg epoch acc = 0.9992\n",
      "training took 83.0 s\n",
      "Avg test loss = 0.036\tAvg test acc = 0.992\n"
     ]
    }
   ],
   "source": [
    "mini_opt_naive = MiniBatchOptimizer(net_naive.parameters(), lr=learning_rate, decreasing_lr=decreasing_lr)\n",
    "loss_train, acc_train = training(net_naive, train_loader, mini_opt_naive, criterion, accuracy, epochs=epochs, device=device)\n",
    "loss_test, acc_test = testing(net_naive, test_loader, criterion, accuracy, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee845db",
   "metadata": {},
   "source": [
    "## Attack Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b3b34b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.00\tTest Accuracy = 0.966\n",
      "Epsilon: 0.05\tTest Accuracy = 0.960\n",
      "Epsilon: 0.10\tTest Accuracy = 0.952\n",
      "Epsilon: 0.15\tTest Accuracy = 0.943\n",
      "Epsilon: 0.20\tTest Accuracy = 0.930\n",
      "Epsilon: 0.25\tTest Accuracy = 0.914\n",
      "Epsilon: 0.30\tTest Accuracy = 0.891\n",
      "Epsilon: 0.35\tTest Accuracy = 0.862\n",
      "Epsilon: 0.40\tTest Accuracy = 0.818\n",
      "Epsilon: 0.45\tTest Accuracy = 0.763\n"
     ]
    }
   ],
   "source": [
    "accuracy_naive= []\n",
    "losses_naive= []\n",
    "\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack  = attack(net_naive, criterion, test_loader, epsilon=eps, device=device)\n",
    "    accuracy_naive.append(acc_attack)\n",
    "    losses_naive.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3952bdc",
   "metadata": {},
   "source": [
    "## Train the Robust Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2850f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_net = Net().to(device)\n",
    "protect_epochs = epochs\n",
    "protect_lr = learning_rate\n",
    "protect_bz = batch_size\n",
    "protect_dec_lr = decreasing_lr\n",
    "prot_train_loader, prot_test_loader = build_data_loaders(train_dataset, test_dataset, protect_bz)\n",
    "mini_opt_proc = MiniBatchOptimizer(robust_net.parameters(), lr=protect_lr, decreasing_lr=protect_dec_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bcbf15",
   "metadata": {},
   "source": [
    "### Call the protect function to make the model robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d553631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0.00 | Test accuracy: 0.97660\n",
      "Epoch 1.00 | Test accuracy: 0.98300\n",
      "Epoch 2.00 | Test accuracy: 0.98390\n",
      "Epoch 3.00 | Test accuracy: 0.98410\n",
      "Epoch 4.00 | Test accuracy: 0.98350\n",
      "Epoch 5.00 | Test accuracy: 0.98310\n",
      "Epoch 6.00 | Test accuracy: 0.98110\n",
      "Epoch 7.00 | Test accuracy: 0.97810\n",
      "Epoch 8.00 | Test accuracy: 0.98190\n",
      "Epoch 9.00 | Test accuracy: 0.98150\n",
      "training took 140.9 s\n"
     ]
    }
   ],
   "source": [
    "robust_net = protect(robust_net, mini_opt_proc, criterion, prot_train_loader, prot_test_loader, device=device, epochs=protect_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5c317",
   "metadata": {},
   "source": [
    "## Attack the Robust Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86c3befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.00\tTest Accuracy = 0.997\n",
      "Epsilon: 0.05\tTest Accuracy = 0.997\n",
      "Epsilon: 0.10\tTest Accuracy = 0.996\n",
      "Epsilon: 0.15\tTest Accuracy = 0.994\n",
      "Epsilon: 0.20\tTest Accuracy = 0.992\n",
      "Epsilon: 0.25\tTest Accuracy = 0.988\n",
      "Epsilon: 0.30\tTest Accuracy = 0.981\n",
      "Epsilon: 0.35\tTest Accuracy = 0.970\n",
      "Epsilon: 0.40\tTest Accuracy = 0.953\n",
      "Epsilon: 0.45\tTest Accuracy = 0.928\n"
     ]
    }
   ],
   "source": [
    "accuracy_robust = []\n",
    "losses_robust = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack = attack(robust_net, criterion, prot_train_loader, eps, device=device)\n",
    "    accuracy_robust.append(acc_attack)\n",
    "    losses_robust.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503c9f0",
   "metadata": {},
   "source": [
    "## Comparative Analysis of the Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69824bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracy_naive, \"*-\", c='blue', label='Naive Model')\n",
    "plt.plot(epsilons, accuracy_robust, \"*-\", c='orange', label='Robust Model')\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, 0.5, step=0.05))\n",
    "\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab92763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
