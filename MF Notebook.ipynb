{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch GD and How Adverserial Examples affects its Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of this Notebook: <br>\n",
    "#### 1. Import of Python libraries and the dataset<br>\n",
    "#### 2. Tuning of the Hyperparameters of the Mini-Batch Optimiser<br>\n",
    "#### 3. Training a Naive Model<br>\n",
    "#### 4. Evaluate the Attack against the Naive Model<br>\n",
    "#### 5. Training a Robust Model <br>\n",
    "#### 6. Evaluate the Attack against the Robust Model <br>\n",
    "#### 7. Comparision of the two performances <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversary import attack, protect\n",
    "from net import Net\n",
    "import numpy as np\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "from training import training, testing, accuracy, tune_optimizer\n",
    "from minibatch import MiniBatchOptimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import get_mnist, build_data_loaders\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device chosen is cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device chosen is {}\".format(device))\n",
    "train_dataset, test_dataset = get_mnist(normalize=True)\n",
    "epsilons = np.arange(0, 0.5, 0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "batch_size = 16 # Mardy has a bigger batch size of 100 + it shuffles the train loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "### To test the tuning, just set hyperparamter_tune to True. Otherwise, there is a JSON file with previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamter_tune = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the tuning setup. It initialises the Optimiser, a new Neural net and then generates a the True/False options for the flag for a Decreasing Learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tune = Net().to(device)\n",
    "mini_opt_tune = MiniBatchOptimizer(net_tune.parameters()) # Just using defaults\n",
    "dec_lr_set =  [0]*1 + [1]*1\n",
    "random.shuffle(dec_lr_set)\n",
    "fp = 'mini_tuning.json'\n",
    "if not hyperparamter_tune:\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If tuning is desired, then the following will use a custom function that takes only the training dataset, and uses it as the basic for new training/testing sets to prevent overfitting on the test data. The algorithm tries out every combination of elements in its search grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 1e-05, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 1\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 2\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 3\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 4\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 5\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 6\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 7\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 8\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "epoch 9\tavg epoch loss = 2.308\tavg epoch acc = 0.07744\n",
      "training took 61.18 s\n",
      "Avg test loss = 2.31\tAvg test acc = 0.0812\n",
      "{'lr': 1e-05, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.303\tavg epoch acc = 0.08608\n",
      "epoch 1\tavg epoch loss = 2.293\tavg epoch acc = 0.1052\n",
      "epoch 2\tavg epoch loss = 2.285\tavg epoch acc = 0.1234\n",
      "epoch 3\tavg epoch loss = 2.276\tavg epoch acc = 0.1397\n",
      "epoch 4\tavg epoch loss = 2.268\tavg epoch acc = 0.1552\n",
      "epoch 5\tavg epoch loss = 2.259\tavg epoch acc = 0.1759\n",
      "epoch 6\tavg epoch loss = 2.251\tavg epoch acc = 0.2123\n",
      "epoch 7\tavg epoch loss = 2.242\tavg epoch acc = 0.2595\n",
      "epoch 8\tavg epoch loss = 2.234\tavg epoch acc = 0.3089\n",
      "epoch 9\tavg epoch loss = 2.225\tavg epoch acc = 0.3534\n",
      "training took 61.35 s\n",
      "Avg test loss = 2.22\tAvg test acc = 0.38\n",
      "{'lr': 0.0025075, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.301\tavg epoch acc = 0.09594\n",
      "epoch 1\tavg epoch loss = 2.3\tavg epoch acc = 0.09754\n",
      "epoch 2\tavg epoch loss = 2.3\tavg epoch acc = 0.09821\n",
      "epoch 3\tavg epoch loss = 2.299\tavg epoch acc = 0.09854\n",
      "epoch 4\tavg epoch loss = 2.299\tavg epoch acc = 0.09894\n",
      "epoch 5\tavg epoch loss = 2.299\tavg epoch acc = 0.09917\n",
      "epoch 6\tavg epoch loss = 2.299\tavg epoch acc = 0.09927\n",
      "epoch 7\tavg epoch loss = 2.299\tavg epoch acc = 0.09944\n",
      "epoch 8\tavg epoch loss = 2.299\tavg epoch acc = 0.09952\n",
      "epoch 9\tavg epoch loss = 2.299\tavg epoch acc = 0.09952\n",
      "training took 59.59 s\n",
      "Avg test loss = 2.3\tAvg test acc = 0.102\n",
      "{'lr': 0.0025075, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.6994\tavg epoch acc = 0.8213\n",
      "epoch 1\tavg epoch loss = 0.1798\tavg epoch acc = 0.9464\n",
      "epoch 2\tavg epoch loss = 0.1157\tavg epoch acc = 0.9658\n",
      "epoch 3\tavg epoch loss = 0.08786\tavg epoch acc = 0.9735\n",
      "epoch 4\tavg epoch loss = 0.0722\tavg epoch acc = 0.9782\n",
      "epoch 5\tavg epoch loss = 0.06174\tavg epoch acc = 0.9811\n",
      "epoch 6\tavg epoch loss = 0.05412\tavg epoch acc = 0.9835\n",
      "epoch 7\tavg epoch loss = 0.0482\tavg epoch acc = 0.9856\n",
      "epoch 8\tavg epoch loss = 0.04343\tavg epoch acc = 0.9873\n",
      "epoch 9\tavg epoch loss = 0.03946\tavg epoch acc = 0.9886\n",
      "training took 63.74 s\n",
      "Avg test loss = 0.0509\tAvg test acc = 0.984\n",
      "{'lr': 0.005005, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.295\tavg epoch acc = 0.1095\n",
      "epoch 1\tavg epoch loss = 2.293\tavg epoch acc = 0.112\n",
      "epoch 2\tavg epoch loss = 2.292\tavg epoch acc = 0.1131\n",
      "epoch 3\tavg epoch loss = 2.292\tavg epoch acc = 0.1137\n",
      "epoch 4\tavg epoch loss = 2.291\tavg epoch acc = 0.1142\n",
      "epoch 5\tavg epoch loss = 2.291\tavg epoch acc = 0.1146\n",
      "epoch 6\tavg epoch loss = 2.291\tavg epoch acc = 0.1149\n",
      "epoch 7\tavg epoch loss = 2.291\tavg epoch acc = 0.1151\n",
      "epoch 8\tavg epoch loss = 2.291\tavg epoch acc = 0.1153\n",
      "epoch 9\tavg epoch loss = 2.291\tavg epoch acc = 0.1155\n",
      "training took 65.29 s\n",
      "Avg test loss = 2.29\tAvg test acc = 0.12\n",
      "{'lr': 0.005005, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.4546\tavg epoch acc = 0.8746\n",
      "epoch 1\tavg epoch loss = 0.1093\tavg epoch acc = 0.9673\n",
      "epoch 2\tavg epoch loss = 0.07275\tavg epoch acc = 0.9774\n",
      "epoch 3\tavg epoch loss = 0.05609\tavg epoch acc = 0.9828\n",
      "epoch 4\tavg epoch loss = 0.0459\tavg epoch acc = 0.9861\n",
      "epoch 5\tavg epoch loss = 0.03867\tavg epoch acc = 0.9886\n",
      "epoch 6\tavg epoch loss = 0.0332\tavg epoch acc = 0.9906\n",
      "epoch 7\tavg epoch loss = 0.02886\tavg epoch acc = 0.9922\n",
      "epoch 8\tavg epoch loss = 0.02533\tavg epoch acc = 0.9931\n",
      "epoch 9\tavg epoch loss = 0.02238\tavg epoch acc = 0.9942\n",
      "training took 62.49 s\n",
      "Avg test loss = 0.0435\tAvg test acc = 0.986\n",
      "{'lr': 0.0075025000000000005, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.289\tavg epoch acc = 0.117\n",
      "epoch 1\tavg epoch loss = 2.286\tavg epoch acc = 0.1201\n",
      "epoch 2\tavg epoch loss = 2.285\tavg epoch acc = 0.1212\n",
      "epoch 3\tavg epoch loss = 2.285\tavg epoch acc = 0.1219\n",
      "epoch 4\tavg epoch loss = 2.284\tavg epoch acc = 0.1223\n",
      "epoch 5\tavg epoch loss = 2.284\tavg epoch acc = 0.1227\n",
      "epoch 6\tavg epoch loss = 2.283\tavg epoch acc = 0.1232\n",
      "epoch 7\tavg epoch loss = 2.283\tavg epoch acc = 0.1237\n",
      "epoch 8\tavg epoch loss = 2.283\tavg epoch acc = 0.1239\n",
      "epoch 9\tavg epoch loss = 2.283\tavg epoch acc = 0.1241\n",
      "training took 59.31 s\n",
      "Avg test loss = 2.28\tAvg test acc = 0.129\n",
      "{'lr': 0.0075025000000000005, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.3548\tavg epoch acc = 0.8979\n",
      "epoch 1\tavg epoch loss = 0.08504\tavg epoch acc = 0.9739\n",
      "epoch 2\tavg epoch loss = 0.05739\tavg epoch acc = 0.9823\n",
      "epoch 3\tavg epoch loss = 0.0437\tavg epoch acc = 0.9871\n",
      "epoch 4\tavg epoch loss = 0.03516\tavg epoch acc = 0.9895\n",
      "epoch 5\tavg epoch loss = 0.0291\tavg epoch acc = 0.992\n",
      "epoch 6\tavg epoch loss = 0.02448\tavg epoch acc = 0.9933\n",
      "epoch 7\tavg epoch loss = 0.02064\tavg epoch acc = 0.9945\n",
      "epoch 8\tavg epoch loss = 0.01755\tavg epoch acc = 0.9955\n",
      "epoch 9\tavg epoch loss = 0.01496\tavg epoch acc = 0.9962\n",
      "training took 58.39 s\n",
      "Avg test loss = 0.0435\tAvg test acc = 0.986\n",
      "{'lr': 0.01, 'decreasing_lr': 1}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 2.284\tavg epoch acc = 0.1207\n",
      "epoch 1\tavg epoch loss = 2.28\tavg epoch acc = 0.1244\n",
      "epoch 2\tavg epoch loss = 2.279\tavg epoch acc = 0.1259\n",
      "epoch 3\tavg epoch loss = 2.278\tavg epoch acc = 0.1273\n",
      "epoch 4\tavg epoch loss = 2.277\tavg epoch acc = 0.1281\n",
      "epoch 5\tavg epoch loss = 2.276\tavg epoch acc = 0.1287\n",
      "epoch 6\tavg epoch loss = 2.276\tavg epoch acc = 0.1293\n",
      "epoch 7\tavg epoch loss = 2.276\tavg epoch acc = 0.1298\n",
      "epoch 8\tavg epoch loss = 2.275\tavg epoch acc = 0.1302\n",
      "epoch 9\tavg epoch loss = 2.275\tavg epoch acc = 0.1306\n",
      "training took 59.28 s\n",
      "Avg test loss = 2.27\tAvg test acc = 0.133\n",
      "{'lr': 0.01, 'decreasing_lr': 0}\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.2994\tavg epoch acc = 0.9134\n",
      "epoch 1\tavg epoch loss = 0.07268\tavg epoch acc = 0.9776\n",
      "epoch 2\tavg epoch loss = 0.04889\tavg epoch acc = 0.9853\n",
      "epoch 3\tavg epoch loss = 0.03689\tavg epoch acc = 0.9892\n",
      "epoch 4\tavg epoch loss = 0.02919\tavg epoch acc = 0.9917\n",
      "epoch 5\tavg epoch loss = 0.0236\tavg epoch acc = 0.9934\n",
      "epoch 6\tavg epoch loss = 0.0191\tavg epoch acc = 0.9949\n",
      "epoch 7\tavg epoch loss = 0.01556\tavg epoch acc = 0.996\n",
      "epoch 8\tavg epoch loss = 0.01266\tavg epoch acc = 0.9969\n",
      "epoch 9\tavg epoch loss = 0.01034\tavg epoch acc = 0.9976\n",
      "training took 58.69 s\n",
      "Avg test loss = 0.0446\tAvg test acc = 0.988\n"
     ]
    }
   ],
   "source": [
    "if hyperparamter_tune:\n",
    "    results = tune_optimizer(\n",
    "    net_tune,\n",
    "    train_dataset.data,\n",
    "    train_dataset.targets,\n",
    "    criterion,\n",
    "    accuracy,\n",
    "    device,\n",
    "    MiniBatchOptimizer,\n",
    "    epochs=10,\n",
    "    search_grid={\n",
    "        'lr': np.linspace(0.00001, 0.01, 5),\n",
    "        'decreasing_lr': dec_lr_set,\n",
    "    }, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then append any new results into the json file for the specific Optimiser so that we do not need to retread previous configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(fp).exists():\n",
    "    with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)\n",
    "\n",
    "    results = old_results + results\n",
    "\n",
    "with open(fp, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Select Best Hyperparamters\n",
    "with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we read out the best configuration, as determined by the Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy was 99.175% with Learning Rate 0.050005 and Decreasing LR: 0\n"
     ]
    }
   ],
   "source": [
    "df_analysis = pd.DataFrame(results)\n",
    "best_acc = 0.0\n",
    "for index, row in df_analysis.iterrows():    \n",
    "        trial_acc = row[\"metric_test\"]\n",
    "        if trial_acc > best_acc:\n",
    "            best_acc = trial_acc\n",
    "            learning_rate = round(row[\"lr\"], 6)\n",
    "            decreasing_lr = row[\"decreasing_lr\"]\n",
    "\n",
    "print(\"Best Accuracy was {}% with Learning Rate {} and Decreasing LR: {}\".format(100*best_acc, learning_rate, decreasing_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_naive = Net().to(device)\n",
    "train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on cuda\n",
      "batch 100\tloss = 0.5917\tacc = 0.875\n",
      "batch 200\tloss = 0.2235\tacc = 0.875\n",
      "batch 300\tloss = 0.3495\tacc = 0.9375\n",
      "batch 400\tloss = 0.02615\tacc = 1.0\n",
      "batch 500\tloss = 0.5\tacc = 0.875\n",
      "batch 600\tloss = 0.1581\tacc = 0.9375\n",
      "batch 700\tloss = 0.1347\tacc = 0.9375\n",
      "batch 800\tloss = 0.2088\tacc = 0.9375\n",
      "batch 900\tloss = 0.1165\tacc = 0.9375\n",
      "batch 1000\tloss = 0.08273\tacc = 1.0\n",
      "batch 1100\tloss = 0.2696\tacc = 0.9375\n",
      "batch 1200\tloss = 0.01921\tacc = 1.0\n",
      "batch 1300\tloss = 0.08345\tacc = 0.9375\n",
      "batch 1400\tloss = 0.00366\tacc = 1.0\n",
      "batch 1500\tloss = 0.001584\tacc = 1.0\n",
      "batch 1600\tloss = 0.01564\tacc = 1.0\n",
      "batch 1700\tloss = 0.237\tacc = 0.875\n",
      "batch 1800\tloss = 0.04046\tacc = 1.0\n",
      "batch 1900\tloss = 0.003784\tacc = 1.0\n",
      "batch 2000\tloss = 0.1373\tacc = 0.875\n",
      "batch 2100\tloss = 0.01713\tacc = 1.0\n",
      "batch 2200\tloss = 0.006495\tacc = 1.0\n",
      "batch 2300\tloss = 0.0008367\tacc = 1.0\n",
      "batch 2400\tloss = 0.04061\tacc = 1.0\n",
      "batch 2500\tloss = 0.06928\tacc = 1.0\n",
      "batch 2600\tloss = 0.006125\tacc = 1.0\n",
      "batch 2700\tloss = 0.01955\tacc = 1.0\n",
      "batch 2800\tloss = 0.01392\tacc = 1.0\n",
      "batch 2900\tloss = 0.01401\tacc = 1.0\n",
      "batch 3000\tloss = 0.04857\tacc = 0.9375\n",
      "batch 3100\tloss = 0.02835\tacc = 1.0\n",
      "batch 3200\tloss = 0.009029\tacc = 1.0\n",
      "batch 3300\tloss = 0.00435\tacc = 1.0\n",
      "batch 3400\tloss = 0.004002\tacc = 1.0\n",
      "batch 3500\tloss = 0.0005085\tacc = 1.0\n",
      "batch 3600\tloss = 0.000731\tacc = 1.0\n",
      "batch 3700\tloss = 0.0002159\tacc = 1.0\n",
      "epoch 0\tavg epoch loss = 0.1213\tavg epoch acc = 0.9634\n",
      "batch 100\tloss = 0.09139\tacc = 0.9375\n",
      "batch 200\tloss = 0.001172\tacc = 1.0\n",
      "batch 300\tloss = 0.001094\tacc = 1.0\n",
      "batch 400\tloss = 0.06053\tacc = 0.9375\n",
      "batch 500\tloss = 0.2733\tacc = 0.9375\n",
      "batch 600\tloss = 0.09306\tacc = 0.9375\n",
      "batch 700\tloss = 0.01712\tacc = 1.0\n",
      "batch 800\tloss = 0.02323\tacc = 1.0\n",
      "batch 900\tloss = 0.007518\tacc = 1.0\n",
      "batch 1000\tloss = 0.05155\tacc = 1.0\n",
      "batch 1100\tloss = 0.01144\tacc = 1.0\n",
      "batch 1200\tloss = 0.05417\tacc = 0.9375\n",
      "batch 1300\tloss = 0.01031\tacc = 1.0\n",
      "batch 1400\tloss = 0.0006662\tacc = 1.0\n",
      "batch 1500\tloss = 0.0003408\tacc = 1.0\n",
      "batch 1600\tloss = 0.004947\tacc = 1.0\n",
      "batch 1700\tloss = 0.04109\tacc = 1.0\n",
      "batch 1800\tloss = 0.03243\tacc = 1.0\n",
      "batch 1900\tloss = 0.001468\tacc = 1.0\n",
      "batch 2000\tloss = 0.01435\tacc = 1.0\n",
      "batch 2100\tloss = 0.001893\tacc = 1.0\n",
      "batch 2200\tloss = 0.001517\tacc = 1.0\n",
      "batch 2300\tloss = 0.0002063\tacc = 1.0\n",
      "batch 2400\tloss = 0.05226\tacc = 1.0\n",
      "batch 2500\tloss = 0.05013\tacc = 0.9375\n",
      "batch 2600\tloss = 0.000606\tacc = 1.0\n",
      "batch 2700\tloss = 0.005086\tacc = 1.0\n",
      "batch 2800\tloss = 0.004323\tacc = 1.0\n",
      "batch 2900\tloss = 0.0005478\tacc = 1.0\n",
      "batch 3000\tloss = 0.04144\tacc = 1.0\n",
      "batch 3100\tloss = 0.00579\tacc = 1.0\n",
      "batch 3200\tloss = 0.002075\tacc = 1.0\n",
      "batch 3300\tloss = 0.0007159\tacc = 1.0\n",
      "batch 3400\tloss = 0.005054\tacc = 1.0\n",
      "batch 3500\tloss = 8.325e-05\tacc = 1.0\n",
      "batch 3600\tloss = 0.0008209\tacc = 1.0\n",
      "batch 3700\tloss = 4.205e-05\tacc = 1.0\n",
      "epoch 1\tavg epoch loss = 0.03791\tavg epoch acc = 0.9886\n",
      "batch 100\tloss = 0.02594\tacc = 1.0\n",
      "batch 200\tloss = 0.000477\tacc = 1.0\n",
      "batch 300\tloss = 0.0004982\tacc = 1.0\n",
      "batch 400\tloss = 0.002806\tacc = 1.0\n",
      "batch 500\tloss = 0.08709\tacc = 0.9375\n",
      "batch 600\tloss = 0.005941\tacc = 1.0\n",
      "batch 700\tloss = 0.001947\tacc = 1.0\n",
      "batch 800\tloss = 0.01547\tacc = 1.0\n",
      "batch 900\tloss = 0.00137\tacc = 1.0\n",
      "batch 1000\tloss = 0.02346\tacc = 1.0\n",
      "batch 1100\tloss = 0.1146\tacc = 0.9375\n",
      "batch 1200\tloss = 0.002208\tacc = 1.0\n",
      "batch 1300\tloss = 0.004113\tacc = 1.0\n",
      "batch 1400\tloss = 0.001247\tacc = 1.0\n",
      "batch 1500\tloss = 0.0002587\tacc = 1.0\n",
      "batch 1600\tloss = 0.001822\tacc = 1.0\n",
      "batch 1700\tloss = 0.003456\tacc = 1.0\n",
      "batch 1800\tloss = 0.1233\tacc = 0.9375\n",
      "batch 1900\tloss = 0.001145\tacc = 1.0\n",
      "batch 2000\tloss = 0.0004287\tacc = 1.0\n",
      "batch 2100\tloss = 9.53e-05\tacc = 1.0\n",
      "batch 2200\tloss = 0.000952\tacc = 1.0\n",
      "batch 2300\tloss = 3.618e-05\tacc = 1.0\n",
      "batch 2400\tloss = 0.01802\tacc = 1.0\n",
      "batch 2500\tloss = 0.007225\tacc = 1.0\n",
      "batch 2600\tloss = 0.0005168\tacc = 1.0\n",
      "batch 2700\tloss = 0.002539\tacc = 1.0\n",
      "batch 2800\tloss = 0.001016\tacc = 1.0\n",
      "batch 2900\tloss = 0.0004058\tacc = 1.0\n",
      "batch 3000\tloss = 0.05024\tacc = 0.9375\n",
      "batch 3100\tloss = 0.005767\tacc = 1.0\n",
      "batch 3200\tloss = 0.004285\tacc = 1.0\n",
      "batch 3300\tloss = 0.0003567\tacc = 1.0\n",
      "batch 3400\tloss = 0.002726\tacc = 1.0\n",
      "batch 3500\tloss = 0.0006738\tacc = 1.0\n",
      "batch 3600\tloss = 0.0008838\tacc = 1.0\n",
      "batch 3700\tloss = 7.398e-06\tacc = 1.0\n",
      "epoch 2\tavg epoch loss = 0.02287\tavg epoch acc = 0.993\n",
      "batch 100\tloss = 0.02434\tacc = 1.0\n",
      "batch 200\tloss = 0.0004103\tacc = 1.0\n",
      "batch 300\tloss = 0.0001032\tacc = 1.0\n",
      "batch 400\tloss = 0.0006014\tacc = 1.0\n",
      "batch 500\tloss = 0.03974\tacc = 1.0\n",
      "batch 600\tloss = 0.0007357\tacc = 1.0\n",
      "batch 700\tloss = 0.001425\tacc = 1.0\n",
      "batch 800\tloss = 0.07177\tacc = 0.9375\n",
      "batch 900\tloss = 0.001091\tacc = 1.0\n",
      "batch 1000\tloss = 0.01424\tacc = 1.0\n",
      "batch 1100\tloss = 0.008902\tacc = 1.0\n",
      "batch 1200\tloss = 0.0003258\tacc = 1.0\n",
      "batch 1300\tloss = 0.0001808\tacc = 1.0\n",
      "batch 1400\tloss = 0.0008182\tacc = 1.0\n",
      "batch 1500\tloss = 0.0001119\tacc = 1.0\n",
      "batch 1600\tloss = 0.0008408\tacc = 1.0\n",
      "batch 1700\tloss = 0.003131\tacc = 1.0\n",
      "batch 1800\tloss = 0.0173\tacc = 1.0\n",
      "batch 1900\tloss = 0.00245\tacc = 1.0\n",
      "batch 2000\tloss = 0.000138\tacc = 1.0\n",
      "batch 2100\tloss = 0.0001682\tacc = 1.0\n",
      "batch 2200\tloss = 0.000674\tacc = 1.0\n",
      "batch 2300\tloss = 3.939e-05\tacc = 1.0\n",
      "batch 2400\tloss = 0.006349\tacc = 1.0\n",
      "batch 2500\tloss = 0.003075\tacc = 1.0\n",
      "batch 2600\tloss = 0.0002837\tacc = 1.0\n",
      "batch 2700\tloss = 0.001094\tacc = 1.0\n",
      "batch 2800\tloss = 0.0006117\tacc = 1.0\n",
      "batch 2900\tloss = 0.0006176\tacc = 1.0\n",
      "batch 3000\tloss = 0.007238\tacc = 1.0\n",
      "batch 3100\tloss = 0.0001012\tacc = 1.0\n",
      "batch 3200\tloss = 0.001396\tacc = 1.0\n",
      "batch 3300\tloss = 0.0002242\tacc = 1.0\n",
      "batch 3400\tloss = 0.00142\tacc = 1.0\n",
      "batch 3500\tloss = 8.761e-05\tacc = 1.0\n",
      "batch 3600\tloss = 5.973e-05\tacc = 1.0\n",
      "batch 3700\tloss = 9.909e-07\tacc = 1.0\n",
      "epoch 3\tavg epoch loss = 0.01512\tavg epoch acc = 0.9957\n",
      "batch 100\tloss = 0.003408\tacc = 1.0\n",
      "batch 200\tloss = 0.0003178\tacc = 1.0\n",
      "batch 300\tloss = 0.0003376\tacc = 1.0\n",
      "batch 400\tloss = 0.0003757\tacc = 1.0\n",
      "batch 500\tloss = 0.002402\tacc = 1.0\n",
      "batch 600\tloss = 0.0003566\tacc = 1.0\n",
      "batch 700\tloss = 0.007544\tacc = 1.0\n",
      "batch 800\tloss = 0.0007958\tacc = 1.0\n",
      "batch 900\tloss = 0.0009098\tacc = 1.0\n",
      "batch 1000\tloss = 0.007549\tacc = 1.0\n",
      "batch 1100\tloss = 0.01898\tacc = 1.0\n",
      "batch 1200\tloss = 9.464e-05\tacc = 1.0\n",
      "batch 1300\tloss = 0.0001853\tacc = 1.0\n",
      "batch 1400\tloss = 6.978e-05\tacc = 1.0\n",
      "batch 1500\tloss = 4.415e-05\tacc = 1.0\n",
      "batch 1600\tloss = 7.743e-05\tacc = 1.0\n",
      "batch 1700\tloss = 0.0004753\tacc = 1.0\n",
      "batch 1800\tloss = 0.01546\tacc = 1.0\n",
      "batch 1900\tloss = 0.007747\tacc = 1.0\n",
      "batch 2000\tloss = 3.424e-05\tacc = 1.0\n",
      "batch 2100\tloss = 2.081e-05\tacc = 1.0\n",
      "batch 2200\tloss = 0.0001294\tacc = 1.0\n",
      "batch 2300\tloss = 5.592e-05\tacc = 1.0\n",
      "batch 2400\tloss = 0.02318\tacc = 1.0\n",
      "batch 2500\tloss = 0.001509\tacc = 1.0\n",
      "batch 2600\tloss = 0.000534\tacc = 1.0\n",
      "batch 2700\tloss = 0.0003174\tacc = 1.0\n",
      "batch 2800\tloss = 0.0005029\tacc = 1.0\n",
      "batch 2900\tloss = 0.001851\tacc = 1.0\n",
      "batch 3000\tloss = 0.004126\tacc = 1.0\n",
      "batch 3100\tloss = 0.000315\tacc = 1.0\n",
      "batch 3200\tloss = 0.0008359\tacc = 1.0\n",
      "batch 3300\tloss = 0.0001476\tacc = 1.0\n",
      "batch 3400\tloss = 0.002142\tacc = 1.0\n",
      "batch 3500\tloss = 2.445e-05\tacc = 1.0\n",
      "batch 3600\tloss = 0.0001756\tacc = 1.0\n",
      "batch 3700\tloss = 3.494e-06\tacc = 1.0\n",
      "epoch 4\tavg epoch loss = 0.01035\tavg epoch acc = 0.9969\n",
      "batch 100\tloss = 0.004406\tacc = 1.0\n",
      "batch 200\tloss = 3.98e-05\tacc = 1.0\n",
      "batch 300\tloss = 3.765e-05\tacc = 1.0\n",
      "batch 400\tloss = 0.0001575\tacc = 1.0\n",
      "batch 500\tloss = 0.001042\tacc = 1.0\n",
      "batch 600\tloss = 0.000179\tacc = 1.0\n",
      "batch 700\tloss = 0.000579\tacc = 1.0\n",
      "batch 800\tloss = 0.008732\tacc = 1.0\n",
      "batch 900\tloss = 0.00273\tacc = 1.0\n",
      "batch 1000\tloss = 0.01263\tacc = 1.0\n",
      "batch 1100\tloss = 0.009639\tacc = 1.0\n",
      "batch 1200\tloss = 0.000406\tacc = 1.0\n",
      "batch 1300\tloss = 9.851e-05\tacc = 1.0\n",
      "batch 1400\tloss = 0.0003993\tacc = 1.0\n",
      "batch 1500\tloss = 1.072e-05\tacc = 1.0\n",
      "batch 1600\tloss = 1.459e-05\tacc = 1.0\n",
      "batch 1700\tloss = 0.0176\tacc = 1.0\n",
      "batch 1800\tloss = 0.002835\tacc = 1.0\n",
      "batch 1900\tloss = 1.922e-05\tacc = 1.0\n",
      "batch 2000\tloss = 2.768e-05\tacc = 1.0\n",
      "batch 2100\tloss = 1.034e-05\tacc = 1.0\n",
      "batch 2200\tloss = 5.873e-05\tacc = 1.0\n",
      "batch 2300\tloss = 0.0001944\tacc = 1.0\n",
      "batch 2400\tloss = 0.004366\tacc = 1.0\n",
      "batch 2500\tloss = 0.0001934\tacc = 1.0\n",
      "batch 2600\tloss = 0.0004642\tacc = 1.0\n",
      "batch 2700\tloss = 0.01182\tacc = 1.0\n",
      "batch 2800\tloss = 6.435e-05\tacc = 1.0\n",
      "batch 2900\tloss = 0.000243\tacc = 1.0\n",
      "batch 3000\tloss = 0.004584\tacc = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3100\tloss = 3.283e-05\tacc = 1.0\n",
      "batch 3200\tloss = 0.0028\tacc = 1.0\n",
      "batch 3300\tloss = 3.032e-06\tacc = 1.0\n",
      "batch 3400\tloss = 4.881e-05\tacc = 1.0\n",
      "batch 3500\tloss = 1.974e-06\tacc = 1.0\n",
      "batch 3600\tloss = 0.0001843\tacc = 1.0\n",
      "batch 3700\tloss = 1.267e-07\tacc = 1.0\n",
      "epoch 5\tavg epoch loss = 0.008723\tavg epoch acc = 0.9973\n",
      "batch 100\tloss = 0.008194\tacc = 1.0\n",
      "batch 200\tloss = 4.291e-05\tacc = 1.0\n",
      "batch 300\tloss = 3.184e-05\tacc = 1.0\n",
      "batch 400\tloss = 0.0001363\tacc = 1.0\n",
      "batch 500\tloss = 6.29e-05\tacc = 1.0\n",
      "batch 600\tloss = 3.156e-05\tacc = 1.0\n",
      "batch 700\tloss = 0.001697\tacc = 1.0\n",
      "batch 800\tloss = 0.006512\tacc = 1.0\n",
      "batch 900\tloss = 0.0008288\tacc = 1.0\n",
      "batch 1000\tloss = 0.03873\tacc = 1.0\n",
      "batch 1100\tloss = 0.001326\tacc = 1.0\n",
      "batch 1200\tloss = 4.614e-05\tacc = 1.0\n",
      "batch 1300\tloss = 1.057e-05\tacc = 1.0\n",
      "batch 1400\tloss = 6.318e-06\tacc = 1.0\n",
      "batch 1500\tloss = 6.512e-06\tacc = 1.0\n",
      "batch 1600\tloss = 6.376e-05\tacc = 1.0\n",
      "batch 1700\tloss = 0.001117\tacc = 1.0\n",
      "batch 1800\tloss = 2.868e-05\tacc = 1.0\n",
      "batch 1900\tloss = 7.513e-05\tacc = 1.0\n",
      "batch 2000\tloss = 3.532e-06\tacc = 1.0\n",
      "batch 2100\tloss = 1.965e-05\tacc = 1.0\n",
      "batch 2200\tloss = 1.612e-05\tacc = 1.0\n",
      "batch 2300\tloss = 1.788e-06\tacc = 1.0\n",
      "batch 2400\tloss = 0.02416\tacc = 1.0\n",
      "batch 2500\tloss = 0.004314\tacc = 1.0\n",
      "batch 2600\tloss = 0.000159\tacc = 1.0\n",
      "batch 2700\tloss = 0.0001597\tacc = 1.0\n",
      "batch 2800\tloss = 8.846e-05\tacc = 1.0\n",
      "batch 2900\tloss = 0.005745\tacc = 1.0\n",
      "batch 3000\tloss = 0.0005153\tacc = 1.0\n",
      "batch 3100\tloss = 4.677e-05\tacc = 1.0\n",
      "batch 3200\tloss = 0.004254\tacc = 1.0\n",
      "batch 3300\tloss = 2.637e-06\tacc = 1.0\n",
      "batch 3400\tloss = 1.491e-05\tacc = 1.0\n",
      "batch 3500\tloss = 1.781e-06\tacc = 1.0\n",
      "batch 3600\tloss = 0.0001842\tacc = 1.0\n",
      "batch 3700\tloss = 7.451e-09\tacc = 1.0\n",
      "epoch 6\tavg epoch loss = 0.00634\tavg epoch acc = 0.9981\n",
      "batch 100\tloss = 6.01e-05\tacc = 1.0\n",
      "batch 200\tloss = 0.00504\tacc = 1.0\n",
      "batch 300\tloss = 8.501e-06\tacc = 1.0\n",
      "batch 400\tloss = 2.858e-05\tacc = 1.0\n",
      "batch 500\tloss = 0.0001853\tacc = 1.0\n",
      "batch 600\tloss = 0.0002955\tacc = 1.0\n",
      "batch 700\tloss = 0.0006628\tacc = 1.0\n",
      "batch 800\tloss = 0.003387\tacc = 1.0\n",
      "batch 900\tloss = 0.0002526\tacc = 1.0\n",
      "batch 1000\tloss = 0.001254\tacc = 1.0\n",
      "batch 1100\tloss = 0.01523\tacc = 1.0\n",
      "batch 1200\tloss = 0.0001303\tacc = 1.0\n",
      "batch 1300\tloss = 8.434e-06\tacc = 1.0\n",
      "batch 1400\tloss = 3.204e-07\tacc = 1.0\n",
      "batch 1500\tloss = 5.32e-05\tacc = 1.0\n",
      "batch 1600\tloss = 2.503e-06\tacc = 1.0\n",
      "batch 1700\tloss = 5.088e-05\tacc = 1.0\n",
      "batch 1800\tloss = 0.05027\tacc = 0.9375\n",
      "batch 1900\tloss = 1.624e-06\tacc = 1.0\n",
      "batch 2000\tloss = 4.724e-06\tacc = 1.0\n",
      "batch 2100\tloss = 6.914e-06\tacc = 1.0\n",
      "batch 2200\tloss = 0.002182\tacc = 1.0\n",
      "batch 2300\tloss = 1.781e-05\tacc = 1.0\n",
      "batch 2400\tloss = 0.714\tacc = 0.875\n",
      "batch 2500\tloss = 0.001753\tacc = 1.0\n",
      "batch 2600\tloss = 0.001518\tacc = 1.0\n",
      "batch 2700\tloss = 6.579e-05\tacc = 1.0\n",
      "batch 2800\tloss = 6.582e-05\tacc = 1.0\n",
      "batch 2900\tloss = 0.0001928\tacc = 1.0\n",
      "batch 3000\tloss = 0.0007572\tacc = 1.0\n",
      "batch 3100\tloss = 0.0005535\tacc = 1.0\n",
      "batch 3200\tloss = 0.0008674\tacc = 1.0\n",
      "batch 3300\tloss = 2.287e-06\tacc = 1.0\n",
      "batch 3400\tloss = 7.518e-06\tacc = 1.0\n",
      "batch 3500\tloss = 5.215e-08\tacc = 1.0\n",
      "batch 3600\tloss = 0.0003632\tacc = 1.0\n",
      "batch 3700\tloss = 1.49e-08\tacc = 1.0\n",
      "epoch 7\tavg epoch loss = 0.008128\tavg epoch acc = 0.9977\n",
      "batch 100\tloss = 0.01049\tacc = 1.0\n",
      "batch 200\tloss = 1.38e-05\tacc = 1.0\n",
      "batch 300\tloss = 2.801e-06\tacc = 1.0\n",
      "batch 400\tloss = 3.006e-05\tacc = 1.0\n",
      "batch 500\tloss = 0.0005604\tacc = 1.0\n",
      "batch 600\tloss = 4.344e-05\tacc = 1.0\n",
      "batch 700\tloss = 0.000184\tacc = 1.0\n",
      "batch 800\tloss = 0.01098\tacc = 1.0\n",
      "batch 900\tloss = 0.001567\tacc = 1.0\n",
      "batch 1000\tloss = 0.001343\tacc = 1.0\n",
      "batch 1100\tloss = 0.001021\tacc = 1.0\n",
      "batch 1200\tloss = 0.002743\tacc = 1.0\n",
      "batch 1300\tloss = 0.0003516\tacc = 1.0\n",
      "batch 1400\tloss = 0.007165\tacc = 1.0\n",
      "batch 1500\tloss = 4.942e-05\tacc = 1.0\n",
      "batch 1600\tloss = 1.421e-05\tacc = 1.0\n",
      "batch 1700\tloss = 0.000739\tacc = 1.0\n",
      "batch 1800\tloss = 3.334e-05\tacc = 1.0\n",
      "batch 1900\tloss = 9.365e-06\tacc = 1.0\n",
      "batch 2000\tloss = 1.164e-05\tacc = 1.0\n",
      "batch 2100\tloss = 2.794e-06\tacc = 1.0\n",
      "batch 2200\tloss = 0.0001086\tacc = 1.0\n",
      "batch 2300\tloss = 5.498e-06\tacc = 1.0\n",
      "batch 2400\tloss = 0.07802\tacc = 0.9375\n",
      "batch 2500\tloss = 0.0009358\tacc = 1.0\n",
      "batch 2600\tloss = 0.0008832\tacc = 1.0\n",
      "batch 2700\tloss = 2.864e-05\tacc = 1.0\n",
      "batch 2800\tloss = 5.163e-05\tacc = 1.0\n",
      "batch 2900\tloss = 7.6e-07\tacc = 1.0\n",
      "batch 3000\tloss = 0.0004112\tacc = 1.0\n",
      "batch 3100\tloss = 3.219e-06\tacc = 1.0\n",
      "batch 3200\tloss = 0.001643\tacc = 1.0\n",
      "batch 3300\tloss = 9.462e-07\tacc = 1.0\n",
      "batch 3400\tloss = 0.001361\tacc = 1.0\n",
      "batch 3500\tloss = 6.482e-07\tacc = 1.0\n",
      "batch 3600\tloss = 0.0003075\tacc = 1.0\n",
      "batch 3700\tloss = 0.0\tacc = 1.0\n",
      "epoch 8\tavg epoch loss = 0.00766\tavg epoch acc = 0.9974\n",
      "batch 100\tloss = 0.0002499\tacc = 1.0\n",
      "batch 200\tloss = 8.277e-06\tacc = 1.0\n",
      "batch 300\tloss = 3.674e-05\tacc = 1.0\n",
      "batch 400\tloss = 2.354e-06\tacc = 1.0\n",
      "batch 500\tloss = 0.001282\tacc = 1.0\n",
      "batch 600\tloss = 0.0001134\tacc = 1.0\n",
      "batch 700\tloss = 0.0002883\tacc = 1.0\n",
      "batch 800\tloss = 0.001658\tacc = 1.0\n",
      "batch 900\tloss = 0.001395\tacc = 1.0\n",
      "batch 1000\tloss = 0.05169\tacc = 0.9375\n",
      "batch 1100\tloss = 0.1635\tacc = 0.9375\n",
      "batch 1200\tloss = 8.638e-05\tacc = 1.0\n",
      "batch 1300\tloss = 0.0004969\tacc = 1.0\n",
      "batch 1400\tloss = 5.811e-07\tacc = 1.0\n",
      "batch 1500\tloss = 9.148e-05\tacc = 1.0\n",
      "batch 1600\tloss = 8.159e-05\tacc = 1.0\n",
      "batch 1700\tloss = 1.784e-05\tacc = 1.0\n",
      "batch 1800\tloss = 0.2373\tacc = 0.9375\n",
      "batch 1900\tloss = 3.576e-07\tacc = 1.0\n",
      "batch 2000\tloss = 8.139e-05\tacc = 1.0\n",
      "batch 2100\tloss = 0.0001442\tacc = 1.0\n",
      "batch 2200\tloss = 1.081e-05\tacc = 1.0\n",
      "batch 2300\tloss = 3.874e-07\tacc = 1.0\n",
      "batch 2400\tloss = 0.001816\tacc = 1.0\n",
      "batch 2500\tloss = 0.001139\tacc = 1.0\n",
      "batch 2600\tloss = 2.121e-05\tacc = 1.0\n",
      "batch 2700\tloss = 5.896e-05\tacc = 1.0\n",
      "batch 2800\tloss = 0.001199\tacc = 1.0\n",
      "batch 2900\tloss = 0.0004404\tacc = 1.0\n",
      "batch 3000\tloss = 0.0004569\tacc = 1.0\n",
      "batch 3100\tloss = 0.0009957\tacc = 1.0\n",
      "batch 3200\tloss = 0.0001014\tacc = 1.0\n",
      "batch 3300\tloss = 7.972e-07\tacc = 1.0\n",
      "batch 3400\tloss = 0.0001699\tacc = 1.0\n",
      "batch 3500\tloss = 2.593e-06\tacc = 1.0\n",
      "batch 3600\tloss = 1.883e-05\tacc = 1.0\n",
      "batch 3700\tloss = 0.0\tacc = 1.0\n",
      "epoch 9\tavg epoch loss = 0.006875\tavg epoch acc = 0.9977\n",
      "training took 76.19 s\n",
      "Avg test loss = 0.0388\tAvg test acc = 0.99\n"
     ]
    }
   ],
   "source": [
    "mini_opt_naive = MiniBatchOptimizer(net_naive.parameters(), lr=learning_rate, decreasing_lr=decreasing_lr)\n",
    "loss_train, acc_train = training(net_naive, train_loader, mini_opt_naive, criterion, accuracy, epochs=epochs, device=device)\n",
    "loss_test, acc_test = testing(net_naive, test_loader, criterion, accuracy, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.00\tTest Accuracy = 0.979\n",
      "Epsilon: 0.05\tTest Accuracy = 0.976\n",
      "Epsilon: 0.10\tTest Accuracy = 0.970\n",
      "Epsilon: 0.15\tTest Accuracy = 0.963\n",
      "Epsilon: 0.20\tTest Accuracy = 0.954\n",
      "Epsilon: 0.25\tTest Accuracy = 0.941\n",
      "Epsilon: 0.30\tTest Accuracy = 0.920\n",
      "Epsilon: 0.35\tTest Accuracy = 0.897\n",
      "Epsilon: 0.40\tTest Accuracy = 0.863\n",
      "Epsilon: 0.45\tTest Accuracy = 0.818\n"
     ]
    }
   ],
   "source": [
    "accuracy_naive= []\n",
    "losses_naive= []\n",
    "\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack  = attack(net_naive, criterion, test_loader, epsilon=eps, device=device)\n",
    "    accuracy_naive.append(acc_attack)\n",
    "    losses_naive.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Robust Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_net = Net().to(device)\n",
    "protect_epochs = epochs\n",
    "protect_lr = learning_rate\n",
    "protect_bz = batch_size\n",
    "protect_dec_lr = decreasing_lr\n",
    "prot_train_loader, prot_test_loader = build_data_loaders(train_dataset, test_dataset, protect_bz)\n",
    "mini_opt_proc = MiniBatchOptimizer(robust_net.parameters(), lr=protect_lr, decreasing_lr=protect_dec_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the protect function to make the model robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0.00 | Test accuracy: 0.97810\n",
      "Epoch 1.00 | Test accuracy: 0.98420\n",
      "Epoch 2.00 | Test accuracy: 0.98730\n",
      "Epoch 3.00 | Test accuracy: 0.98760\n",
      "Epoch 4.00 | Test accuracy: 0.98740\n",
      "Epoch 5.00 | Test accuracy: 0.98710\n",
      "Epoch 6.00 | Test accuracy: 0.98710\n",
      "Epoch 7.00 | Test accuracy: 0.98710\n",
      "Epoch 8.00 | Test accuracy: 0.98680\n",
      "Epoch 9.00 | Test accuracy: 0.98790\n",
      "training took 140.8 s\n"
     ]
    }
   ],
   "source": [
    "robust_net = protect(robust_net, mini_opt_proc, criterion, prot_train_loader, prot_test_loader, device=device, epochs=protect_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack the Robust Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.00\tTest Accuracy = 0.991\n",
      "Epsilon: 0.05\tTest Accuracy = 0.990\n",
      "Epsilon: 0.10\tTest Accuracy = 0.989\n",
      "Epsilon: 0.15\tTest Accuracy = 0.987\n",
      "Epsilon: 0.20\tTest Accuracy = 0.983\n",
      "Epsilon: 0.25\tTest Accuracy = 0.977\n",
      "Epsilon: 0.30\tTest Accuracy = 0.972\n",
      "Epsilon: 0.35\tTest Accuracy = 0.962\n",
      "Epsilon: 0.40\tTest Accuracy = 0.948\n",
      "Epsilon: 0.45\tTest Accuracy = 0.924\n"
     ]
    }
   ],
   "source": [
    "accuracy_robust = []\n",
    "losses_robust = []\n",
    "# This should be the first term test_loader is used\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack = attack(robust_net, criterion, prot_test_loader, eps, device=device)\n",
    "    accuracy_robust.append(acc_attack)\n",
    "    losses_robust.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis of the Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracy_naive, \"*-\", c='blue', label='Naive Model')\n",
    "plt.plot(epsilons, accuracy_robust, \"*-\", c='orange', label='Robust Model')\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, 0.5, step=0.05))\n",
    "\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
