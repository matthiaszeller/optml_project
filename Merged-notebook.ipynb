{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "filled-portsmouth",
   "metadata": {},
   "source": [
    "# guidelines\n",
    "\n",
    "TODO : import whenever needed, not centralized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-cylinder",
   "metadata": {},
   "source": [
    "states https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-poverty",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-runner",
   "metadata": {},
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-alfred",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-middle",
   "metadata": {},
   "source": [
    "# Import (Remove section later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "residential-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "saving-scroll",
   "metadata": {},
   "source": [
    "from adversary import attack, protect\n",
    "from net import Net\n",
    "from torch.optim import Optimizer\n",
    "from training import training, testing, accuracy, tune_optimizer\n",
    "from minibatch import MiniBatchOptimizer\n",
    "from adam import AdamOptimizer\n",
    "from data_utils import get_mnist, build_data_loaders\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-spectacular",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-socket",
   "metadata": {},
   "source": [
    "Below one can find flags that will setup the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "trained-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to tune the hyperparameters in this notebook\n",
    "# Note that this might take a long time (especially for Adam)\n",
    "hyperparameter_tune = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "foreign-directive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device chosen is cuda\n"
     ]
    }
   ],
   "source": [
    "# Whether to use the GPU, if it's not available, this will be ignored\n",
    "use_cuda = True\n",
    "\n",
    "device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device chosen is {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-joyce",
   "metadata": {},
   "source": [
    "We now load the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-adrian",
   "metadata": {},
   "source": [
    "**TODO add data downloading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classified-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confident-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_mnist\n",
    "\n",
    "train_dataset, test_dataset = get_mnist(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-theology",
   "metadata": {},
   "source": [
    "We setup the training parameters that we will use all along the notebook, in order to improve readability in downstream code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flush-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import accuracy\n",
    "\n",
    "training_config = {\n",
    "    # Loss function\n",
    "    'loss_fun': torch.nn.CrossEntropyLoss(),\n",
    "    # Performance evaluation function\n",
    "    'metric_fun': accuracy,\n",
    "    # The device to train on\n",
    "    'device': device,\n",
    "    # Number of epochs\n",
    "    'epochs': 10\n",
    "}\n",
    "\n",
    "test_config = training_config.copy()\n",
    "test_config.pop('epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-table",
   "metadata": {},
   "source": [
    "Note that we will use a model with a 10-dimensional output, where each output is passed through softmax. When receiving an output \n",
    "\n",
    "$$Z = \\begin{bmatrix} \\mathbf z_1 & \\dots & \\mathbf z_B \\end{bmatrix}^\\top \\in \\mathbb R^{B \\times 10}$$\n",
    "\n",
    "with $B$ the batch size, we first retrieve the maximal component of each $\\mathbf z_i$:\n",
    "\n",
    "$$\\hat y_i = \\text{argmax}_{k = 1, \\ldots, 10} \\; z_{ik}, \\quad i = 1, \\ldots, B$$\n",
    "\n",
    "and then compute the accuracy:\n",
    "\n",
    "$$\\text{acc} = \\frac 1 B \\sum_{i=1}^B I\\left\\{ \\hat y_i = y_i \\right\\} $$\n",
    "\n",
    "with $I$ the indicator function and $y_i \\in \\{1, \\ldots, 10\\}$ the true target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "further-image",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /media/maousi/Data/Documents/Programmation/courses/DS-MA2/optml_project/training.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the source code\n",
    "??accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-danger",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-watershed",
   "metadata": {},
   "source": [
    "We use a simple standard model for the MNIST dataset (can be found [here](https://github.com/floydhub/mnist/blob/master/ConvNet.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "changing-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unlikely-columbia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"ConvNet -> Max_Pool -> RELU -> ConvNet -> Max_Pool -> RELU -> FC -> RELU -> FC -> SOFTMAX\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           /media/maousi/Data/Documents/Programmation/courses/DS-MA2/optml_project/net.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-silver",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "integrated-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import tune_optimizer\n",
    "from optimizer import AdamOptimizer, NesterovOptimizer, MiniBatchOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-encounter",
   "metadata": {},
   "source": [
    "If the `hyperparameter_tune` flag was set to `True` above, the following code will run hyperparameter tuning on all optimizers. Note that one can either run KFold cross validation (by providing `n_folds`) or use a simple train/test split (by providing `train_ratio`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-lease",
   "metadata": {},
   "source": [
    "If the flag is set to `False`, the cell below will simply set up the hyperparameters that we carefully cross-validated. Check the notebook **Hyperparam-tuning.ipynb** for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expired-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-define best parameters, used if hyperparameter_tune = False\n",
    "optimizers = {\n",
    "    AdamOptimizer: {'lr': 8e-05, 'beta1': 0.9, 'beta2': 0.999, 'weight_decay': 0.01, 'epsilon': 1e-08, 'batch_size': 32},\n",
    "    NesterovOptimizer: {'lr': 5e-05, 'batch_size': 64},\n",
    "    MiniBatchOptimizer: {'lr': 0.26389342601937466, 'decreasing_lr': False, 'batch_size': 128}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-complexity",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "metropolitan-importance",
   "metadata": {},
   "source": [
    "def find_best_params_adam(adam_tuning):\n",
    "    \"\"\"Given the output of tune_optimizer, return a dictionnary of the best parameters.\"\"\"\n",
    "    best_params = dict()\n",
    "    best_params['loss_test'] = float('inf')\n",
    "    best_params['acc_test'] = -float('inf')\n",
    "\n",
    "        \n",
    "    #IF STD, divide metrics by std\n",
    "    #Wait for the new tuning\n",
    "    for item in adam_tuning:\n",
    "        if best_params['acc_test'] < item['metric_test'] or best_params['loss_test'] > item['loss_test']:\n",
    "            best_params['loss_train'] = item['loss_train']\n",
    "            best_params['acc_train'] = item['metric_train']\n",
    "            best_params['acc_test'] = item['metric_test']\n",
    "            best_params['loss_test'] = item['loss_test']\n",
    "            best_params['lr'] = item['lr']\n",
    "            best_params['beta1'] = item['beta1']\n",
    "            best_params['beta2'] = item['beta2']\n",
    "            best_params['weight_decay'] = item['weight_decay']\n",
    "            best_params['epsilon'] = item['epsilon']\n",
    "            \n",
    "    return best_params\n",
    "best_adam_params = find_best_params_adam(adam_tuning)\n",
    "print_adam_stats(best_adam_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-banner",
   "metadata": {},
   "source": [
    "## Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "natural-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import NesterovOptimizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "musical-courtesy",
   "metadata": {},
   "source": [
    "search_grid_nesterov = {\n",
    "    'lr': np.logspace(0, 1),\n",
    "    'batch_size': [32, 64, 128]\n",
    "}\n",
    "\n",
    "if hyperparameter_tune:\n",
    "    results_nesterov = tune_optimizer(\n",
    "        model=Net().to(device),\n",
    "        optim_fun=NesterovOptimizer,\n",
    "        xtrain=train_dataset.data,\n",
    "        ytrain=train_dataset.targets,\n",
    "        search_grid=search_grid_nesterov,\n",
    "        nfolds=5,\n",
    "        **training_config\n",
    "    )\n",
    "\n",
    "# Generic function to select best config\n",
    "# so something like\n",
    "# optimizers[NesterovOptimizer] = select_best_params(results_nesterov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-chicken",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fitting-intranet",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "net_tune = Net().to(device)\n",
    "adam_tune = AdamOptimizer(net_tune.parameters()) \n",
    "fp = './res/adam_tuning.json'\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fabulous-minute",
   "metadata": {},
   "source": [
    "if not hyperparameter_tune:\n",
    "    results = []\n",
    "else:\n",
    "    results = tune_optimizer(\n",
    "    net_tune,\n",
    "    train_dataset.data,\n",
    "    train_dataset.targets,\n",
    "    criterion,\n",
    "    accuracy,\n",
    "    device,\n",
    "    AdamOptimizer,\n",
    "    epochs=10,\n",
    "    search_grid={\n",
    "        'lr': np.linspace(0.00001, 0.01, 5),\n",
    "        'beta1':  np.linspace(0.1, 0.9, 5),\n",
    "        'beta2': np.linspace(0.5, 0.999, 5),\n",
    "        'weight_decay': np.linspace(0.0001, 0.1, 4),\n",
    "        'epsilon': np.linspace(1e-10, 1e-8, 3),\n",
    "    }\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "unnecessary-particle",
   "metadata": {},
   "source": [
    "if Path(fp).exists():\n",
    "    with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)\n",
    "\n",
    "    results = old_results + results\n",
    "\n",
    "with open(fp, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Select Best Hyperparamters\n",
    "with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-angola",
   "metadata": {},
   "source": [
    "## Minibatch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "specialized-governor",
   "metadata": {},
   "source": [
    "net_tune = Net().to(device)\n",
    "mini_opt_tune = MiniBatchOptimizer(net_tune.parameters()) # Just using defaults\n",
    "dec_lr_set =  [0]*1 + [1]*1\n",
    "random.shuffle(dec_lr_set)\n",
    "fp = './res/mini_tuning.json'\n",
    "if not hyperparamter_tune:\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "meaningful-minutes",
   "metadata": {},
   "source": [
    "if hyperparamter_tune:\n",
    "    results = tune_optimizer(\n",
    "    net_tune,\n",
    "    train_dataset.data,\n",
    "    train_dataset.targets,\n",
    "    criterion,\n",
    "    accuracy,\n",
    "    device,\n",
    "    MiniBatchOptimizer,\n",
    "    epochs=10,\n",
    "    search_grid={\n",
    "        'lr': np.linspace(0.00001, 0.01, 5),\n",
    "        'decreasing_lr': dec_lr_set,\n",
    "    }, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "running-parish",
   "metadata": {},
   "source": [
    "if Path(fp).exists():\n",
    "    with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)\n",
    "\n",
    "    results = old_results + results\n",
    "\n",
    "with open(fp, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Select Best Hyperparamters\n",
    "with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "classical-console",
   "metadata": {},
   "source": [
    "df_analysis = pd.DataFrame(results)\n",
    "best_acc = 0.0\n",
    "for index, row in df_analysis.iterrows():    \n",
    "        trial_acc = row[\"metric_test\"]\n",
    "        if trial_acc > best_acc:\n",
    "            best_acc = trial_acc\n",
    "            learning_rate = round(row[\"lr\"], 6)\n",
    "            decreasing_lr = row[\"decreasing_lr\"]\n",
    "\n",
    "print(\"Best Accuracy was {}% with Learning Rate {} and Decreasing LR: {}\".format(100*best_acc, learning_rate, decreasing_lr))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sophisticated-taylor",
   "metadata": {},
   "source": [
    "# lst_optimizer = {'name': function_optimizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-extraction",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-standing",
   "metadata": {},
   "source": [
    "# Attack on naive model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conventional-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import build_data_loaders\n",
    "from training import training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-girlfriend",
   "metadata": {},
   "source": [
    "## Train naive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "august-twenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- <class 'optimizer.AdamOptimizer'>\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 0.8383\tavg epoch acc = 0.7607\n",
      "epoch 1\tavg epoch loss = 0.1175\tavg epoch acc = 0.9645\n",
      "epoch 2\tavg epoch loss = 0.06908\tavg epoch acc = 0.9785\n",
      "epoch 3\tavg epoch loss = 0.0502\tavg epoch acc = 0.9847\n",
      "epoch 4\tavg epoch loss = 0.0386\tavg epoch acc = 0.988\n",
      "epoch 5\tavg epoch loss = 0.03044\tavg epoch acc = 0.9908\n",
      "epoch 6\tavg epoch loss = 0.02454\tavg epoch acc = 0.993\n",
      "epoch 7\tavg epoch loss = 0.0198\tavg epoch acc = 0.9947\n",
      "epoch 8\tavg epoch loss = 0.01592\tavg epoch acc = 0.9959\n",
      "epoch 9\tavg epoch loss = 0.01274\tavg epoch acc = 0.9968\n",
      "training took 43.96 s\n",
      "Avg test loss = 0.0334\tAvg test acc = 0.989\n",
      "--- <class 'optimizer.NesterovOptimizer'>\n",
      "Launching training on cuda\n",
      "epoch 0\tavg epoch loss = 1.415\tavg epoch acc = 0.6081\n",
      "epoch 1\tavg epoch loss = 0.2533\tavg epoch acc = 0.9273\n",
      "epoch 2\tavg epoch loss = 0.1027\tavg epoch acc = 0.9681\n",
      "epoch 3\tavg epoch loss = 0.06678\tavg epoch acc = 0.9791\n",
      "epoch 4\tavg epoch loss = 0.04783\tavg epoch acc = 0.9848\n",
      "epoch 5\tavg epoch loss = 0.03374\tavg epoch acc = 0.989\n",
      "epoch 6\tavg epoch loss = 0.02604\tavg epoch acc = 0.9917\n",
      "epoch 7\tavg epoch loss = 0.02206\tavg epoch acc = 0.9931\n",
      "epoch 8\tavg epoch loss = 0.02163\tavg epoch acc = 0.993\n",
      "epoch 9\tavg epoch loss = 0.01991\tavg epoch acc = 0.9933\n",
      "training took 19.18 s\n",
      "Avg test loss = 0.0359\tAvg test acc = 0.989\n"
     ]
    }
   ],
   "source": [
    "naive_networks = dict()\n",
    "data_naive = list()\n",
    "batch_log_interval = 0\n",
    "\n",
    "for optimizer, optimizer_params in optimizers.items():\n",
    "    print(f'--- {optimizer}')\n",
    "    optimizer_params = optimizer_params.copy()\n",
    "    \n",
    "    net = Net().to(device)\n",
    "    # Instantiate data loaders with selected batch size\n",
    "    batch_size = optimizer_params.pop('batch_size')\n",
    "    train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)\n",
    "    # Instantiate optimizer\n",
    "    optimizer_instance = optimizer(net.parameters(), **optimizer_params)\n",
    "    # Train\n",
    "    loss_train, acc_train = training(\n",
    "        model=net, \n",
    "        dataset=train_loader, \n",
    "        optim=optimizer_instance,\n",
    "        batch_log_interval=batch_log_interval,\n",
    "        **training_config\n",
    "    )\n",
    "    # Test\n",
    "    loss_test, acc_test = testing(\n",
    "        model=net,\n",
    "        dataset=test_loader,\n",
    "        **test_config\n",
    "    )\n",
    "    # Log\n",
    "    data_naive.append({\n",
    "        'optimizer': str(optimizer),\n",
    "        'loss_train': loss_train,\n",
    "        'acc_train': acc_train,\n",
    "        'loss_test': loss_test,\n",
    "        'acc_test': acc_test\n",
    "    })\n",
    "    # Save naive model\n",
    "    naive_networks[optimizer] = net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-finger",
   "metadata": {},
   "source": [
    "### Minibatch (for now, loop later)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "varied-highland",
   "metadata": {},
   "source": [
    "net_naive = Net().to(device)\n",
    "train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "declared-pipeline",
   "metadata": {},
   "source": [
    "mini_opt_naive = MiniBatchOptimizer(net_naive.parameters(), lr=learning_rate, decreasing_lr=decreasing_lr)\n",
    "loss_train, acc_train = training(net_naive, train_loader, mini_opt_naive, criterion, accuracy, epochs=epochs, device=device)\n",
    "loss_test, acc_test = testing(net_naive, test_loader, criterion, accuracy, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-substitute",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "mature-conditions",
   "metadata": {},
   "source": [
    "net_naive_adam = Net().to(device)\n",
    "train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "homeless-treatment",
   "metadata": {},
   "source": [
    "adam_opt_naive = AdamOptimizer(net_naive.parameters(), lr=best_adam_params['lr'], beta1=best_adam_params['beta1'],beta2=best_adam_params['beta2'],weight_decay=best_adam_params['weight_decay'],epsilon=best_adam_params['epsilon'])\n",
    "loss_train, acc_train = training(net_naive, train_loader, adam_opt_naive, criterion, accuracy, epochs=epochs, device=device)\n",
    "loss_test, acc_test = testing(net_naive, test_loader, criterion, accuracy, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-print",
   "metadata": {},
   "source": [
    "## Nesterov\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-hacker",
   "metadata": {},
   "source": [
    "## Attack naive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nearby-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversary import attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "increasing-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.arange(0, 0.5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reasonable-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the lst_optimizer\n",
    "# Only one optimizer used in this part?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-thunder",
   "metadata": {},
   "source": [
    "### Minibatch (for now, loop later)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "saved-mozambique",
   "metadata": {},
   "source": [
    "accuracy_naive= []\n",
    "losses_naive= []\n",
    "\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack  = attack(net_naive, criterion, test_loader, epsilon=eps, device=device)\n",
    "    accuracy_naive.append(acc_attack)\n",
    "    losses_naive.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-findings",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "moving-barbados",
   "metadata": {},
   "source": [
    "accuracy_naive_adam= []\n",
    "losses_naive_adam= []\n",
    "\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack  = attack(net_naive_adam, criterion, test_loader, epsilon=eps, device=device)\n",
    "    accuracy_naive_adam.append(acc_attack)\n",
    "    losses_naive_adam.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-connection",
   "metadata": {},
   "source": [
    "### Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "facial-substance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_fun': CrossEntropyLoss(),\n",
       " 'metric_fun': <function training.accuracy(yhat, y)>,\n",
       " 'device': device(type='cuda')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "continued-combination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- <class 'optimizer.AdamOptimizer'>\n",
      "Epsilon: 0.00\tTest Accuracy = 0.941\n",
      "Epsilon: 0.05\tTest Accuracy = 0.934\n",
      "Epsilon: 0.10\tTest Accuracy = 0.925\n",
      "Epsilon: 0.15\tTest Accuracy = 0.914\n",
      "Epsilon: 0.20\tTest Accuracy = 0.898\n",
      "Epsilon: 0.25\tTest Accuracy = 0.876\n",
      "Epsilon: 0.30\tTest Accuracy = 0.849\n",
      "Epsilon: 0.35\tTest Accuracy = 0.806\n",
      "Epsilon: 0.40\tTest Accuracy = 0.755\n",
      "Epsilon: 0.45\tTest Accuracy = 0.689\n",
      "--- <class 'optimizer.NesterovOptimizer'>\n",
      "Epsilon: 0.00\tTest Accuracy = 0.973\n",
      "Epsilon: 0.05\tTest Accuracy = 0.968\n",
      "Epsilon: 0.10\tTest Accuracy = 0.960\n",
      "Epsilon: 0.15\tTest Accuracy = 0.951\n",
      "Epsilon: 0.20\tTest Accuracy = 0.939\n",
      "Epsilon: 0.25\tTest Accuracy = 0.920\n",
      "Epsilon: 0.30\tTest Accuracy = 0.894\n",
      "Epsilon: 0.35\tTest Accuracy = 0.856\n",
      "Epsilon: 0.40\tTest Accuracy = 0.806\n",
      "Epsilon: 0.45\tTest Accuracy = 0.744\n"
     ]
    }
   ],
   "source": [
    "data_naive = list()\n",
    "\n",
    "for optimizer, network in naive_networks.items():\n",
    "    print(f'--- {optimizer}')\n",
    "    \n",
    "    for eps in epsilons:\n",
    "        loss_attack, acc_attack = attack(\n",
    "            model=network, \n",
    "            test_loader=test_loader, \n",
    "            epsilon=eps,\n",
    "            **test_config\n",
    "        )\n",
    "        # Log\n",
    "        data_naive.append({\n",
    "            'optimizer': str(optimizer),\n",
    "            'epsilon': eps,\n",
    "            'loss': loss_attack,\n",
    "            'acc': acc_attack\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-terrace",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-disabled",
   "metadata": {},
   "source": [
    "**TODO COMPARE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-africa",
   "metadata": {},
   "source": [
    "# Attack on robust model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-palestinian",
   "metadata": {},
   "source": [
    "## Train robust models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversary import protect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-champagne",
   "metadata": {},
   "source": [
    "### Minibatch (for now, loop later)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "becoming-pasta",
   "metadata": {},
   "source": [
    "robust_net = Net().to(device)\n",
    "protect_epochs = epochs\n",
    "protect_lr = learning_rate\n",
    "protect_bz = batch_size\n",
    "protect_dec_lr = decreasing_lr\n",
    "prot_train_loader, prot_test_loader = build_data_loaders(train_dataset, test_dataset, protect_bz)\n",
    "mini_opt_proc = MiniBatchOptimizer(robust_net.parameters(), lr=protect_lr, decreasing_lr=protect_dec_lr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "corresponding-assignment",
   "metadata": {},
   "source": [
    "robust_net = protect(robust_net, mini_opt_proc, criterion, prot_train_loader, prot_test_loader, device=device, epochs=protect_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-mississippi",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fewer-victory",
   "metadata": {},
   "source": [
    "robust_net = Net().to(device)\n",
    "protect_epochs = epochs\n",
    "protect_lr = learning_rate\n",
    "protect_bz = batch_size\n",
    "\n",
    "prot_train_loader, prot_test_loader = build_data_loaders(train_dataset, test_dataset, protect_bz)\n",
    "adam_opt_proc = AdamOptimizer(net_naive.parameters(), lr=best_adam_params['lr'], beta1=best_adam_params['beta1'],beta2=best_adam_params['beta2'],weight_decay=best_adam_params['weight_decay'],epsilon=best_adam_params['epsilon'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "front-interim",
   "metadata": {},
   "source": [
    "robust_net_adam = protect(robust_net, adam_opt_proc, criterion, prot_train_loader, prot_test_loader, device=device, epochs=protect_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-insulation",
   "metadata": {},
   "source": [
    "## Nesterov\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "partial-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'optimizer.AdamOptimizer'>\n",
      "Epoch 0.00 | Test accuracy: 0.88648\n",
      "Epoch 1.00 | Test accuracy: 0.92442\n",
      "Epoch 2.00 | Test accuracy: 0.94109\n",
      "Epoch 3.00 | Test accuracy: 0.94599\n",
      "Epoch 4.00 | Test accuracy: 0.95068\n",
      "Epoch 5.00 | Test accuracy: 0.95637\n",
      "Epoch 6.00 | Test accuracy: 0.95917\n",
      "Epoch 7.00 | Test accuracy: 0.95996\n",
      "Epoch 8.00 | Test accuracy: 0.96046\n",
      "Epoch 9.00 | Test accuracy: 0.96166\n",
      "training took 67.39 s\n",
      "<class 'optimizer.NesterovOptimizer'>\n",
      "Epoch 0.00 | Test accuracy: 0.68521\n",
      "Epoch 1.00 | Test accuracy: 0.92645\n",
      "Epoch 2.00 | Test accuracy: 0.95969\n",
      "Epoch 3.00 | Test accuracy: 0.97870\n",
      "Epoch 4.00 | Test accuracy: 0.97980\n",
      "Epoch 5.00 | Test accuracy: 0.98497\n",
      "Epoch 6.00 | Test accuracy: 0.98507\n",
      "Epoch 7.00 | Test accuracy: 0.98318\n",
      "Epoch 8.00 | Test accuracy: 0.98816\n",
      "Epoch 9.00 | Test accuracy: 0.98776\n",
      "training took 30.92 s\n"
     ]
    }
   ],
   "source": [
    "robust_networks = dict()\n",
    "batch_log_interval = 0\n",
    "epsilon = 0.25\n",
    "\n",
    "for optimizer, optimizer_params in optimizers.items():\n",
    "    print(optimizer)\n",
    "    # Instantiate model\n",
    "    net = Net().to(device)\n",
    "    # Instantiate optimizer\n",
    "    optimizer_params = optimizer_params.copy()\n",
    "    batch_size = optimizer_params.pop('batch_size')\n",
    "    optimizer_instance = optimizer(net.parameters(), **optimizer_params)\n",
    "    # Instantiate data loaders\n",
    "    train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)\n",
    "    # Train robust model\n",
    "    protect(\n",
    "        model=net,\n",
    "        optim=optimizer_instance,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epsilon=epsilon,\n",
    "        **training_config\n",
    "    )\n",
    "    # Save robust net\n",
    "    robust_networks[optimizer] = net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-textbook",
   "metadata": {},
   "source": [
    "## Attack robust models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-worthy",
   "metadata": {},
   "source": [
    "### Minibatch (for now, loop later)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "nonprofit-transparency",
   "metadata": {},
   "source": [
    "accuracy_robust = []\n",
    "losses_robust = []\n",
    "# This should be the first term test_loader is used\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack = attack(robust_net, criterion, prot_test_loader, eps, device=device)\n",
    "    accuracy_robust.append(acc_attack)\n",
    "    losses_robust.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-class",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "internal-private",
   "metadata": {},
   "source": [
    "accuracy_robust_adam = []\n",
    "losses_robust_adam = []\n",
    "# This should be the first term test_loader is used\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack = attack(robust_net_adam, criterion, prot_test_loader, eps, device=device)\n",
    "    accuracy_robust_adam.append(acc_attack)\n",
    "    losses_robust_adam.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-barbados",
   "metadata": {},
   "source": [
    "**TODO put the loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-spoke",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-player",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-structure",
   "metadata": {},
   "source": [
    "# Attack 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-directive",
   "metadata": {},
   "source": [
    "# Comparative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-florida",
   "metadata": {},
   "source": [
    "### Minibatch (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "through-external",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_naive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c1c6b3b1a8b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_naive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Naive Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_robust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Robust Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_naive' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracy_naive, \"*-\", c='blue', label='Naive Model')\n",
    "plt.plot(epsilons, accuracy_robust, \"*-\", c='orange', label='Robust Model')\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, 0.5, step=0.05))\n",
    "\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-clock",
   "metadata": {},
   "source": [
    "Lots of plots\n",
    "\n",
    "* diff naive vs robust (algo as hue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
