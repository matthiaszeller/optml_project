{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# guidelines\n",
    "\n",
    "TODO : import whenever needed, not centralized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "states https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import (Remove section later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversary import attack, protect\n",
    "from net import Net\n",
    "import numpy as np\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "from training import training, testing, accuracy, tune_optimizer\n",
    "from minibatch import MiniBatchOptimizer\n",
    "from adam import AdamOptimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import get_mnist, build_data_loaders\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags\n",
    "# use cuda = True\n",
    "# tune_hyperparam = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device chosen is {}\".format(device))\n",
    "train_dataset, test_dataset = get_mnist(normalize=True)\n",
    "epsilons = np.arange(0, 0.5, 0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model\n",
    "\n",
    "# net = fhreifg\n",
    "# initial_state = net.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters that are in common batch size, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tune = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tune = Net().to(device)\n",
    "adam_tune = AdamOptimizer(net_tune.parameters()) \n",
    "fp = './res/adam_tuning.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hyperparameter_tune:\n",
    "    results = []\n",
    "else:\n",
    "    results = tune_optimizer(\n",
    "    net_tune,\n",
    "    train_dataset.data,\n",
    "    train_dataset.targets,\n",
    "    criterion,\n",
    "    accuracy,\n",
    "    device,\n",
    "    AdamOptimizer,\n",
    "    epochs=10,\n",
    "    search_grid={\n",
    "        'lr': np.linspace(0.00001, 0.01, 5),\n",
    "        'beta1':  np.linspace(0.1, 0.9, 5),\n",
    "        'beta2': np.linspace(0.5, 0.999, 5),\n",
    "        'weight_decay': np.linspace(0.0001, 0.1, 4),\n",
    "        'epsilon': np.linspace(1e-10, 1e-8, 3),\n",
    "    }\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(fp).exists():\n",
    "    with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)\n",
    "\n",
    "    results = old_results + results\n",
    "\n",
    "with open(fp, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Select Best Hyperparamters\n",
    "with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params_adam(adam_tuning):\n",
    "    best_params = dict()\n",
    "    best_params['loss_test'] = float('inf')\n",
    "    best_params['acc_test'] = -float('inf')\n",
    "\n",
    "        \n",
    "    #IF STD, divide metrics by std\n",
    "    #Wait for the new tuning\n",
    "    for item in adam_tuning:\n",
    "        if best_params['acc_test'] < item['metric_test'] or best_params['loss_test'] > item['loss_test']:\n",
    "            best_params['loss_train'] = item['loss_train']\n",
    "            best_params['acc_train'] = item['metric_train']\n",
    "            best_params['acc_test'] = item['metric_test']\n",
    "            best_params['loss_test'] = item['loss_test']\n",
    "            best_params['lr'] = item['lr']\n",
    "            best_params['beta1'] = item['beta1']\n",
    "            best_params['beta2'] = item['beta2']\n",
    "            best_params['weight_decay'] = item['weight_decay']\n",
    "            best_params['epsilon'] = item['epsilon']\n",
    "            \n",
    "    return best_params\n",
    "best_adam_params = find_best_params_adam(adam_tuning)\n",
    "print_adam_stats(best_adam_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tune = Net().to(device)\n",
    "mini_opt_tune = MiniBatchOptimizer(net_tune.parameters()) # Just using defaults\n",
    "dec_lr_set =  [0]*1 + [1]*1\n",
    "random.shuffle(dec_lr_set)\n",
    "fp = './res/mini_tuning.json'\n",
    "if not hyperparamter_tune:\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparamter_tune:\n",
    "    results = tune_optimizer(\n",
    "    net_tune,\n",
    "    train_dataset.data,\n",
    "    train_dataset.targets,\n",
    "    criterion,\n",
    "    accuracy,\n",
    "    device,\n",
    "    MiniBatchOptimizer,\n",
    "    epochs=10,\n",
    "    search_grid={\n",
    "        'lr': np.linspace(0.00001, 0.01, 5),\n",
    "        'decreasing_lr': dec_lr_set,\n",
    "    }, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(fp).exists():\n",
    "    with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)\n",
    "\n",
    "    results = old_results + results\n",
    "\n",
    "with open(fp, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Select Best Hyperparamters\n",
    "with open(fp, 'r') as f:\n",
    "        old_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.DataFrame(results)\n",
    "best_acc = 0.0\n",
    "for index, row in df_analysis.iterrows():    \n",
    "        trial_acc = row[\"metric_test\"]\n",
    "        if trial_acc > best_acc:\n",
    "            best_acc = trial_acc\n",
    "            learning_rate = round(row[\"lr\"], 6)\n",
    "            decreasing_lr = row[\"decreasing_lr\"]\n",
    "\n",
    "print(\"Best Accuracy was {}% with Learning Rate {} and Decreasing LR: {}\".format(100*best_acc, learning_rate, decreasing_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_optimizer = {'name': function_optimizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack on naive model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train naive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch (for now, loop later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_naive = Net().to(device)\n",
    "train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_opt_naive = MiniBatchOptimizer(net_naive.parameters(), lr=learning_rate, decreasing_lr=decreasing_lr)\n",
    "loss_train, acc_train = training(net_naive, train_loader, mini_opt_naive, criterion, accuracy, epochs=epochs, device=device)\n",
    "loss_test, acc_test = testing(net_naive, test_loader, criterion, accuracy, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_naive_adam = Net().to(device)\n",
    "train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt_naive = AdamOptimizer(net_naive.parameters(), lr=best_adam_params['lr'], beta1=best_adam_params['beta1'],beta2=best_adam_params['beta2'],weight_decay=best_adam_params['weight_decay'],epsilon=best_adam_params['epsilon'])\n",
    "loss_train, acc_train = training(net_naive, train_loader, adam_opt_naive, criterion, accuracy, epochs=epochs, device=device)\n",
    "loss_test, acc_test = testing(net_naive, test_loader, criterion, accuracy, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack naive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the lst_optimizer\n",
    "# Only one optimizer used in this part?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch (for now, loop later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_naive= []\n",
    "losses_naive= []\n",
    "\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack  = attack(net_naive, criterion, test_loader, epsilon=eps, device=device)\n",
    "    accuracy_naive.append(acc_attack)\n",
    "    losses_naive.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_naive_adam= []\n",
    "losses_naive_adam= []\n",
    "\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack  = attack(net_naive_adam, criterion, test_loader, epsilon=eps, device=device)\n",
    "    accuracy_naive_adam.append(acc_attack)\n",
    "    losses_naive_adam.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack on robust model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train robust models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch (for now, loop later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_net = Net().to(device)\n",
    "protect_epochs = epochs\n",
    "protect_lr = learning_rate\n",
    "protect_bz = batch_size\n",
    "protect_dec_lr = decreasing_lr\n",
    "prot_train_loader, prot_test_loader = build_data_loaders(train_dataset, test_dataset, protect_bz)\n",
    "mini_opt_proc = MiniBatchOptimizer(robust_net.parameters(), lr=protect_lr, decreasing_lr=protect_dec_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_net = protect(robust_net, mini_opt_proc, criterion, prot_train_loader, prot_test_loader, device=device, epochs=protect_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_net = Net().to(device)\n",
    "protect_epochs = epochs\n",
    "protect_lr = learning_rate\n",
    "protect_bz = batch_size\n",
    "\n",
    "prot_train_loader, prot_test_loader = build_data_loaders(train_dataset, test_dataset, protect_bz)\n",
    "adam_opt_proc = AdamOptimizer(net_naive.parameters(), lr=best_adam_params['lr'], beta1=best_adam_params['beta1'],beta2=best_adam_params['beta2'],weight_decay=best_adam_params['weight_decay'],epsilon=best_adam_params['epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_net_adam = protect(robust_net, adam_opt_proc, criterion, prot_train_loader, prot_test_loader, device=device, epochs=protect_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack robust models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch (for now, loop later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_robust = []\n",
    "losses_robust = []\n",
    "# This should be the first term test_loader is used\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack = attack(robust_net, criterion, prot_test_loader, eps, device=device)\n",
    "    accuracy_robust.append(acc_attack)\n",
    "    losses_robust.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_robust_adam = []\n",
    "losses_robust_adam = []\n",
    "# This should be the first term test_loader is used\n",
    "for eps in epsilons:\n",
    "    loss_attack, acc_attack = attack(robust_net_adam, criterion, prot_test_loader, eps, device=device)\n",
    "    accuracy_robust_adam.append(acc_attack)\n",
    "    losses_robust_adam.append(loss_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracy_naive, \"*-\", c='blue', label='Naive Model')\n",
    "plt.plot(epsilons, accuracy_robust, \"*-\", c='orange', label='Robust Model')\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, 0.5, step=0.05))\n",
    "\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of plots\n",
    "\n",
    "* diff naive vs robust (algo as hue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs456env]",
   "language": "python",
   "name": "conda-env-cs456env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
