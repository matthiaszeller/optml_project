{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation Algorithms in Non-Convex Classification Problems with Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-BATCH GRADIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_cuda = False # GPU seems to raise erros on my side\n",
    "device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
    "from data_utils import get_mnist, build_data_loaders\n",
    "train_dataset, test_dataset = get_mnist(normalize=True)\n",
    "train_dataset.data.mean(0).mean().item(), test_dataset.data.mean(0).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Values should be (-7.328089801639237e-10, 0.002495675580576062) if the import was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Hyperparameters for Minibatch to ensure equitable comparision to the other algorithms tested\n",
    "# Use the Ray Tune library to run Bayes Search on the following parameters:\n",
    "# The learning rate\n",
    "# The batch size\n",
    "# Whether we have a decreasing learning rate or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from net import Net\n",
    "import numpy as np \n",
    "import pandas\n",
    "from minibatch import minibatch_run, minibatch_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "space = {\n",
    "            \"lr\": tune.uniform(1e-4, 1e-2),\n",
    "            \"b_size\": tune.uniform(2, 128),\n",
    "            \"dec_lr\": tune.uniform(0, 1)\n",
    "        }\n",
    "samples = 5 # Suggested value by the Ray Tune Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note here that the neural net deployed here is found in the net.py file\n",
    "# The Loss function throughout is simply Torch's CrossEntropyLoss\n",
    "# Bayes Search here focuses on maximising the average accuracy (out of 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_net = Net().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "bayesopt = BayesOptSearch(metric=\"mean_accuracy\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tuning function is found in the minibatch.py file and based on the standard format for Ray Tune calls\n",
    "# It stores the result of the 10 runs in a pandas dataframe and then returns the highest average accuracy and the associated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "best_config, best_result = minibatch_tune(tune_net, train_dataset, test_dataset, samples, criterion, space, bayesopt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 91\n",
    "learning_rate = 0.01\n",
    "decreasing_lr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = int(best_config[\"b_size\"])\n",
    "learning_rate = best_config[\"lr\"]\n",
    "decreasing_lr = best_config[\"dec_lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if decreasing_lr < 0.5:\n",
    "    print(\"Best Result was {:.2f}% with a Batch Size of {} and a fixed learning rate of {:.4f}\".format(best_result*100, batch_size, learning_rate))\n",
    "else:\n",
    "    print(\"Best Result was {:.2f}% with a Batch Size of {} and a learning rate of {:.4f} that was decreasing\".format(best_result*100, batch_size, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing under Normal Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_naive = Net().to(device)\n",
    "train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the setup with the Minibatch Optimiser from the minibatch.py file\n",
    "### Accuracy here is the percentage of correctly guessed labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import training, testing, accuracy\n",
    "from minibatch import MiniBatchOptimizer\n",
    "\n",
    "mini_opt = MiniBatchOptimizer(net_naive.parameters(), lr=learning_rate, decreasing_lr=decreasing_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, acc = training(net_naive, train_loader, mini_opt, criterion, accuracy, epochs=epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, acc = testing(net_naive, test_loader, criterion, accuracy, device=device)\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_compact = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses, accs = minibatch_run(net_compact, train_dataset, test_dataset, criterion, accuracy, device=device, bz=batch_size, lr=learning_rate, epochs=epochs, dec_lr=decreasing_lr)\n",
    "# Has the weird shaping issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot of losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack against Unprotected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversary import attack, protect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_lenet= []\n",
    "examples_lenet = []\n",
    "\n",
    "epsilons_lenet = np.arange(0, 0.5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons_lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attack the model with different epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in epsilons_lenet:\n",
    "    acc, ex = attack(net_naive, criterion, test_loader, update_max_norm=eps, device=device)\n",
    "    accuracies_lenet.append(acc)\n",
    "    examples_lenet.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Results of Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons_lenet, accuracies_lenet, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protect Model against FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_net = Net().to(device)\n",
    "protect_epochs = 10\n",
    "protect_lr = 0.01\n",
    "protect_bz = 16\n",
    "protect_dec_lr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_train_loader, prot_test_loader = build_data_loaders(train_dataset, test_dataset, protect_bz)\n",
    "mini_opt = MiniBatchOptimizer(robust_net.parameters(), lr=protect_lr, decreasing_lr=protect_dec_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(model: Module, optim: Optimizer,criterion: Module, train_loader: Iterable, test_loader: Iterable, device, epochs: int = 10):\n",
    "robust_net = protect(robust_net, mini_opt, criterion, prot_train_loader, prot_test_loader, device=device, epochs=protect_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(protect_epochs):\n",
    "        # Train an epoch\n",
    "        #robust_net.train()\n",
    "        for batch_x, batch_y in prot_train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Forward pass for adversarial perturbations\n",
    "            batch_x.requires_grad = True\n",
    "            output = robust_net(batch_x)\n",
    "\n",
    "            original_predictions = output.argmax(1) # get the index of the max logit\n",
    "            original_accuracy = accuracy(output, batch_y)\n",
    "            loss = criterion(output, batch_y)\n",
    "            robust_net.zero_grad()\n",
    "            loss.backward()\n",
    "            perturbed_data = fgsm(batch_x, batch_x.grad, 0.25)\n",
    "            \n",
    "            # Evaluate the network (forward pass)\n",
    "            prediction = robust_net(perturbed_data)\n",
    "            loss = criterion(prediction, batch_y)\n",
    "            \n",
    "            # Compute the gradient\n",
    "            mini_opt.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model with a gradient step\n",
    "            mini_opt.step()\n",
    "\n",
    "        # Test the quality on the test set\n",
    "        robust_net.eval()\n",
    "        accuracies = []\n",
    "        for batch_x, batch_y in prot_test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            # Evaluate the network (forward pass)\n",
    "            prediction = robust_net(batch_x)\n",
    "            accuracies.append(accuracy(prediction, batch_y))\n",
    "        \n",
    "        print(\"Epoch {:.2f} | Test accuracy: {:.5f}\".format(epoch, sum(accuracies).item()/len(accuracies)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack against Protected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_lenet_robust = []\n",
    "examples_lenet_robust = []\n",
    "\n",
    "epsilons_lenet_robust = np.arange(0, 0.5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in epsilons_lenet_robust:\n",
    "    acc, ex = attack(protected_net, CrossEntropyLoss(), prot_train_loader, eps, device=device)\n",
    "    accuracies_lenet_robust.append(acc)\n",
    "    examples_lenet_robust.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the models\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons_lenet, accuracies_lenet, \"*-\", c='blue', label='Convolutional network')\n",
    "plt.plot(epsilons_lenet_robust, accuracies_lenet_robust, \"*-\", c='orange', label='Convolutional network (robust)')\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
