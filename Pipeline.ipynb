{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"Pipeline.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"metadata":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}},"cells":[{"cell_type":"code","metadata":{"id":"mechanical-vienna"},"source":["%load_ext autoreload\n","%autoreload 2"],"id":"mechanical-vienna","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"super-finland"},"source":["# guidelines\n","\n","TODO : import whenever needed, not centralized"],"id":"super-finland"},{"cell_type":"markdown","metadata":{"id":"coastal-sugar"},"source":["states https://pytorch.org/tutorials/beginner/saving_loading_models.html"],"id":"coastal-sugar"},{"cell_type":"markdown","metadata":{"id":"stable-latter"},"source":["# Introduction "],"id":"stable-latter"},{"cell_type":"markdown","metadata":{"id":"educated-soviet"},"source":["## Aim"],"id":"educated-soviet"},{"cell_type":"markdown","metadata":{"id":"electrical-density"},"source":["## Data"],"id":"electrical-density"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_URHgbeWjLB","executionInfo":{"status":"ok","timestamp":1623891269013,"user_tz":-120,"elapsed":21348,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}},"outputId":"e351ca47-041d-4538-b293-6044685e800b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"8_URHgbeWjLB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIYuAo6eXCTg","executionInfo":{"status":"ok","timestamp":1623891271652,"user_tz":-120,"elapsed":992,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}},"outputId":"a9c12a99-6682-418f-c053-a1f0b842d09e"},"source":["#%cd /content/drive/MyDrive/Colab\\ Notebooks/\n","#%cd CS439/optml_project/\n","!ls"],"id":"iIYuAo6eXCTg","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"foreign-chemical"},"source":["First load the dataset:"],"id":"foreign-chemical"},{"cell_type":"code","metadata":{"id":"aerial-riding","executionInfo":{"status":"ok","timestamp":1623891288068,"user_tz":-120,"elapsed":8801,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["from data_utils import get_mnist\n","\n","train_dataset, test_dataset = get_mnist(normalize=True)"],"id":"aerial-riding","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"possible-bulletin","executionInfo":{"status":"ok","timestamp":1623891288692,"user_tz":-120,"elapsed":636,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["import numpy as np\n","#import random\n","import torch\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns"],"id":"possible-bulletin","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"resident-mineral"},"source":["## Setup"],"id":"resident-mineral"},{"cell_type":"markdown","metadata":{"id":"maritime-globe"},"source":["Below one can find flags that will setup the notebook:"],"id":"maritime-globe"},{"cell_type":"code","metadata":{"id":"sixth-belfast","executionInfo":{"status":"ok","timestamp":1623891319857,"user_tz":-120,"elapsed":200,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["# Whether to tune the hyperparameters in this notebook\n","# Note that this might take a long time (especially for Adam)\n","hyperparameter_tune = False\n","prot_hyperparameter_tune = True"],"id":"sixth-belfast","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suspected-madison","executionInfo":{"status":"ok","timestamp":1623891288698,"user_tz":-120,"elapsed":9,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}},"outputId":"317070a6-98b3-4832-ab12-1f36cdd65ee3"},"source":["# Whether to use the GPU, if it's not available, this will be ignored\n","use_cuda = True\n","device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n","print(\"Device chosen is {}\".format(device))"],"id":"suspected-madison","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Device chosen is cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"sophisticated-karaoke"},"source":["We setup the training parameters that we will use all along the notebook, in order to improve readability in downstream code:"],"id":"sophisticated-karaoke"},{"cell_type":"code","metadata":{"id":"hundred-swedish","executionInfo":{"status":"ok","timestamp":1623891288899,"user_tz":-120,"elapsed":208,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["from training import accuracy\n","\n","training_config = {\n","    # Loss function\n","    'loss_fun': torch.nn.CrossEntropyLoss(),\n","    # Performance evaluation function\n","    'metric_fun': accuracy,\n","    # The device to train on\n","    'device': device,\n","    # Number of epochs\n","    'epochs': 10,\n","}\n","\n","test_config = training_config.copy()\n","test_config.pop('epochs');"],"id":"hundred-swedish","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flying-assessment"},"source":["Note that we will use a model with a 10-dimensional output, where each output is passed through softmax. When receiving an output \n","\n","$$Z = \\begin{bmatrix} \\mathbf z_1 & \\dots & \\mathbf z_B \\end{bmatrix}^\\top \\in \\mathbb R^{B \\times 10}$$\n","\n","with $B$ the batch size, we first retrieve the maximal component of each $\\mathbf z_i$:\n","\n","$$\\hat y_i = \\text{argmax}_{k = 1, \\ldots, 10} \\; z_{ik}, \\quad i = 1, \\ldots, B$$\n","\n","and then compute the accuracy:\n","\n","$$\\text{acc} = \\frac 1 B \\sum_{i=1}^B I\\left\\{ \\hat y_i = y_i \\right\\} $$\n","\n","with $I$ the indicator function and $y_i \\in \\{1, \\ldots, 10\\}$ the true target. "],"id":"flying-assessment"},{"cell_type":"code","metadata":{"id":"monetary-filter","executionInfo":{"status":"ok","timestamp":1623891289366,"user_tz":-120,"elapsed":471,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["# View the source code\n","??accuracy"],"id":"monetary-filter","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"touched-alfred"},"source":["# Model"],"id":"touched-alfred"},{"cell_type":"markdown","metadata":{"id":"dependent-alabama"},"source":["We use a simple standard model for the MNIST dataset (can be found [here](https://github.com/floydhub/mnist/blob/master/ConvNet.py))."],"id":"dependent-alabama"},{"cell_type":"code","metadata":{"id":"grave-dominican","executionInfo":{"status":"ok","timestamp":1623891290139,"user_tz":-120,"elapsed":776,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["from net import Net\n","\n","??Net"],"id":"grave-dominican","execution_count":16,"outputs":[{"output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSource:\u001b[0m        \n","\u001b[0;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"ConvNet -> Max_Pool -> RELU -> ConvNet -> Max_Pool -> RELU -> FC -> RELU -> FC -> SOFTMAX\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFile:\u001b[0m           /media/storage/Semester 2/Opt. ML/Project/REPO/optml_project/net.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"private-business"},"source":["# Hyperparameter tuning"],"id":"private-business"},{"cell_type":"code","metadata":{"id":"charming-lingerie","executionInfo":{"status":"ok","timestamp":1623891290154,"user_tz":-120,"elapsed":18,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["from torch.optim import Optimizer\n","from training import tune_optimizer\n","from optimizer import AdamOptimizer, NesterovOptimizer, MiniBatchOptimizer\n","from data_utils import get_best_hyperparams"],"id":"charming-lingerie","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"referenced-consultation"},"source":["If the `hyperparameter_tune` flag was set to `True` above, the following code will run hyperparameter tuning on all optimizers. Note that one can either run KFold cross validation (by providing `n_folds`) or use a simple train/test split (by providing `train_ratio`)."],"id":"referenced-consultation"},{"cell_type":"markdown","metadata":{"id":"vocal-funeral"},"source":["If the flag is set to `False`, the cell below will simply set up the hyperparameters that we carefully cross-validated:"],"id":"vocal-funeral"},{"cell_type":"code","metadata":{"id":"sticky-three","executionInfo":{"status":"ok","timestamp":1623891291129,"user_tz":-120,"elapsed":992,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["optimizers = {\n","    AdamOptimizer: get_best_hyperparams('./res/adam_tuning_round3.json'),\n","    NesterovOptimizer: get_best_hyperparams('./res/nesterov_tuning_round2.json'),\n","    MiniBatchOptimizer: get_best_hyperparams('./res/minibatch_tuning_round2.json')\n","}"],"id":"sticky-three","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"atlantic-cooking"},"source":["## Adam"],"id":"atlantic-cooking"},{"cell_type":"code","metadata":{"id":"developed-sending","executionInfo":{"status":"ok","timestamp":1623891291130,"user_tz":-120,"elapsed":12,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["search_grid_adam = {\n","        'lr': np.linspace(0.001, 0.01, 3),\n","        'beta1':  np.linspace(0.1, 0.9, 2),\n","        'beta2': np.linspace(0.5, 0.999, 2),\n","        'batch_size': [32, 64, 128],\n","        'weight_decay': np.linspace(0.001, 0.1, 2),\n","        'epsilon': np.linspace(1e-10, 1e-8, 2),\n","    }\n","\n","if hyperparameter_tune:\n","    results_adam = tune_optimizer(\n","        model=Net().to(device),\n","        optim_fun=AdamOptimizer,\n","        xtrain=train_dataset.data,\n","        ytrain=train_dataset.targets,\n","        search_grid=search_grid_adam,\n","        nfolds=3,\n","        **training_config)\n","\n","else:\n","    results_adam = optimizers[AdamOptimizer]"],"id":"developed-sending","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"falling-dividend"},"source":["## Nesterov"],"id":"falling-dividend"},{"cell_type":"code","metadata":{"id":"saved-delivery","executionInfo":{"status":"ok","timestamp":1623891291132,"user_tz":-120,"elapsed":13,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["search_grid_nesterov = {\n","    'lr': np.logspace(0, 1),\n","    'batch_size': [32, 64, 128]\n","}\n","\n","if hyperparameter_tune:\n","    results_nesterov = tune_optimizer(\n","        model=Net().to(device),\n","        optim_fun=NesterovOptimizer,\n","        xtrain=train_dataset.data,\n","        ytrain=train_dataset.targets,\n","        search_grid=search_grid_nesterov,\n","        nfolds=3,\n","        **training_config\n","    )\n","\n","else:\n","    results_nesterov = optimizers[NesterovOptimizer]"],"id":"saved-delivery","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"instrumental-opinion"},"source":["## Minibatch"],"id":"instrumental-opinion"},{"cell_type":"code","metadata":{"id":"absent-prayer","executionInfo":{"status":"ok","timestamp":1623891291132,"user_tz":-120,"elapsed":13,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["search_grid_mini  = {\n","    'lr': np.linspace(0.00001, 0.01, 5),\n","    'batch_size': [32, 64, 128],\n","    'decreasing_lr': [0, 1],\n","}\n","if hyperparameter_tune:\n","    results_mini = tune_optimizer(\n","        model=Net().to(device),\n","        optim_fun=MiniBatchOptimizer,\n","        xtrain=train_dataset.data,\n","        ytrain=train_dataset.targets,\n","        search_grid=search_grid_mini,\n","        nfolds=3,\n","        **training_config\n","    )\n","\n","else:\n","    results_mini = optimizers[MiniBatchOptimizer]"],"id":"absent-prayer","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"south-promise","executionInfo":{"status":"ok","timestamp":1623891291133,"user_tz":-120,"elapsed":13,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}},"outputId":"bf4acede-f4b9-41a6-b780-321e1af735b8"},"source":["print(\"ADAM: Highest Test Accuracy {:.4f} with standart deviation of {:.4f}\".format(results_adam[\"metric_test\"], results_adam[\"metric_test_std\"]))\n","print(\"Hyperparameter set: Learning rate =  {:.4f}, Beta1 = {:.1f}, Beta2 = {:.3f}, Weight decay = {:.2f}, Epsilon = {:.8f},  Batch Size = {:.0f}\\n\".format(results_adam[\"lr\"], results_adam[\"beta1\"], results_adam[\"beta2\"], results_adam[\"weight_decay\"], results_adam[\"epsilon\"], results_adam['batch_size']))\n","print(\"NESTEROV: Highest Test Accuracy {:.4f} with standart deviation of {:.4f}\".format(results_nesterov[\"metric_test\"], results_nesterov[\"metric_test_std\"]))\n","print(\"Hyperparameter set: Learning rate =  {:.4f}, Batch Size = {:.0f}\\n\".format(results_nesterov[\"lr\"], results_nesterov[\"batch_size\"]))\n","print(\"MINIBATCH: Highest Test Accuracy {:.4f} with standart deviation of {:.4f}\".format(results_mini[\"metric_test\"], results_mini[\"metric_test_std\"]))\n","print(\"Hyperparameter set: Learning rate =  {:.4f}, Decreasing Learning rate {:.1f}, Batch Size = {:.0f}\\n\".format(results_mini[\"lr\"], results_mini[\"decreasing_lr\"], results_mini[\"batch_size\"]))\n"],"id":"south-promise","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["ADAM: Highest Test Accuracy 0.9868 with standart deviation of 0.0007\nHyperparameter set: Learning rate =  0.0001, Beta1 = 0.9, Beta2 = 0.999, Weight decay = 0.01, Epsilon = 0.00000001,  Batch Size = 32\n\nNESTEROV: Highest Test Accuracy 0.9876 with standart deviation of 0.0010\nHyperparameter set: Learning rate =  0.0001, Batch Size = 64\n\nMINIBATCH: Highest Test Accuracy 0.9886 with standart deviation of 0.0002\nHyperparameter set: Learning rate =  0.2639, Decreasing Learning rate 0.0, Batch Size = 128\n\n"]}]},{"cell_type":"markdown","metadata":{"id":"better-archives"},"source":["# Attack on naive model\n","\n"],"id":"better-archives"},{"cell_type":"code","metadata":{"id":"better-agreement","executionInfo":{"status":"ok","timestamp":1623891336717,"user_tz":-120,"elapsed":574,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["from data_utils import build_data_loaders\n","from training import training, testing\n","from adversary import attack, projected_attack"],"id":"better-agreement","execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"saved-parameter","executionInfo":{"status":"ok","timestamp":1623891336718,"user_tz":-120,"elapsed":7,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["optimizers = {\n","    AdamOptimizer: get_best_hyperparams('./res/adam_tuning_round3.json', get_performance=False),\n","    NesterovOptimizer: get_best_hyperparams('./res/nesterov_tuning_round2.json', get_performance=False),\n","    MiniBatchOptimizer: get_best_hyperparams('./res/minibatch_tuning_round2.json', get_performance=False)\n","}"],"id":"saved-parameter","execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"posted-portable","executionInfo":{"status":"ok","timestamp":1623891338326,"user_tz":-120,"elapsed":194,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}},"outputId":"b253cb92-2f98-4484-8753-7a089e436492"},"source":["optimizers"],"id":"posted-portable","execution_count":null,"outputs":[]},{"cell_type":"raw","metadata":{"executionInfo":{"elapsed":27,"status":"error","timestamp":1623615469045,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"},"user_tz":-120},"id":"accomplished-lawrence","jupyter":{"source_hidden":true},"outputId":"1ade6c5a-b1d0-4739-c1df-26c5777bdfdc","tags":[]},"source":["# NON LOOPY VERSION\n","naive_networks = dict()\n","data_naive = list()\n","batch_log_interval = -1\n","\n","for optimizer, optimizer_params in optimizers.items():\n","    optimizer_params = optimizer_params.copy()\n","    \n","    net = Net().to(device)\n","    # Instantiate data loaders with selected batch size\n","    batch_size = int(optimizer_params.pop('batch_size'))\n","    metric_test = optimizer_params.pop('metric_test')\n","    metric_test_std = optimizer_params.pop('metric_test_std')\n","    train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)\n","    # Instantiate optimizer\n","    optimizer_instance = optimizer(net.parameters(), **optimizer_params)\n","    print(f'--- {optimizer_instance}')\n","    # Train\n","    loss_train, acc_train = training(\n","        model=net, \n","        dataset=train_loader, \n","        optim=optimizer_instance,\n","        batch_log_interval=batch_log_interval,\n","        **training_config\n","    )\n","    # Test\n","    loss_test, acc_test = testing(\n","        model=net,\n","        dataset=test_loader,\n","        **test_config\n","    )\n","    # Log\n","    data_naive.append({\n","        'optimizer': str(optimizer_instance),\n","        'loss_train': loss_train,\n","        'acc_train': acc_train,\n","        'loss_test': loss_test,\n","        'acc_test': acc_test\n","    })\n","    # Save naive model\n","    naive_networks[optimizer] = net"],"id":"accomplished-lawrence"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"intended-michigan","executionInfo":{"status":"ok","timestamp":1623891667535,"user_tz":-120,"elapsed":328127,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}},"outputId":"b3c4dda3-2c2c-4c68-f0ad-c87a58dd43f0"},"source":["# Logging data structures\n","data_naive = list()\n","data_naive_attack = list()\n","\n","# Training / attack config\n","n_try = 3\n","batch_log_interval = -1\n","epsilons = np.arange(0, 0.6, 0.05)\n","\n","for optimizer, optimizer_params in optimizers.items():\n","    print(f'--- {optimizer.__name__}')\n","    # --------- SETUP OPTIMIZER\n","    optimizer_params = optimizer_params.copy()\n","    # Instantiate data loaders with selected batch size\n","    batch_size = int(optimizer_params.pop('batch_size'))\n","    train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)\n","    \n","    # --------- Train & Attack several times per optimizer\n","    for n in range(1, n_try + 1):\n","        net = Net().to(device)\n","        optimizer_instance = optimizer(net.parameters(), **optimizer_params)\n","        # --------- TRAIN MODEL\n","        loss_train, acc_train = training(\n","            model=net, \n","            dataset=train_loader, \n","            optim=optimizer_instance,\n","            batch_log_interval=batch_log_interval,\n","            **training_config\n","        )\n","        # --------- TEST MODEL\n","        loss_test, acc_test = testing(\n","            model=net,\n","            dataset=test_loader,\n","            **test_config\n","        )\n","        # Log\n","        data_naive.append({\n","            'optimizer': str(optimizer_instance),\n","            'n': n,\n","            'loss_train': loss_train,\n","            'acc_train': acc_train,\n","            'loss_test': loss_test,\n","            'acc_test': acc_test\n","        })\n","\n","        # --------- ATTACK MODEL\n","        print(f'Launching attacks', end=' ')\n","        for eps in epsilons:\n","            print('', end='.')\n","            loss_attack, acc_attack = attack(\n","                model=net,\n","                test_loader=test_loader,\n","                epsilon=eps,\n","                verbose=False,\n","                **test_config\n","            )\n","            # Log\n","            data_naive_attack.append({\n","                'optimizer': str(optimizer_instance),\n","                'n': n,\n","                'epsilon': eps,\n","                'loss': loss_attack,\n","                'acc': acc_attack\n","            })\n","\n","        print()"],"id":"intended-michigan","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suffering-proposal"},"source":["### Training curves"],"id":"suffering-proposal"},{"cell_type":"code","metadata":{"id":"julian-anderson","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"ok","timestamp":1623891668243,"user_tz":-120,"elapsed":717,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}},"outputId":"ad42db05-f370-4410-9fbc-e8f311d5d2e5"},"source":["df_naive = pd.DataFrame(data_naive).sort_values(['optimizer', 'n'])\n","# Average training loss per epoch\n","df_naive.loss_train = df_naive.loss_train.apply(lambda s: np.mean(s, axis=1))\n","\n","colors = {'AdamOptimizer': 'r', 'MiniBatchOptimizer': 'b', 'NesterovOptimizer': 'm'}\n","for _, row in df_naive.iterrows():\n","    plt.plot(range(1, training_config['epochs'] + 1), row.loss_train, '-o', label=row.optimizer, color=colors[row.optimizer])\n","\n","plt.grid(alpha=.6)\n","plt.legend();\n","plt.yscale('log')\n","plt.xlabel('Epoch'); plt.ylabel('Training Loss')\n","plt.title('Training curves of naive models');"],"id":"julian-anderson","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"generic-terrorism"},"source":["### Attack plots"],"id":"generic-terrorism"},{"cell_type":"code","metadata":{"id":"orange-consumer","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"ok","timestamp":1623891668998,"user_tz":-120,"elapsed":759,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}},"outputId":"101e1ee0-589d-49ce-eb90-92332d1d37c7"},"source":["df = pd.DataFrame(data_naive_attack).sort_values(['optimizer', 'epsilon'])\n","# Raw accuracy in function of attack stength\n","sns.lineplot(x='epsilon', y='acc', data=df, hue='optimizer', marker='o')\n","plt.grid(alpha=.6)\n","plt.ylabel('Accuracy');\n","plt.title('Attack on naive models');"],"id":"orange-consumer","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"meaning-amount","executionInfo":{"status":"ok","timestamp":1623891669213,"user_tz":-120,"elapsed":4,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["df.T.to_json('res/log_attack_naive.json', indent=2)"],"id":"meaning-amount","execution_count":null,"outputs":[]},{"cell_type":"raw","metadata":{"id":"moral-married"},"source":["# THIS DOESNT WORK, PIVOTING -> several values (1 per run), want to keep list of values or compute std \n","# Difference in accuracy w.r.t. to no attack (epsilon = 0)\n","df_delta = df.pivot_table(index='epsilon', columns='optimizer', values='acc')\n","df_delta -= df_delta.loc[0]\n","df_delta = df_delta.unstack().rename('acc').reset_index()\n","sns.lineplot(x='epsilon', y='acc', data=df_delta, hue='optimizer', marker='o')\n","plt.grid(alpha=.6)\n","plt.ylabel('Difference in accuracy'); plt.xlabel('Attack strength $\\epsilon$');"],"id":"moral-married"},{"cell_type":"raw","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1623616171457,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"},"user_tz":-120},"id":"stock-header","tags":[]},"source":["# NON LOOPY VERSION FOR ATTACK\n","data_naive_attack = list()\n","epsilons = np.arange(0, 0.6, 0.05)\n","\n","for optimizer, network in naive_networks.items():\n","    name = optimizer_names[optimizer]\n","    print(f'\\n{name:<10}', end='')\n","    \n","    for eps in epsilons:\n","        print('', end='.')\n","        loss_attack, acc_attack = attack(\n","            model=network,\n","            test_loader=test_loader,\n","            epsilon=eps,\n","            verbose=False,\n","            **test_config\n","        )\n","        # Log\n","        data_naive_attack.append({\n","            'optimizer': name,\n","            'epsilon': eps,\n","            'loss': loss_attack,\n","            'acc': acc_attack\n","        })"],"id":"stock-header"},{"cell_type":"raw","metadata":{"tags":[],"id":"later-poster"},"source":["# Plots before loops version\n","_, ax = plt.subplots(2, 1, figsize=(7, 6), sharex=True)\n","plt.axes(ax[0])\n","df = pd.DataFrame(data_naive_attack).sort_values(['optimizer', 'epsilon'])\n","\n","# Raw accuracy in function of attack stength\n","sns.lineplot(x='epsilon', y='acc', data=df, hue='optimizer', marker='o')\n","plt.grid(alpha=.6)\n","plt.ylabel('Accuracy');\n","plt.title('Attack on naive models')\n","\n","# Difference in accuracy w.r.t. to no attack (epsilon = 0)\n","plt.axes(ax[1])\n","df_delta = df.pivot(index='epsilon', columns='optimizer', values='acc')\n","df_delta -= df_delta.loc[0]\n","df_delta = df_delta.unstack().rename('acc').reset_index()\n","sns.lineplot(x='epsilon', y='acc', data=df_delta, hue='optimizer', marker='o')\n","plt.grid(alpha=.6)\n","plt.ylabel('Difference in accuracy'); plt.xlabel('Attack strength $\\epsilon$');"],"id":"later-poster"},{"cell_type":"markdown","metadata":{"id":"cloudy-warren"},"source":["# Attack on robust model"],"id":"cloudy-warren"},{"cell_type":"markdown","metadata":{"id":"apart-preference"},"source":["## Hyperparameter optimization on robust models\n","\n","- If the `prot_hyperparameter_tune` flag was set to `True` above, the following code will run hyperparameter tuning on all optimizers for robust models. Note that one can either run KFold cross validation (by providing `n_folds`) or use a simple train/test split (by providing `train_ratio`).\n"],"id":"apart-preference"},{"cell_type":"raw","metadata":{"executionInfo":{"elapsed":228,"status":"error","timestamp":1623617263506,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"},"user_tz":-120},"id":"economic-shock","outputId":"f6b4fdd5-0490-47a7-88f7-80319364fa38"},"source":["prot_optimizers = {\n","    AdamOptimizer: get_best_hyperparams('./res/prot_adam_tuning.json',get_performance=False),\n","    NesterovOptimizer: get_best_hyperparams('./res/prot_nesterov_tuning.json', get_performance=False),\n","    MiniBatchOptimizer: get_best_hyperparams('./res/prot_minibatch_tuning.json', get_performance=False)\n","}"],"id":"economic-shock"},{"cell_type":"code","metadata":{"id":"F49mhli4ZS0I","executionInfo":{"status":"ok","timestamp":1623891786914,"user_tz":-120,"elapsed":223,"user":{"displayName":"Baris Sevilmis","photoUrl":"","userId":"05631870087761794056"}}},"source":["from adversary import protected_training"],"id":"F49mhli4ZS0I","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"exciting-motor"},"source":["### Adam"],"id":"exciting-motor"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cordless-anchor","outputId":"2553320f-8c9a-4c3f-abe0-4964847f3f78","tags":["outputPrepend"]},"source":["search_grid_adam = {\n","        'lr': np.linspace(0.001, 0.01, 3),\n","        'beta1':  np.linspace(0.1, 0.9, 2),\n","        'beta2': np.linspace(0.5, 0.999, 2),\n","        'batch_size': [32, 64, 128],\n","        'weight_decay': np.linspace(0.001, 0.1, 2),\n","        'epsilon': np.linspace(1e-10, 1e-8, 2),\n","    }\n","\n","if prot_hyperparameter_tune:\n","    results_adam_prot = tune_optimizer(\n","        model=Net().to(device),\n","        optim_fun=AdamOptimizer,\n","        xtrain=train_dataset.data,\n","        ytrain=train_dataset.targets,\n","        search_grid=search_grid_adam,\n","        nfolds=3,\n","        func=protected_training,\n","        **training_config)\n","\n","else:\n","    results_adam_prot = optimizers[AdamOptimizer]"],"id":"cordless-anchor","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["ss = 0.5739\tavg epoch acc = 0.9309\n","epoch 1\tavg epoch loss = 0.4455\tavg epoch acc = 0.964\n","epoch 2\tavg epoch loss = 0.5219\tavg epoch acc = 0.9628\n","epoch 3\tavg epoch loss = 0.4548\tavg epoch acc = 0.9659\n","epoch 4\tavg epoch loss = 0.4922\tavg epoch acc = 0.9651\n","epoch 5\tavg epoch loss = 0.4419\tavg epoch acc = 0.9676\n","epoch 6\tavg epoch loss = 0.4185\tavg epoch acc = 0.9705\n","epoch 7\tavg epoch loss = 0.3731\tavg epoch acc = 0.9717\n","epoch 8\tavg epoch loss = 0.3954\tavg epoch acc = 0.9712\n","epoch 9\tavg epoch loss = 0.4016\tavg epoch acc = 0.9712\n","training took 62.22 s\n","Avg test loss = 0.125\tAvg test acc = 0.965\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 32, 'weight_decay': 0.1, 'epsilon': 1e-10}\n","epoch 0\tavg epoch loss = 0.5752\tavg epoch acc = 0.9313\n","epoch 1\tavg epoch loss = 0.4458\tavg epoch acc = 0.9613\n","epoch 2\tavg epoch loss = 0.4835\tavg epoch acc = 0.9602\n","epoch 3\tavg epoch loss = 0.4352\tavg epoch acc = 0.9612\n","epoch 4\tavg epoch loss = 0.4325\tavg epoch acc = 0.9629\n","epoch 5\tavg epoch loss = 0.4172\tavg epoch acc = 0.9643\n","epoch 6\tavg epoch loss = 0.4183\tavg epoch acc = 0.9638\n","epoch 7\tavg epoch loss = 0.4256\tavg epoch acc = 0.9632\n","epoch 8\tavg epoch loss = 0.4168\tavg epoch acc = 0.9637\n","epoch 9\tavg epoch loss = 0.4161\tavg epoch acc = 0.9638\n","training took 63.7 s\n","Avg test loss = 0.133\tAvg test acc = 0.96\n","epoch 0\tavg epoch loss = 0.5777\tavg epoch acc = 0.929\n","epoch 1\tavg epoch loss = 0.4398\tavg epoch acc = 0.9606\n","epoch 2\tavg epoch loss = 0.4665\tavg epoch acc = 0.959\n","epoch 3\tavg epoch loss = 0.4396\tavg epoch acc = 0.961\n","epoch 4\tavg epoch loss = 0.4334\tavg epoch acc = 0.9598\n","epoch 5\tavg epoch loss = 0.4228\tavg epoch acc = 0.9622\n","epoch 6\tavg epoch loss = 0.4105\tavg epoch acc = 0.9605\n","epoch 7\tavg epoch loss = 0.4019\tavg epoch acc = 0.9613\n","epoch 8\tavg epoch loss = 0.4068\tavg epoch acc = 0.9614\n","epoch 9\tavg epoch loss = 0.41\tavg epoch acc = 0.9614\n","training took 62.4 s\n","Avg test loss = 0.135\tAvg test acc = 0.959\n","epoch 0\tavg epoch loss = 0.5633\tavg epoch acc = 0.933\n","epoch 1\tavg epoch loss = 0.4427\tavg epoch acc = 0.9625\n","epoch 2\tavg epoch loss = 0.4658\tavg epoch acc = 0.9605\n","epoch 3\tavg epoch loss = 0.4359\tavg epoch acc = 0.9624\n","epoch 4\tavg epoch loss = 0.4326\tavg epoch acc = 0.9621\n","epoch 5\tavg epoch loss = 0.4003\tavg epoch acc = 0.9649\n","epoch 6\tavg epoch loss = 0.4179\tavg epoch acc = 0.964\n","epoch 7\tavg epoch loss = 0.4142\tavg epoch acc = 0.9641\n","epoch 8\tavg epoch loss = 0.4146\tavg epoch acc = 0.9618\n","epoch 9\tavg epoch loss = 0.4169\tavg epoch acc = 0.9613\n","training took 62.57 s\n","Avg test loss = 0.177\tAvg test acc = 0.944\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 32, 'weight_decay': 0.1, 'epsilon': 1e-08}\n","epoch 0\tavg epoch loss = 0.5687\tavg epoch acc = 0.9313\n","epoch 1\tavg epoch loss = 0.429\tavg epoch acc = 0.9624\n","epoch 2\tavg epoch loss = 0.4543\tavg epoch acc = 0.9609\n","epoch 3\tavg epoch loss = 0.4355\tavg epoch acc = 0.9629\n","epoch 4\tavg epoch loss = 0.3947\tavg epoch acc = 0.9642\n","epoch 5\tavg epoch loss = 0.412\tavg epoch acc = 0.9644\n","epoch 6\tavg epoch loss = 0.4101\tavg epoch acc = 0.9647\n","epoch 7\tavg epoch loss = 0.4035\tavg epoch acc = 0.9633\n","epoch 8\tavg epoch loss = 0.4087\tavg epoch acc = 0.9634\n","epoch 9\tavg epoch loss = 0.4074\tavg epoch acc = 0.964\n","training took 62.51 s\n","Avg test loss = 0.236\tAvg test acc = 0.93\n","epoch 0\tavg epoch loss = 0.5747\tavg epoch acc = 0.9273\n","epoch 1\tavg epoch loss = 0.4501\tavg epoch acc = 0.96\n","epoch 2\tavg epoch loss = 0.4646\tavg epoch acc = 0.9568\n","epoch 3\tavg epoch loss = 0.438\tavg epoch acc = 0.9588\n","epoch 4\tavg epoch loss = 0.4282\tavg epoch acc = 0.9589\n","epoch 5\tavg epoch loss = 0.4162\tavg epoch acc = 0.9601\n","epoch 6\tavg epoch loss = 0.416\tavg epoch acc = 0.9621\n","epoch 7\tavg epoch loss = 0.4508\tavg epoch acc = 0.9581\n","epoch 8\tavg epoch loss = 0.4298\tavg epoch acc = 0.9591\n","epoch 9\tavg epoch loss = 0.4507\tavg epoch acc = 0.9569\n","training took 62.52 s\n","Avg test loss = 0.214\tAvg test acc = 0.937\n","epoch 0\tavg epoch loss = 0.5738\tavg epoch acc = 0.9314\n","epoch 1\tavg epoch loss = 0.4411\tavg epoch acc = 0.9612\n","epoch 2\tavg epoch loss = 0.46\tavg epoch acc = 0.9596\n","epoch 3\tavg epoch loss = 0.4355\tavg epoch acc = 0.9609\n","epoch 4\tavg epoch loss = 0.4237\tavg epoch acc = 0.961\n","epoch 5\tavg epoch loss = 0.4161\tavg epoch acc = 0.9612\n","epoch 6\tavg epoch loss = 0.4113\tavg epoch acc = 0.9638\n","epoch 7\tavg epoch loss = 0.3949\tavg epoch acc = 0.9646\n","epoch 8\tavg epoch loss = 0.4006\tavg epoch acc = 0.9636\n","epoch 9\tavg epoch loss = 0.4009\tavg epoch acc = 0.9635\n","training took 62.72 s\n","Avg test loss = 0.155\tAvg test acc = 0.952\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 64, 'weight_decay': 0.001, 'epsilon': 1e-10}\n","epoch 0\tavg epoch loss = 0.68\tavg epoch acc = 0.9122\n","epoch 1\tavg epoch loss = 0.3051\tavg epoch acc = 0.9762\n","epoch 2\tavg epoch loss = 0.2893\tavg epoch acc = 0.9789\n","epoch 3\tavg epoch loss = 0.331\tavg epoch acc = 0.9769\n","epoch 4\tavg epoch loss = 0.3322\tavg epoch acc = 0.9783\n","epoch 5\tavg epoch loss = 0.3449\tavg epoch acc = 0.9786\n","epoch 6\tavg epoch loss = 0.3283\tavg epoch acc = 0.981\n","epoch 7\tavg epoch loss = 0.3413\tavg epoch acc = 0.9798\n","epoch 8\tavg epoch loss = 0.3641\tavg epoch acc = 0.9794\n","epoch 9\tavg epoch loss = 0.3174\tavg epoch acc = 0.9816\n","training took 38.26 s\n","Avg test loss = 0.0949\tAvg test acc = 0.977\n","epoch 0\tavg epoch loss = 0.6662\tavg epoch acc = 0.9188\n","epoch 1\tavg epoch loss = 0.296\tavg epoch acc = 0.9771\n","epoch 2\tavg epoch loss = 0.286\tavg epoch acc = 0.9782\n","epoch 3\tavg epoch loss = 0.3323\tavg epoch acc = 0.9785\n","epoch 4\tavg epoch loss = 0.345\tavg epoch acc = 0.9782\n","epoch 5\tavg epoch loss = 0.3662\tavg epoch acc = 0.9772\n","epoch 6\tavg epoch loss = 0.3529\tavg epoch acc = 0.9794\n","epoch 7\tavg epoch loss = 0.3356\tavg epoch acc = 0.9812\n","epoch 8\tavg epoch loss = 0.3141\tavg epoch acc = 0.9827\n","epoch 9\tavg epoch loss = 0.2837\tavg epoch acc = 0.9834\n","training took 38.14 s\n","Avg test loss = 0.0815\tAvg test acc = 0.978\n","epoch 0\tavg epoch loss = 0.6598\tavg epoch acc = 0.9196\n","epoch 1\tavg epoch loss = 0.2958\tavg epoch acc = 0.9767\n","epoch 2\tavg epoch loss = 0.2936\tavg epoch acc = 0.9781\n","epoch 3\tavg epoch loss = 0.3402\tavg epoch acc = 0.9761\n","epoch 4\tavg epoch loss = 0.3564\tavg epoch acc = 0.9763\n","epoch 5\tavg epoch loss = 0.3322\tavg epoch acc = 0.9802\n","epoch 6\tavg epoch loss = 0.3642\tavg epoch acc = 0.978\n","epoch 7\tavg epoch loss = 0.3175\tavg epoch acc = 0.9809\n","epoch 8\tavg epoch loss = 0.3202\tavg epoch acc = 0.9798\n","epoch 9\tavg epoch loss = 0.3261\tavg epoch acc = 0.9804\n","training took 38.07 s\n","Avg test loss = 0.117\tAvg test acc = 0.97\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 64, 'weight_decay': 0.001, 'epsilon': 1e-08}\n","epoch 0\tavg epoch loss = 0.6749\tavg epoch acc = 0.9166\n","epoch 1\tavg epoch loss = 0.2977\tavg epoch acc = 0.9764\n","epoch 2\tavg epoch loss = 0.3053\tavg epoch acc = 0.9759\n","epoch 3\tavg epoch loss = 0.3512\tavg epoch acc = 0.9746\n","epoch 4\tavg epoch loss = 0.3335\tavg epoch acc = 0.978\n","epoch 5\tavg epoch loss = 0.3514\tavg epoch acc = 0.9779\n","epoch 6\tavg epoch loss = 0.3752\tavg epoch acc = 0.9775\n","epoch 7\tavg epoch loss = 0.3649\tavg epoch acc = 0.9775\n","epoch 8\tavg epoch loss = 0.3566\tavg epoch acc = 0.9787\n","epoch 9\tavg epoch loss = 0.3006\tavg epoch acc = 0.9815\n","training took 38.95 s\n","Avg test loss = 0.0794\tAvg test acc = 0.978\n","epoch 0\tavg epoch loss = 0.6739\tavg epoch acc = 0.9177\n","epoch 1\tavg epoch loss = 0.2958\tavg epoch acc = 0.9767\n","epoch 2\tavg epoch loss = 0.282\tavg epoch acc = 0.979\n","epoch 3\tavg epoch loss = 0.3235\tavg epoch acc = 0.9769\n","epoch 4\tavg epoch loss = 0.3508\tavg epoch acc = 0.9776\n","epoch 5\tavg epoch loss = 0.3669\tavg epoch acc = 0.9771\n","epoch 6\tavg epoch loss = 0.3289\tavg epoch acc = 0.979\n","epoch 7\tavg epoch loss = 0.3082\tavg epoch acc = 0.982\n","epoch 8\tavg epoch loss = 0.3096\tavg epoch acc = 0.9812\n","epoch 9\tavg epoch loss = 0.326\tavg epoch acc = 0.981\n","training took 38.68 s\n","Avg test loss = 0.0962\tAvg test acc = 0.976\n","epoch 0\tavg epoch loss = 0.6624\tavg epoch acc = 0.9134\n","epoch 1\tavg epoch loss = 0.2903\tavg epoch acc = 0.9765\n","epoch 2\tavg epoch loss = 0.3106\tavg epoch acc = 0.9753\n","epoch 3\tavg epoch loss = 0.3177\tavg epoch acc = 0.9769\n","epoch 4\tavg epoch loss = 0.3786\tavg epoch acc = 0.9766\n","epoch 5\tavg epoch loss = 0.3326\tavg epoch acc = 0.9788\n","epoch 6\tavg epoch loss = 0.3194\tavg epoch acc = 0.9797\n","epoch 7\tavg epoch loss = 0.3455\tavg epoch acc = 0.9799\n","epoch 8\tavg epoch loss = 0.3421\tavg epoch acc = 0.9798\n","epoch 9\tavg epoch loss = 0.2924\tavg epoch acc = 0.9835\n","training took 38.61 s\n","Avg test loss = 0.0758\tAvg test acc = 0.978\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 64, 'weight_decay': 0.1, 'epsilon': 1e-10}\n","epoch 0\tavg epoch loss = 0.6724\tavg epoch acc = 0.9112\n","epoch 1\tavg epoch loss = 0.2986\tavg epoch acc = 0.9754\n","epoch 2\tavg epoch loss = 0.3045\tavg epoch acc = 0.9746\n","epoch 3\tavg epoch loss = 0.3339\tavg epoch acc = 0.9726\n","epoch 4\tavg epoch loss = 0.3347\tavg epoch acc = 0.9733\n","epoch 5\tavg epoch loss = 0.3386\tavg epoch acc = 0.9727\n","epoch 6\tavg epoch loss = 0.3383\tavg epoch acc = 0.9724\n","epoch 7\tavg epoch loss = 0.3387\tavg epoch acc = 0.9723\n","epoch 8\tavg epoch loss = 0.3339\tavg epoch acc = 0.9736\n","epoch 9\tavg epoch loss = 0.3351\tavg epoch acc = 0.9731\n","training took 38.24 s\n","Avg test loss = 0.102\tAvg test acc = 0.968\n","epoch 0\tavg epoch loss = 0.6678\tavg epoch acc = 0.9156\n","epoch 1\tavg epoch loss = 0.2905\tavg epoch acc = 0.9757\n","epoch 2\tavg epoch loss = 0.2996\tavg epoch acc = 0.975\n","epoch 3\tavg epoch loss = 0.3323\tavg epoch acc = 0.9728\n","epoch 4\tavg epoch loss = 0.3417\tavg epoch acc = 0.9736\n","epoch 5\tavg epoch loss = 0.3456\tavg epoch acc = 0.9729\n","epoch 6\tavg epoch loss = 0.3424\tavg epoch acc = 0.9728\n","epoch 7\tavg epoch loss = 0.3249\tavg epoch acc = 0.9739\n","epoch 8\tavg epoch loss = 0.3365\tavg epoch acc = 0.9739\n","epoch 9\tavg epoch loss = 0.307\tavg epoch acc = 0.9755\n","training took 38.51 s\n","Avg test loss = 0.12\tAvg test acc = 0.963\n","epoch 0\tavg epoch loss = 0.6622\tavg epoch acc = 0.9167\n","epoch 1\tavg epoch loss = 0.2902\tavg epoch acc = 0.977\n","epoch 2\tavg epoch loss = 0.2959\tavg epoch acc = 0.9752\n","epoch 3\tavg epoch loss = 0.3195\tavg epoch acc = 0.975\n","epoch 4\tavg epoch loss = 0.3393\tavg epoch acc = 0.974\n","epoch 5\tavg epoch loss = 0.3182\tavg epoch acc = 0.9751\n","epoch 6\tavg epoch loss = 0.3265\tavg epoch acc = 0.9753\n","epoch 7\tavg epoch loss = 0.327\tavg epoch acc = 0.9756\n","epoch 8\tavg epoch loss = 0.3151\tavg epoch acc = 0.9754\n","epoch 9\tavg epoch loss = 0.3208\tavg epoch acc = 0.9755\n","training took 38.05 s\n","Avg test loss = 0.141\tAvg test acc = 0.959\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 64, 'weight_decay': 0.1, 'epsilon': 1e-08}\n","epoch 0\tavg epoch loss = 0.6634\tavg epoch acc = 0.9153\n","epoch 1\tavg epoch loss = 0.2969\tavg epoch acc = 0.9749\n","epoch 2\tavg epoch loss = 0.3042\tavg epoch acc = 0.9747\n","epoch 3\tavg epoch loss = 0.3255\tavg epoch acc = 0.9737\n","epoch 4\tavg epoch loss = 0.339\tavg epoch acc = 0.9736\n","epoch 5\tavg epoch loss = 0.346\tavg epoch acc = 0.9734\n","epoch 6\tavg epoch loss = 0.3509\tavg epoch acc = 0.9735\n","epoch 7\tavg epoch loss = 0.3329\tavg epoch acc = 0.9741\n","epoch 8\tavg epoch loss = 0.325\tavg epoch acc = 0.9737\n","epoch 9\tavg epoch loss = 0.3229\tavg epoch acc = 0.9744\n","training took 38.16 s\n","Avg test loss = 0.117\tAvg test acc = 0.965\n","epoch 0\tavg epoch loss = 0.6633\tavg epoch acc = 0.9158\n","epoch 1\tavg epoch loss = 0.2843\tavg epoch acc = 0.9765\n","epoch 2\tavg epoch loss = 0.2963\tavg epoch acc = 0.9754\n","epoch 3\tavg epoch loss = 0.3256\tavg epoch acc = 0.9749\n","epoch 4\tavg epoch loss = 0.3312\tavg epoch acc = 0.9746\n","epoch 5\tavg epoch loss = 0.3189\tavg epoch acc = 0.9752\n","epoch 6\tavg epoch loss = 0.3231\tavg epoch acc = 0.9746\n","epoch 7\tavg epoch loss = 0.3219\tavg epoch acc = 0.9743\n","epoch 8\tavg epoch loss = 0.3194\tavg epoch acc = 0.9755\n","epoch 9\tavg epoch loss = 0.3206\tavg epoch acc = 0.9748\n","training took 38.65 s\n","Avg test loss = 0.121\tAvg test acc = 0.965\n","epoch 0\tavg epoch loss = 0.6652\tavg epoch acc = 0.9185\n","epoch 1\tavg epoch loss = 0.2914\tavg epoch acc = 0.9766\n","epoch 2\tavg epoch loss = 0.2935\tavg epoch acc = 0.9767\n","epoch 3\tavg epoch loss = 0.3188\tavg epoch acc = 0.9742\n","epoch 4\tavg epoch loss = 0.3312\tavg epoch acc = 0.9735\n","epoch 5\tavg epoch loss = 0.3505\tavg epoch acc = 0.973\n","epoch 6\tavg epoch loss = 0.3276\tavg epoch acc = 0.9744\n","epoch 7\tavg epoch loss = 0.3296\tavg epoch acc = 0.9752\n","epoch 8\tavg epoch loss = 0.3371\tavg epoch acc = 0.9741\n","epoch 9\tavg epoch loss = 0.3285\tavg epoch acc = 0.9757\n","training took 38.13 s\n","Avg test loss = 0.108\tAvg test acc = 0.967\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 128, 'weight_decay': 0.001, 'epsilon': 1e-10}\n","epoch 0\tavg epoch loss = 0.8803\tavg epoch acc = 0.885\n","epoch 1\tavg epoch loss = 0.3052\tavg epoch acc = 0.9764\n","epoch 2\tavg epoch loss = 0.2329\tavg epoch acc = 0.9832\n","epoch 3\tavg epoch loss = 0.2089\tavg epoch acc = 0.9855\n","epoch 4\tavg epoch loss = 0.2026\tavg epoch acc = 0.9863\n","epoch 5\tavg epoch loss = 0.2106\tavg epoch acc = 0.9866\n","epoch 6\tavg epoch loss = 0.2173\tavg epoch acc = 0.9869\n","epoch 7\tavg epoch loss = 0.222\tavg epoch acc = 0.9874\n","epoch 8\tavg epoch loss = 0.2186\tavg epoch acc = 0.9885\n","epoch 9\tavg epoch loss = 0.2126\tavg epoch acc = 0.9885\n","training took 29.12 s\n","Avg test loss = 0.0616\tAvg test acc = 0.983\n","epoch 0\tavg epoch loss = 0.8862\tavg epoch acc = 0.8777\n","epoch 1\tavg epoch loss = 0.3048\tavg epoch acc = 0.9766\n","epoch 2\tavg epoch loss = 0.2221\tavg epoch acc = 0.9836\n","epoch 3\tavg epoch loss = 0.2094\tavg epoch acc = 0.9855\n","epoch 4\tavg epoch loss = 0.194\tavg epoch acc = 0.9875\n","epoch 5\tavg epoch loss = 0.198\tavg epoch acc = 0.9882\n","epoch 6\tavg epoch loss = 0.2417\tavg epoch acc = 0.9871\n","epoch 7\tavg epoch loss = 0.212\tavg epoch acc = 0.988\n","epoch 8\tavg epoch loss = 0.2096\tavg epoch acc = 0.9886\n","epoch 9\tavg epoch loss = 0.2451\tavg epoch acc = 0.9878\n","training took 29.12 s\n","Avg test loss = 0.0728\tAvg test acc = 0.981\n","epoch 0\tavg epoch loss = 0.8847\tavg epoch acc = 0.8838\n","epoch 1\tavg epoch loss = 0.3074\tavg epoch acc = 0.9761\n","epoch 2\tavg epoch loss = 0.2375\tavg epoch acc = 0.9826\n","epoch 3\tavg epoch loss = 0.204\tavg epoch acc = 0.9862\n","epoch 4\tavg epoch loss = 0.2142\tavg epoch acc = 0.9864\n","epoch 5\tavg epoch loss = 0.2047\tavg epoch acc = 0.9881\n","epoch 6\tavg epoch loss = 0.1996\tavg epoch acc = 0.9883\n","epoch 7\tavg epoch loss = 0.2217\tavg epoch acc = 0.9881\n","epoch 8\tavg epoch loss = 0.2073\tavg epoch acc = 0.9897\n","epoch 9\tavg epoch loss = 0.2286\tavg epoch acc = 0.989\n","training took 28.77 s\n","Avg test loss = 0.0631\tAvg test acc = 0.983\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 128, 'weight_decay': 0.001, 'epsilon': 1e-08}\n","epoch 0\tavg epoch loss = 0.891\tavg epoch acc = 0.8786\n","epoch 1\tavg epoch loss = 0.307\tavg epoch acc = 0.976\n","epoch 2\tavg epoch loss = 0.2315\tavg epoch acc = 0.9826\n","epoch 3\tavg epoch loss = 0.205\tavg epoch acc = 0.9857\n","epoch 4\tavg epoch loss = 0.1994\tavg epoch acc = 0.9865\n","epoch 5\tavg epoch loss = 0.2263\tavg epoch acc = 0.9857\n","epoch 6\tavg epoch loss = 0.223\tavg epoch acc = 0.9858\n","epoch 7\tavg epoch loss = 0.1996\tavg epoch acc = 0.9892\n","epoch 8\tavg epoch loss = 0.2456\tavg epoch acc = 0.9872\n","epoch 9\tavg epoch loss = 0.2208\tavg epoch acc = 0.989\n","training took 28.82 s\n","Avg test loss = 0.0738\tAvg test acc = 0.982\n","epoch 0\tavg epoch loss = 0.8781\tavg epoch acc = 0.886\n","epoch 1\tavg epoch loss = 0.3072\tavg epoch acc = 0.9757\n","epoch 2\tavg epoch loss = 0.2395\tavg epoch acc = 0.9824\n","epoch 3\tavg epoch loss = 0.2064\tavg epoch acc = 0.9854\n","epoch 4\tavg epoch loss = 0.2126\tavg epoch acc = 0.9859\n","epoch 5\tavg epoch loss = 0.2059\tavg epoch acc = 0.9871\n","epoch 6\tavg epoch loss = 0.2176\tavg epoch acc = 0.9867\n","epoch 7\tavg epoch loss = 0.2056\tavg epoch acc = 0.9886\n","epoch 8\tavg epoch loss = 0.2264\tavg epoch acc = 0.9883\n","epoch 9\tavg epoch loss = 0.2436\tavg epoch acc = 0.988\n","training took 28.9 s\n","Avg test loss = 0.0635\tAvg test acc = 0.983\n","epoch 0\tavg epoch loss = 0.8865\tavg epoch acc = 0.8777\n","epoch 1\tavg epoch loss = 0.3065\tavg epoch acc = 0.9771\n","epoch 2\tavg epoch loss = 0.2293\tavg epoch acc = 0.9842\n","epoch 3\tavg epoch loss = 0.2126\tavg epoch acc = 0.9866\n","epoch 4\tavg epoch loss = 0.2063\tavg epoch acc = 0.9877\n","epoch 5\tavg epoch loss = 0.2162\tavg epoch acc = 0.9881\n","epoch 6\tavg epoch loss = 0.2233\tavg epoch acc = 0.9878\n","epoch 7\tavg epoch loss = 0.2275\tavg epoch acc = 0.9882\n","epoch 8\tavg epoch loss = 0.2132\tavg epoch acc = 0.9895\n","epoch 9\tavg epoch loss = 0.2212\tavg epoch acc = 0.9894\n","training took 28.73 s\n","Avg test loss = 0.0823\tAvg test acc = 0.981\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 128, 'weight_decay': 0.1, 'epsilon': 1e-10}\n","epoch 0\tavg epoch loss = 0.8838\tavg epoch acc = 0.8886\n","epoch 1\tavg epoch loss = 0.3065\tavg epoch acc = 0.9774\n","epoch 2\tavg epoch loss = 0.2389\tavg epoch acc = 0.9828\n","epoch 3\tavg epoch loss = 0.2143\tavg epoch acc = 0.9849\n","epoch 4\tavg epoch loss = 0.2194\tavg epoch acc = 0.9848\n","epoch 5\tavg epoch loss = 0.209\tavg epoch acc = 0.9851\n","epoch 6\tavg epoch loss = 0.2095\tavg epoch acc = 0.9844\n","epoch 7\tavg epoch loss = 0.224\tavg epoch acc = 0.9844\n","epoch 8\tavg epoch loss = 0.2311\tavg epoch acc = 0.9841\n","epoch 9\tavg epoch loss = 0.2399\tavg epoch acc = 0.9839\n","training took 28.85 s\n","Avg test loss = 0.143\tAvg test acc = 0.962\n","epoch 0\tavg epoch loss = 0.8843\tavg epoch acc = 0.8789\n","epoch 1\tavg epoch loss = 0.2979\tavg epoch acc = 0.9765\n","epoch 2\tavg epoch loss = 0.2322\tavg epoch acc = 0.9831\n","epoch 3\tavg epoch loss = 0.2046\tavg epoch acc = 0.9856\n","epoch 4\tavg epoch loss = 0.1932\tavg epoch acc = 0.9868\n","epoch 5\tavg epoch loss = 0.2155\tavg epoch acc = 0.9853\n","epoch 6\tavg epoch loss = 0.2143\tavg epoch acc = 0.9849\n","epoch 7\tavg epoch loss = 0.2258\tavg epoch acc = 0.985\n","epoch 8\tavg epoch loss = 0.2166\tavg epoch acc = 0.9855\n","epoch 9\tavg epoch loss = 0.2128\tavg epoch acc = 0.9865\n","training took 28.86 s\n","Avg test loss = 0.0791\tAvg test acc = 0.977\n","epoch 0\tavg epoch loss = 0.8854\tavg epoch acc = 0.8742\n","epoch 1\tavg epoch loss = 0.3015\tavg epoch acc = 0.9752\n","epoch 2\tavg epoch loss = 0.2296\tavg epoch acc = 0.9823\n","epoch 3\tavg epoch loss = 0.2008\tavg epoch acc = 0.9851\n","epoch 4\tavg epoch loss = 0.2017\tavg epoch acc = 0.9856\n","epoch 5\tavg epoch loss = 0.2087\tavg epoch acc = 0.9859\n","epoch 6\tavg epoch loss = 0.2132\tavg epoch acc = 0.9852\n","epoch 7\tavg epoch loss = 0.2301\tavg epoch acc = 0.9842\n","epoch 8\tavg epoch loss = 0.2423\tavg epoch acc = 0.9826\n","epoch 9\tavg epoch loss = 0.2439\tavg epoch acc = 0.9827\n","training took 28.49 s\n","Avg test loss = 0.0842\tAvg test acc = 0.974\n","{'lr': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'batch_size': 128, 'weight_decay': 0.1, 'epsilon': 1e-08}\n","epoch 0\tavg epoch loss = 0.8904\tavg epoch acc = 0.8872\n","epoch 1\tavg epoch loss = 0.3069\tavg epoch acc = 0.9758\n","epoch 2\tavg epoch loss = 0.2317\tavg epoch acc = 0.9827\n","epoch 3\tavg epoch loss = 0.2091\tavg epoch acc = 0.9856\n","epoch 4\tavg epoch loss = 0.2181\tavg epoch acc = 0.9845\n","epoch 5\tavg epoch loss = 0.2407\tavg epoch acc = 0.982\n","epoch 6\tavg epoch loss = 0.2086\tavg epoch acc = 0.985\n","epoch 7\tavg epoch loss = 0.2049\tavg epoch acc = 0.9852\n","epoch 8\tavg epoch loss = 0.2153\tavg epoch acc = 0.985\n","epoch 9\tavg epoch loss = 0.2344\tavg epoch acc = 0.9838\n","training took 28.29 s\n","Avg test loss = 0.0793\tAvg test acc = 0.974\n","epoch 0\tavg epoch loss = 0.8808\tavg epoch acc = 0.8801\n","epoch 1\tavg epoch loss = 0.2958\tavg epoch acc = 0.977\n","epoch 2\tavg epoch loss = 0.2227\tavg epoch acc = 0.9836\n","epoch 3\tavg epoch loss = 0.2058\tavg epoch acc = 0.9846\n","epoch 4\tavg epoch loss = 0.2002\tavg epoch acc = 0.9849\n","epoch 5\tavg epoch loss = 0.2183\tavg epoch acc = 0.9841\n","epoch 6\tavg epoch loss = 0.2287\tavg epoch acc = 0.9841\n","epoch 7\tavg epoch loss = 0.2477\tavg epoch acc = 0.9828\n","epoch 8\tavg epoch loss = 0.2318\tavg epoch acc = 0.9838\n","epoch 9\tavg epoch loss = 0.2386\tavg epoch acc = 0.9843\n","training took 28.28 s\n","Avg test loss = 0.0739\tAvg test acc = 0.977\n","epoch 0\tavg epoch loss = 0.8869\tavg epoch acc = 0.8767\n","epoch 1\tavg epoch loss = 0.3121\tavg epoch acc = 0.9749\n","epoch 2\tavg epoch loss = 0.2321\tavg epoch acc = 0.9828\n","epoch 3\tavg epoch loss = 0.2189\tavg epoch acc = 0.9851\n","epoch 4\tavg epoch loss = 0.2039\tavg epoch acc = 0.9856\n","epoch 5\tavg epoch loss = 0.2105\tavg epoch acc = 0.9858\n","epoch 6\tavg epoch loss = 0.2193\tavg epoch acc = 0.9847\n","epoch 7\tavg epoch loss = 0.2273\tavg epoch acc = 0.9844\n","epoch 8\tavg epoch loss = 0.2449\tavg epoch acc = 0.9834\n","epoch 9\tavg epoch loss = 0.2332\tavg epoch acc = 0.9844\n","training took 28.26 s\n","Avg test loss = 0.0869\tAvg test acc = 0.974\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["with open('./res/prot_res_adam_new1.json', 'w', encoding ='utf8') as json_file:\n","    json.dump(results_adam_prot, json_file, indent=2)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import json"]},{"cell_type":"markdown","metadata":{"id":"wanted-grocery"},"source":["### Nesterov"],"id":"wanted-grocery"},{"cell_type":"code","metadata":{"id":"conditional-mirror","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a83bcd07-87e2-447a-e62e-5994ede0b954","tags":["outputPrepend"]},"source":["search_grid_nesterov = {\n","    'lr': np.logspace(0, 1, num=15),\n","    'batch_size': [32, 64, 128]\n","}\n","\n","if prot_hyperparameter_tune:\n","    results_nesterov_prot = tune_optimizer(\n","        model=Net().to(device),\n","        optim_fun=NesterovOptimizer,\n","        xtrain=train_dataset.data,\n","        ytrain=train_dataset.targets,\n","        search_grid=search_grid_nesterov,\n","        nfolds=3,\n","        func=protected_training,\n","        **training_config\n","    )\n","\n","else:\n","    results_nesterov_prot = optimizers[NesterovOptimizer]"],"id":"conditional-mirror","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["avg epoch acc = 0.09903\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09903\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09903\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09903\n","training took 35.77 s\n","Avg test loss = nan\tAvg test acc = 0.0981\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09947\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09788\n","training took 35.77 s\n","Avg test loss = nan\tAvg test acc = 0.1\n","epoch 0\tavg epoch loss = 2.968e+12\tavg epoch acc = 0.09845\n","epoch 1\tavg epoch loss = 2.44\tavg epoch acc = 0.09835\n","epoch 2\tavg epoch loss = 2.433\tavg epoch acc = 0.09817\n","epoch 3\tavg epoch loss = 2.433\tavg epoch acc = 0.09817\n","epoch 4\tavg epoch loss = 2.433\tavg epoch acc = 0.09817\n","epoch 5\tavg epoch loss = 2.433\tavg epoch acc = 0.09817\n","epoch 6\tavg epoch loss = 2.433\tavg epoch acc = 0.09813\n","epoch 7\tavg epoch loss = 2.433\tavg epoch acc = 0.09813\n","epoch 8\tavg epoch loss = 2.433\tavg epoch acc = 0.09813\n","epoch 9\tavg epoch loss = 2.433\tavg epoch acc = 0.09813\n","training took 35.81 s\n","Avg test loss = 2.4\tAvg test acc = 0.0978\n","{'lr': 6.1054022965853285, 'batch_size': 128}\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09834\n","training took 27.69 s\n","Avg test loss = nan\tAvg test acc = 0.0994\n","epoch 0\tavg epoch loss = 8.262\tavg epoch acc = 0.09952\n","epoch 1\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","epoch 2\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","epoch 3\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","epoch 4\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","epoch 5\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","epoch 6\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","epoch 7\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","epoch 8\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","epoch 9\tavg epoch loss = 2.355\tavg epoch acc = 0.09964\n","training took 27.7 s\n","Avg test loss = 2.37\tAvg test acc = 0.0977\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09944\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09949\n","training took 27.7 s\n","Avg test loss = nan\tAvg test acc = 0.097\n","{'lr': 7.196856730011519, 'batch_size': 32}\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09915\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09945\n","training took 53.05 s\n","Avg test loss = nan\tAvg test acc = 0.0973\n","epoch 0\tavg epoch loss = 1.571e+12\tavg epoch acc = 0.09875\n","epoch 1\tavg epoch loss = 1.363e+10\tavg epoch acc = 0.09663\n","epoch 2\tavg epoch loss = 2.584e+04\tavg epoch acc = 0.09823\n","epoch 3\tavg epoch loss = 3.145\tavg epoch acc = 0.09742\n","epoch 4\tavg epoch loss = 3.13\tavg epoch acc = 0.09757\n","epoch 5\tavg epoch loss = 3.122\tavg epoch acc = 0.09845\n","epoch 6\tavg epoch loss = 3.127\tavg epoch acc = 0.09792\n","epoch 7\tavg epoch loss = 3.102\tavg epoch acc = 0.09748\n","epoch 8\tavg epoch loss = 3.129\tavg epoch acc = 0.09632\n","epoch 9\tavg epoch loss = 3.156\tavg epoch acc = 0.0982\n","training took 52.82 s\n","Avg test loss = 2.67\tAvg test acc = 0.0983\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09795\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.0982\n","training took 53.13 s\n","Avg test loss = nan\tAvg test acc = 0.0998\n","{'lr': 7.196856730011519, 'batch_size': 64}\n","epoch 0\tavg epoch loss = 748.3\tavg epoch acc = 0.09813\n","epoch 1\tavg epoch loss = 2.508\tavg epoch acc = 0.09652\n","epoch 2\tavg epoch loss = 2.509\tavg epoch acc = 0.09652\n","epoch 3\tavg epoch loss = 2.509\tavg epoch acc = 0.09652\n","epoch 4\tavg epoch loss = 2.509\tavg epoch acc = 0.09652\n","epoch 5\tavg epoch loss = 2.509\tavg epoch acc = 0.09652\n","epoch 6\tavg epoch loss = 2.509\tavg epoch acc = 0.09652\n","epoch 7\tavg epoch loss = 2.509\tavg epoch acc = 0.09652\n","epoch 8\tavg epoch loss = 2.509\tavg epoch acc = 0.09652\n","epoch 9\tavg epoch loss = 2.509\tavg epoch acc = 0.09652\n","training took 35.8 s\n","Avg test loss = 12.3\tAvg test acc = 0.0998\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09845\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09648\n","training took 35.79 s\n","Avg test loss = nan\tAvg test acc = 0.103\n","epoch 0\tavg epoch loss = 1.374e+04\tavg epoch acc = 0.0978\n","epoch 1\tavg epoch loss = 3.168\tavg epoch acc = 0.09638\n","epoch 2\tavg epoch loss = 2.532\tavg epoch acc = 0.09667\n","epoch 3\tavg epoch loss = 2.408e+04\tavg epoch acc = 0.09748\n","epoch 4\tavg epoch loss = 5.388e+06\tavg epoch acc = 0.09782\n","epoch 5\tavg epoch loss = 3.133e+11\tavg epoch acc = 0.09665\n","epoch 6\tavg epoch loss = 1.252e+10\tavg epoch acc = 0.0985\n","epoch 7\tavg epoch loss = 3.213e+07\tavg epoch acc = 0.09605\n","epoch 8\tavg epoch loss = 7.134e+08\tavg epoch acc = 0.09782\n","epoch 9\tavg epoch loss = 1.012e+05\tavg epoch acc = 0.09667\n","training took 35.88 s\n","Avg test loss = 2.43\tAvg test acc = 0.0943\n","{'lr': 7.196856730011519, 'batch_size': 128}\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09727\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09712\n","training took 27.69 s\n","Avg test loss = nan\tAvg test acc = 0.102\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.1016\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09974\n","training took 27.71 s\n","Avg test loss = nan\tAvg test acc = 0.0969\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09927\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09929\n","training took 27.77 s\n","Avg test loss = nan\tAvg test acc = 0.0974\n","{'lr': 8.483428982440719, 'batch_size': 32}\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.1004\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.1001\n","training took 54.29 s\n","Avg test loss = nan\tAvg test acc = 0.0959\n","epoch 0\tavg epoch loss = 18.11\tavg epoch acc = 0.09905\n","epoch 1\tavg epoch loss = 5.758\tavg epoch acc = 0.09677\n","epoch 2\tavg epoch loss = 5.908\tavg epoch acc = 0.0982\n","epoch 3\tavg epoch loss = 6.54\tavg epoch acc = 0.09955\n","epoch 4\tavg epoch loss = 6.066\tavg epoch acc = 0.09467\n","epoch 5\tavg epoch loss = 5.957\tavg epoch acc = 0.09917\n","epoch 6\tavg epoch loss = 5.798\tavg epoch acc = 0.0987\n","epoch 7\tavg epoch loss = 6.049\tavg epoch acc = 0.09867\n","epoch 8\tavg epoch loss = 6.535\tavg epoch acc = 0.0971\n","epoch 9\tavg epoch loss = 5.783\tavg epoch acc = 0.09685\n","training took 54.39 s\n","Avg test loss = 3.38\tAvg test acc = 0.0996\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09763\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09813\n","training took 54.68 s\n","Avg test loss = nan\tAvg test acc = 0.0999\n","{'lr': 8.483428982440719, 'batch_size': 64}\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09775\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09773\n","training took 36.75 s\n","Avg test loss = nan\tAvg test acc = 0.101\n","epoch 0\tavg epoch loss = 2.642e+15\tavg epoch acc = 0.09885\n","epoch 1\tavg epoch loss = 2.878\tavg epoch acc = 0.09707\n","epoch 2\tavg epoch loss = 2.801\tavg epoch acc = 0.09602\n","epoch 3\tavg epoch loss = 2.804\tavg epoch acc = 0.09598\n","epoch 4\tavg epoch loss = 2.806\tavg epoch acc = 0.09607\n","epoch 5\tavg epoch loss = 2.852\tavg epoch acc = 0.09617\n","epoch 6\tavg epoch loss = 2.798\tavg epoch acc = 0.09532\n","epoch 7\tavg epoch loss = 2.806\tavg epoch acc = 0.09575\n","epoch 8\tavg epoch loss = 2.81\tavg epoch acc = 0.0959\n","epoch 9\tavg epoch loss = 2.818\tavg epoch acc = 0.095\n","training took 36.8 s\n","Avg test loss = 2.77\tAvg test acc = 0.101\n","epoch 0\tavg epoch loss = 1.964e+25\tavg epoch acc = 0.09775\n","epoch 1\tavg epoch loss = 3.824\tavg epoch acc = 0.0996\n","epoch 2\tavg epoch loss = 2.899\tavg epoch acc = 0.09925\n","epoch 3\tavg epoch loss = 2.946\tavg epoch acc = 0.09935\n","epoch 4\tavg epoch loss = 2.858\tavg epoch acc = 0.09897\n","epoch 5\tavg epoch loss = 2.854\tavg epoch acc = 0.09938\n","epoch 6\tavg epoch loss = 2.867\tavg epoch acc = 0.0995\n","epoch 7\tavg epoch loss = 2.896\tavg epoch acc = 0.09832\n","epoch 8\tavg epoch loss = 2.985\tavg epoch acc = 0.09855\n","epoch 9\tavg epoch loss = 2.84\tavg epoch acc = 0.1001\n","training took 36.77 s\n","Avg test loss = 2.68\tAvg test acc = 0.0951\n","{'lr': 8.483428982440719, 'batch_size': 128}\n","epoch 0\tavg epoch loss = 9.602e+13\tavg epoch acc = 0.1012\n","epoch 1\tavg epoch loss = 1.178e+09\tavg epoch acc = 0.09934\n","epoch 2\tavg epoch loss = 2.13e+07\tavg epoch acc = 0.09757\n","epoch 3\tavg epoch loss = 2.431\tavg epoch acc = 0.09784\n","epoch 4\tavg epoch loss = 2.431\tavg epoch acc = 0.09784\n","epoch 5\tavg epoch loss = 2.431\tavg epoch acc = 0.09784\n","epoch 6\tavg epoch loss = 2.431\tavg epoch acc = 0.09784\n","epoch 7\tavg epoch loss = 2.432\tavg epoch acc = 0.09784\n","epoch 8\tavg epoch loss = 2.432\tavg epoch acc = 0.09784\n","epoch 9\tavg epoch loss = 2.432\tavg epoch acc = 0.09784\n","training took 28.56 s\n","Avg test loss = 2.36\tAvg test acc = 0.101\n","epoch 0\tavg epoch loss = 3.282\tavg epoch acc = 0.09864\n","epoch 1\tavg epoch loss = 2.439\tavg epoch acc = 0.09849\n","epoch 2\tavg epoch loss = 2.436\tavg epoch acc = 0.09874\n","epoch 3\tavg epoch loss = 2.437\tavg epoch acc = 0.09879\n","epoch 4\tavg epoch loss = 2.437\tavg epoch acc = 0.09879\n","epoch 5\tavg epoch loss = 2.437\tavg epoch acc = 0.09879\n","epoch 6\tavg epoch loss = 2.437\tavg epoch acc = 0.09879\n","epoch 7\tavg epoch loss = 2.437\tavg epoch acc = 0.09879\n","epoch 8\tavg epoch loss = 2.437\tavg epoch acc = 0.09879\n","epoch 9\tavg epoch loss = 2.437\tavg epoch acc = 0.09879\n","training took 28.72 s\n","Avg test loss = 2.47\tAvg test acc = 0.0958\n","epoch 0\tavg epoch loss = 8.707e+12\tavg epoch acc = 0.09897\n","epoch 1\tavg epoch loss = 2.488e+13\tavg epoch acc = 0.1002\n","epoch 2\tavg epoch loss = 4.479e+10\tavg epoch acc = 0.09927\n","epoch 3\tavg epoch loss = 2.437\tavg epoch acc = 0.09859\n","epoch 4\tavg epoch loss = 2.437\tavg epoch acc = 0.09859\n","epoch 5\tavg epoch loss = 2.437\tavg epoch acc = 0.09859\n","epoch 6\tavg epoch loss = 2.437\tavg epoch acc = 0.09859\n","epoch 7\tavg epoch loss = 2.437\tavg epoch acc = 0.09859\n","epoch 8\tavg epoch loss = 2.437\tavg epoch acc = 0.09859\n","epoch 9\tavg epoch loss = 2.437\tavg epoch acc = 0.09859\n","training took 28.74 s\n","Avg test loss = 2.44\tAvg test acc = 0.0999\n","{'lr': 10.0, 'batch_size': 32}\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.0999\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09972\n","training took 54.72 s\n","Avg test loss = nan\tAvg test acc = 0.0967\n","epoch 0\tavg epoch loss = 1.929e+08\tavg epoch acc = 0.09792\n","epoch 1\tavg epoch loss = 3.293e+19\tavg epoch acc = 0.09717\n","epoch 2\tavg epoch loss = 6.642e+16\tavg epoch acc = 0.1005\n","epoch 3\tavg epoch loss = 2.053e+16\tavg epoch acc = 0.0992\n","epoch 4\tavg epoch loss = 11.37\tavg epoch acc = 0.09763\n","epoch 5\tavg epoch loss = 11.19\tavg epoch acc = 0.09955\n","epoch 6\tavg epoch loss = 6.235e+13\tavg epoch acc = 0.09847\n","epoch 7\tavg epoch loss = 12.32\tavg epoch acc = 0.09835\n","epoch 8\tavg epoch loss = 12.13\tavg epoch acc = 0.09835\n","epoch 9\tavg epoch loss = 12.24\tavg epoch acc = 0.09823\n","training took 54.67 s\n","Avg test loss = 10.2\tAvg test acc = 0.0984\n","epoch 0\tavg epoch loss = 2.746e+10\tavg epoch acc = 0.09715\n","epoch 1\tavg epoch loss = 12.64\tavg epoch acc = 0.09725\n","epoch 2\tavg epoch loss = 12.57\tavg epoch acc = 0.0978\n","epoch 3\tavg epoch loss = 12.03\tavg epoch acc = 0.1014\n","epoch 4\tavg epoch loss = 11.37\tavg epoch acc = 0.0956\n","epoch 5\tavg epoch loss = 12.05\tavg epoch acc = 0.1013\n","epoch 6\tavg epoch loss = 11.79\tavg epoch acc = 0.09765\n","epoch 7\tavg epoch loss = 11.94\tavg epoch acc = 0.0981\n","epoch 8\tavg epoch loss = 11.84\tavg epoch acc = 0.09828\n","epoch 9\tavg epoch loss = 12.19\tavg epoch acc = 0.0973\n","training took 54.91 s\n","Avg test loss = 1.91e+12\tAvg test acc = 0.0998\n","{'lr': 10.0, 'batch_size': 64}\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09878\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.0985\n","training took 36.76 s\n","Avg test loss = nan\tAvg test acc = 0.0992\n","epoch 0\tavg epoch loss = 79.48\tavg epoch acc = 0.099\n","epoch 1\tavg epoch loss = 8.301\tavg epoch acc = 0.09953\n","epoch 2\tavg epoch loss = 8.118\tavg epoch acc = 0.0967\n","epoch 3\tavg epoch loss = 8.529\tavg epoch acc = 0.09757\n","epoch 4\tavg epoch loss = 275.3\tavg epoch acc = 0.1013\n","epoch 5\tavg epoch loss = 7.639\tavg epoch acc = 0.1025\n","epoch 6\tavg epoch loss = 8.392\tavg epoch acc = 0.09995\n","epoch 7\tavg epoch loss = 8.876\tavg epoch acc = 0.09842\n","epoch 8\tavg epoch loss = 6.993\tavg epoch acc = 0.1009\n","epoch 9\tavg epoch loss = 10.4\tavg epoch acc = 0.1009\n","training took 36.79 s\n","Avg test loss = 9.98\tAvg test acc = 0.0969\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09807\n","training took 36.86 s\n","Avg test loss = nan\tAvg test acc = 0.1\n","{'lr': 10.0, 'batch_size': 128}\n","epoch 0\tavg epoch loss = 6.094e+19\tavg epoch acc = 0.1002\n","epoch 1\tavg epoch loss = 2.62\tavg epoch acc = 0.09897\n","epoch 2\tavg epoch loss = 2.65\tavg epoch acc = 0.09907\n","epoch 3\tavg epoch loss = 2.635\tavg epoch acc = 0.09894\n","epoch 4\tavg epoch loss = 2.632\tavg epoch acc = 0.09984\n","epoch 5\tavg epoch loss = 2.631\tavg epoch acc = 0.09964\n","epoch 6\tavg epoch loss = 2.639\tavg epoch acc = 0.09934\n","epoch 7\tavg epoch loss = 2.666\tavg epoch acc = 0.09917\n","epoch 8\tavg epoch loss = 2.639\tavg epoch acc = 0.09944\n","epoch 9\tavg epoch loss = 2.664\tavg epoch acc = 0.09909\n","training took 28.66 s\n","Avg test loss = 2.51\tAvg test acc = 0.0974\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09847\n","training took 28.62 s\n","Avg test loss = nan\tAvg test acc = 0.0991\n","epoch 0\tavg epoch loss = nan\tavg epoch acc = 0.1005\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09979\n","training took 28.68 s\n","Avg test loss = nan\tAvg test acc = 0.0968\n"]}]},{"cell_type":"code","metadata":{"id":"6wdJaMVu8YKC"},"source":["with open('./res/prot_res_nesterov_new1.json', 'w', encoding ='utf8') as json_file:\n","    json.dump(results_nesterov_prot, json_file, indent=2)"],"id":"6wdJaMVu8YKC","execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rental-bidding"},"source":["### Minibatch"],"id":"rental-bidding"},{"cell_type":"code","metadata":{"id":"liberal-tucson","tags":["outputPrepend"]},"source":["search_grid_mini  = {\n","        'lr': np.logspace(-4,-1,num=10),\n","        'batch_size': [64, 128],\n","        'decreasing_lr': [0, 1],\n","    }\n","if prot_hyperparameter_tune:\n","    results_minibatch_prot = tune_optimizer(\n","        model=Net().to(device),\n","        optim_fun=MiniBatchOptimizer,\n","        xtrain=train_dataset.data,\n","        ytrain=train_dataset.targets,\n","        search_grid=search_grid_mini,\n","        nfolds=3,\n","        func=protected_training,\n","        **training_config\n","    )\n","\n","else:\n","    results_minibatch_prot = optimizers[MiniBatchOptimizer]"],"id":"liberal-tucson","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\tavg epoch loss = 2.377\tavg epoch acc = 0.1401\n","epoch 2\tavg epoch loss = 2.375\tavg epoch acc = 0.1507\n","epoch 3\tavg epoch loss = 2.373\tavg epoch acc = 0.1587\n","epoch 4\tavg epoch loss = 2.372\tavg epoch acc = 0.1644\n","epoch 5\tavg epoch loss = 2.371\tavg epoch acc = 0.1684\n","epoch 6\tavg epoch loss = 2.37\tavg epoch acc = 0.1729\n","epoch 7\tavg epoch loss = 2.37\tavg epoch acc = 0.1757\n","epoch 8\tavg epoch loss = 2.369\tavg epoch acc = 0.179\n","epoch 9\tavg epoch loss = 2.369\tavg epoch acc = 0.1817\n","training took 34.51 s\n","Avg test loss = 2.27\tAvg test acc = 0.184\n","epoch 0\tavg epoch loss = 2.384\tavg epoch acc = 0.1435\n","epoch 1\tavg epoch loss = 2.377\tavg epoch acc = 0.1685\n","epoch 2\tavg epoch loss = 2.374\tavg epoch acc = 0.1777\n","epoch 3\tavg epoch loss = 2.372\tavg epoch acc = 0.1847\n","epoch 4\tavg epoch loss = 2.371\tavg epoch acc = 0.1904\n","epoch 5\tavg epoch loss = 2.37\tavg epoch acc = 0.1945\n","epoch 6\tavg epoch loss = 2.37\tavg epoch acc = 0.1978\n","epoch 7\tavg epoch loss = 2.369\tavg epoch acc = 0.2008\n","epoch 8\tavg epoch loss = 2.368\tavg epoch acc = 0.2033\n","epoch 9\tavg epoch loss = 2.368\tavg epoch acc = 0.2052\n","training took 34.45 s\n","Avg test loss = 2.28\tAvg test acc = 0.203\n","{'lr': 0.021544346900318822, 'batch_size': 128, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 1.38\tavg epoch acc = 0.7577\n","epoch 1\tavg epoch loss = 0.5977\tavg epoch acc = 0.9468\n","epoch 2\tavg epoch loss = 0.4385\tavg epoch acc = 0.9668\n","epoch 3\tavg epoch loss = 0.3643\tavg epoch acc = 0.9737\n","epoch 4\tavg epoch loss = 0.3182\tavg epoch acc = 0.9788\n","epoch 5\tavg epoch loss = 0.2859\tavg epoch acc = 0.9818\n","epoch 6\tavg epoch loss = 0.2612\tavg epoch acc = 0.9841\n","epoch 7\tavg epoch loss = 0.2416\tavg epoch acc = 0.9855\n","epoch 8\tavg epoch loss = 0.2255\tavg epoch acc = 0.9869\n","epoch 9\tavg epoch loss = 0.212\tavg epoch acc = 0.9879\n","training took 26.37 s\n","Avg test loss = 0.0656\tAvg test acc = 0.979\n","epoch 0\tavg epoch loss = 1.371\tavg epoch acc = 0.7572\n","epoch 1\tavg epoch loss = 0.5909\tavg epoch acc = 0.9475\n","epoch 2\tavg epoch loss = 0.4322\tavg epoch acc = 0.9673\n","epoch 3\tavg epoch loss = 0.3582\tavg epoch acc = 0.9753\n","epoch 4\tavg epoch loss = 0.313\tavg epoch acc = 0.9793\n","epoch 5\tavg epoch loss = 0.2808\tavg epoch acc = 0.9824\n","epoch 6\tavg epoch loss = 0.2561\tavg epoch acc = 0.9844\n","epoch 7\tavg epoch loss = 0.2366\tavg epoch acc = 0.986\n","epoch 8\tavg epoch loss = 0.2205\tavg epoch acc = 0.9876\n","epoch 9\tavg epoch loss = 0.207\tavg epoch acc = 0.9885\n","training took 26.42 s\n","Avg test loss = 0.059\tAvg test acc = 0.982\n","epoch 0\tavg epoch loss = 1.368\tavg epoch acc = 0.7581\n","epoch 1\tavg epoch loss = 0.5929\tavg epoch acc = 0.9477\n","epoch 2\tavg epoch loss = 0.4357\tavg epoch acc = 0.968\n","epoch 3\tavg epoch loss = 0.3621\tavg epoch acc = 0.975\n","epoch 4\tavg epoch loss = 0.3166\tavg epoch acc = 0.9791\n","epoch 5\tavg epoch loss = 0.2841\tavg epoch acc = 0.9824\n","epoch 6\tavg epoch loss = 0.2592\tavg epoch acc = 0.9846\n","epoch 7\tavg epoch loss = 0.2395\tavg epoch acc = 0.986\n","epoch 8\tavg epoch loss = 0.2233\tavg epoch acc = 0.9873\n","epoch 9\tavg epoch loss = 0.2095\tavg epoch acc = 0.9884\n","training took 26.42 s\n","Avg test loss = 0.065\tAvg test acc = 0.979\n","{'lr': 0.021544346900318822, 'batch_size': 128, 'decreasing_lr': 1}\n","epoch 0\tavg epoch loss = 2.389\tavg epoch acc = 0.1088\n","epoch 1\tavg epoch loss = 2.381\tavg epoch acc = 0.1239\n","epoch 2\tavg epoch loss = 2.379\tavg epoch acc = 0.1319\n","epoch 3\tavg epoch loss = 2.377\tavg epoch acc = 0.1374\n","epoch 4\tavg epoch loss = 2.376\tavg epoch acc = 0.1419\n","epoch 5\tavg epoch loss = 2.375\tavg epoch acc = 0.1453\n","epoch 6\tavg epoch loss = 2.374\tavg epoch acc = 0.1486\n","epoch 7\tavg epoch loss = 2.374\tavg epoch acc = 0.1514\n","epoch 8\tavg epoch loss = 2.373\tavg epoch acc = 0.1538\n","epoch 9\tavg epoch loss = 2.372\tavg epoch acc = 0.1558\n","training took 26.29 s\n","Avg test loss = 2.28\tAvg test acc = 0.16\n","epoch 0\tavg epoch loss = 2.388\tavg epoch acc = 0.1092\n","epoch 1\tavg epoch loss = 2.381\tavg epoch acc = 0.1244\n","epoch 2\tavg epoch loss = 2.378\tavg epoch acc = 0.1321\n","epoch 3\tavg epoch loss = 2.377\tavg epoch acc = 0.1372\n","epoch 4\tavg epoch loss = 2.376\tavg epoch acc = 0.1419\n","epoch 5\tavg epoch loss = 2.375\tavg epoch acc = 0.1455\n","epoch 6\tavg epoch loss = 2.374\tavg epoch acc = 0.1485\n","epoch 7\tavg epoch loss = 2.373\tavg epoch acc = 0.151\n","epoch 8\tavg epoch loss = 2.372\tavg epoch acc = 0.1538\n","epoch 9\tavg epoch loss = 2.372\tavg epoch acc = 0.1561\n","training took 26.37 s\n","Avg test loss = 2.28\tAvg test acc = 0.155\n","epoch 0\tavg epoch loss = 2.388\tavg epoch acc = 0.1166\n","epoch 1\tavg epoch loss = 2.38\tavg epoch acc = 0.137\n","epoch 2\tavg epoch loss = 2.378\tavg epoch acc = 0.146\n","epoch 3\tavg epoch loss = 2.376\tavg epoch acc = 0.1531\n","epoch 4\tavg epoch loss = 2.375\tavg epoch acc = 0.1586\n","epoch 5\tavg epoch loss = 2.374\tavg epoch acc = 0.1632\n","epoch 6\tavg epoch loss = 2.373\tavg epoch acc = 0.1667\n","epoch 7\tavg epoch loss = 2.372\tavg epoch acc = 0.17\n","epoch 8\tavg epoch loss = 2.372\tavg epoch acc = 0.1729\n","epoch 9\tavg epoch loss = 2.371\tavg epoch acc = 0.1752\n","training took 26.4 s\n","Avg test loss = 2.28\tAvg test acc = 0.174\n","{'lr': 0.046415888336127774, 'batch_size': 64, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.7306\tavg epoch acc = 0.8966\n","epoch 1\tavg epoch loss = 0.2974\tavg epoch acc = 0.9798\n","epoch 2\tavg epoch loss = 0.2245\tavg epoch acc = 0.9863\n","epoch 3\tavg epoch loss = 0.1849\tavg epoch acc = 0.9895\n","epoch 4\tavg epoch loss = 0.1584\tavg epoch acc = 0.9913\n","epoch 5\tavg epoch loss = 0.1389\tavg epoch acc = 0.993\n","epoch 6\tavg epoch loss = 0.1237\tavg epoch acc = 0.994\n","epoch 7\tavg epoch loss = 0.1111\tavg epoch acc = 0.995\n","epoch 8\tavg epoch loss = 0.09982\tavg epoch acc = 0.9956\n","epoch 9\tavg epoch loss = 0.09023\tavg epoch acc = 0.9962\n","training took 34.49 s\n","Avg test loss = 0.0314\tAvg test acc = 0.99\n","epoch 0\tavg epoch loss = 0.7414\tavg epoch acc = 0.8961\n","epoch 1\tavg epoch loss = 0.3041\tavg epoch acc = 0.9787\n","epoch 2\tavg epoch loss = 0.2287\tavg epoch acc = 0.9855\n","epoch 3\tavg epoch loss = 0.1896\tavg epoch acc = 0.9886\n","epoch 4\tavg epoch loss = 0.1626\tavg epoch acc = 0.9909\n","epoch 5\tavg epoch loss = 0.1422\tavg epoch acc = 0.9928\n","epoch 6\tavg epoch loss = 0.1261\tavg epoch acc = 0.9938\n","epoch 7\tavg epoch loss = 0.113\tavg epoch acc = 0.9948\n","epoch 8\tavg epoch loss = 0.1013\tavg epoch acc = 0.9954\n","epoch 9\tavg epoch loss = 0.09136\tavg epoch acc = 0.9963\n","training took 34.48 s\n","Avg test loss = 0.0372\tAvg test acc = 0.989\n","epoch 0\tavg epoch loss = 0.7344\tavg epoch acc = 0.9002\n","epoch 1\tavg epoch loss = 0.3045\tavg epoch acc = 0.9773\n","epoch 2\tavg epoch loss = 0.2302\tavg epoch acc = 0.9846\n","epoch 3\tavg epoch loss = 0.1903\tavg epoch acc = 0.9887\n","epoch 4\tavg epoch loss = 0.1629\tavg epoch acc = 0.9905\n","epoch 5\tavg epoch loss = 0.1428\tavg epoch acc = 0.9928\n","epoch 6\tavg epoch loss = 0.1272\tavg epoch acc = 0.9939\n","epoch 7\tavg epoch loss = 0.1139\tavg epoch acc = 0.9949\n","epoch 8\tavg epoch loss = 0.1026\tavg epoch acc = 0.9957\n","epoch 9\tavg epoch loss = 0.09326\tavg epoch acc = 0.9964\n","training took 34.5 s\n","Avg test loss = 0.044\tAvg test acc = 0.986\n","{'lr': 0.046415888336127774, 'batch_size': 64, 'decreasing_lr': 1}\n","epoch 0\tavg epoch loss = 2.354\tavg epoch acc = 0.2489\n","epoch 1\tavg epoch loss = 2.341\tavg epoch acc = 0.3111\n","epoch 2\tavg epoch loss = 2.337\tavg epoch acc = 0.3372\n","epoch 3\tavg epoch loss = 2.334\tavg epoch acc = 0.3525\n","epoch 4\tavg epoch loss = 2.332\tavg epoch acc = 0.3635\n","epoch 5\tavg epoch loss = 2.33\tavg epoch acc = 0.3717\n","epoch 6\tavg epoch loss = 2.329\tavg epoch acc = 0.3788\n","epoch 7\tavg epoch loss = 2.327\tavg epoch acc = 0.3849\n","epoch 8\tavg epoch loss = 2.326\tavg epoch acc = 0.3896\n","epoch 9\tavg epoch loss = 2.326\tavg epoch acc = 0.3934\n","training took 34.44 s\n","Avg test loss = 2.24\tAvg test acc = 0.392\n","epoch 0\tavg epoch loss = 2.353\tavg epoch acc = 0.2746\n","epoch 1\tavg epoch loss = 2.341\tavg epoch acc = 0.3382\n","epoch 2\tavg epoch loss = 2.337\tavg epoch acc = 0.3609\n","epoch 3\tavg epoch loss = 2.334\tavg epoch acc = 0.375\n","epoch 4\tavg epoch loss = 2.332\tavg epoch acc = 0.3855\n","epoch 5\tavg epoch loss = 2.33\tavg epoch acc = 0.3935\n","epoch 6\tavg epoch loss = 2.329\tavg epoch acc = 0.3992\n","epoch 7\tavg epoch loss = 2.328\tavg epoch acc = 0.4049\n","epoch 8\tavg epoch loss = 2.327\tavg epoch acc = 0.4095\n","epoch 9\tavg epoch loss = 2.326\tavg epoch acc = 0.4139\n","training took 34.39 s\n","Avg test loss = 2.24\tAvg test acc = 0.415\n","epoch 0\tavg epoch loss = 2.354\tavg epoch acc = 0.2543\n","epoch 1\tavg epoch loss = 2.342\tavg epoch acc = 0.3358\n","epoch 2\tavg epoch loss = 2.338\tavg epoch acc = 0.363\n","epoch 3\tavg epoch loss = 2.335\tavg epoch acc = 0.3803\n","epoch 4\tavg epoch loss = 2.333\tavg epoch acc = 0.3915\n","epoch 5\tavg epoch loss = 2.332\tavg epoch acc = 0.4005\n","epoch 6\tavg epoch loss = 2.33\tavg epoch acc = 0.4078\n","epoch 7\tavg epoch loss = 2.329\tavg epoch acc = 0.4135\n","epoch 8\tavg epoch loss = 2.328\tavg epoch acc = 0.4192\n","epoch 9\tavg epoch loss = 2.327\tavg epoch acc = 0.423\n","training took 34.49 s\n","Avg test loss = 2.24\tAvg test acc = 0.427\n","{'lr': 0.046415888336127774, 'batch_size': 128, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 1.011\tavg epoch acc = 0.8416\n","epoch 1\tavg epoch loss = 0.4151\tavg epoch acc = 0.9666\n","epoch 2\tavg epoch loss = 0.3106\tavg epoch acc = 0.9777\n","epoch 3\tavg epoch loss = 0.2581\tavg epoch acc = 0.9828\n","epoch 4\tavg epoch loss = 0.2242\tavg epoch acc = 0.986\n","epoch 5\tavg epoch loss = 0.1994\tavg epoch acc = 0.9883\n","epoch 6\tavg epoch loss = 0.1805\tavg epoch acc = 0.9898\n","epoch 7\tavg epoch loss = 0.165\tavg epoch acc = 0.9911\n","epoch 8\tavg epoch loss = 0.1521\tavg epoch acc = 0.9923\n","epoch 9\tavg epoch loss = 0.1409\tavg epoch acc = 0.9931\n","training took 27.29 s\n","Avg test loss = 0.0624\tAvg test acc = 0.979\n","epoch 0\tavg epoch loss = 1.006\tavg epoch acc = 0.8414\n","epoch 1\tavg epoch loss = 0.4139\tavg epoch acc = 0.9678\n","epoch 2\tavg epoch loss = 0.3083\tavg epoch acc = 0.9783\n","epoch 3\tavg epoch loss = 0.2554\tavg epoch acc = 0.9833\n","epoch 4\tavg epoch loss = 0.2218\tavg epoch acc = 0.9862\n","epoch 5\tavg epoch loss = 0.1972\tavg epoch acc = 0.9883\n","epoch 6\tavg epoch loss = 0.178\tavg epoch acc = 0.99\n","epoch 7\tavg epoch loss = 0.1626\tavg epoch acc = 0.9914\n","epoch 8\tavg epoch loss = 0.1497\tavg epoch acc = 0.9923\n","epoch 9\tavg epoch loss = 0.1385\tavg epoch acc = 0.9931\n","training took 28.32 s\n","Avg test loss = 0.0393\tAvg test acc = 0.987\n","epoch 0\tavg epoch loss = 1.009\tavg epoch acc = 0.8446\n","epoch 1\tavg epoch loss = 0.4123\tavg epoch acc = 0.9676\n","epoch 2\tavg epoch loss = 0.3089\tavg epoch acc = 0.9786\n","epoch 3\tavg epoch loss = 0.2569\tavg epoch acc = 0.9835\n","epoch 4\tavg epoch loss = 0.2235\tavg epoch acc = 0.9865\n","epoch 5\tavg epoch loss = 0.1993\tavg epoch acc = 0.9888\n","epoch 6\tavg epoch loss = 0.1805\tavg epoch acc = 0.9902\n","epoch 7\tavg epoch loss = 0.1652\tavg epoch acc = 0.9915\n","epoch 8\tavg epoch loss = 0.152\tavg epoch acc = 0.9924\n","epoch 9\tavg epoch loss = 0.1407\tavg epoch acc = 0.9932\n","training took 29.13 s\n","Avg test loss = 0.0492\tAvg test acc = 0.984\n","{'lr': 0.046415888336127774, 'batch_size': 128, 'decreasing_lr': 1}\n","epoch 0\tavg epoch loss = 2.36\tavg epoch acc = 0.2319\n","epoch 1\tavg epoch loss = 2.347\tavg epoch acc = 0.2991\n","epoch 2\tavg epoch loss = 2.343\tavg epoch acc = 0.3234\n","epoch 3\tavg epoch loss = 2.34\tavg epoch acc = 0.3402\n","epoch 4\tavg epoch loss = 2.338\tavg epoch acc = 0.3533\n","epoch 5\tavg epoch loss = 2.336\tavg epoch acc = 0.3615\n","epoch 6\tavg epoch loss = 2.335\tavg epoch acc = 0.3691\n","epoch 7\tavg epoch loss = 2.334\tavg epoch acc = 0.3755\n","epoch 8\tavg epoch loss = 2.333\tavg epoch acc = 0.3807\n","epoch 9\tavg epoch loss = 2.332\tavg epoch acc = 0.385\n","training took 28.83 s\n","Avg test loss = 2.24\tAvg test acc = 0.386\n","epoch 0\tavg epoch loss = 2.36\tavg epoch acc = 0.2101\n","epoch 1\tavg epoch loss = 2.347\tavg epoch acc = 0.2838\n","epoch 2\tavg epoch loss = 2.343\tavg epoch acc = 0.315\n","epoch 3\tavg epoch loss = 2.34\tavg epoch acc = 0.3359\n","epoch 4\tavg epoch loss = 2.338\tavg epoch acc = 0.3515\n","epoch 5\tavg epoch loss = 2.336\tavg epoch acc = 0.3622\n","epoch 6\tavg epoch loss = 2.335\tavg epoch acc = 0.3723\n","epoch 7\tavg epoch loss = 2.334\tavg epoch acc = 0.3796\n","epoch 8\tavg epoch loss = 2.333\tavg epoch acc = 0.3856\n","epoch 9\tavg epoch loss = 2.332\tavg epoch acc = 0.3918\n","training took 28.31 s\n","Avg test loss = 2.24\tAvg test acc = 0.395\n","epoch 0\tavg epoch loss = 2.36\tavg epoch acc = 0.2313\n","epoch 1\tavg epoch loss = 2.347\tavg epoch acc = 0.3047\n","epoch 2\tavg epoch loss = 2.342\tavg epoch acc = 0.3324\n","epoch 3\tavg epoch loss = 2.34\tavg epoch acc = 0.3504\n","epoch 4\tavg epoch loss = 2.337\tavg epoch acc = 0.3633\n","epoch 5\tavg epoch loss = 2.336\tavg epoch acc = 0.3737\n","epoch 6\tavg epoch loss = 2.334\tavg epoch acc = 0.3815\n","epoch 7\tavg epoch loss = 2.333\tavg epoch acc = 0.3885\n","epoch 8\tavg epoch loss = 2.332\tavg epoch acc = 0.3946\n","epoch 9\tavg epoch loss = 2.331\tavg epoch acc = 0.3989\n","training took 28.89 s\n","Avg test loss = 2.24\tAvg test acc = 0.402\n","{'lr': 0.1, 'batch_size': 64, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.5764\tavg epoch acc = 0.9239\n","epoch 1\tavg epoch loss = 0.2366\tavg epoch acc = 0.9834\n","epoch 2\tavg epoch loss = 0.1737\tavg epoch acc = 0.989\n","epoch 3\tavg epoch loss = 0.1395\tavg epoch acc = 0.9918\n","epoch 4\tavg epoch loss = 0.1146\tavg epoch acc = 0.994\n","epoch 5\tavg epoch loss = 0.09663\tavg epoch acc = 0.9952\n","epoch 6\tavg epoch loss = 0.08135\tavg epoch acc = 0.9963\n","epoch 7\tavg epoch loss = 0.07353\tavg epoch acc = 0.9969\n","epoch 8\tavg epoch loss = 0.06037\tavg epoch acc = 0.9976\n","epoch 9\tavg epoch loss = 0.05837\tavg epoch acc = 0.9978\n","training took 36.43 s\n","Avg test loss = 0.0336\tAvg test acc = 0.99\n","epoch 0\tavg epoch loss = 0.5801\tavg epoch acc = 0.9251\n","epoch 1\tavg epoch loss = 0.2334\tavg epoch acc = 0.9836\n","epoch 2\tavg epoch loss = 0.1704\tavg epoch acc = 0.9896\n","epoch 3\tavg epoch loss = 0.1345\tavg epoch acc = 0.9927\n","epoch 4\tavg epoch loss = 0.1104\tavg epoch acc = 0.9947\n","epoch 5\tavg epoch loss = 0.09304\tavg epoch acc = 0.9958\n","epoch 6\tavg epoch loss = 0.08208\tavg epoch acc = 0.9965\n","epoch 7\tavg epoch loss = 0.06937\tavg epoch acc = 0.9972\n","epoch 8\tavg epoch loss = 0.05904\tavg epoch acc = 0.9978\n","epoch 9\tavg epoch loss = 0.05264\tavg epoch acc = 0.9981\n","training took 35.15 s\n","Avg test loss = 0.0611\tAvg test acc = 0.986\n","epoch 0\tavg epoch loss = 0.5783\tavg epoch acc = 0.9237\n","epoch 1\tavg epoch loss = 0.2358\tavg epoch acc = 0.9834\n","epoch 2\tavg epoch loss = 0.1727\tavg epoch acc = 0.9888\n","epoch 3\tavg epoch loss = 0.1383\tavg epoch acc = 0.9919\n","epoch 4\tavg epoch loss = 0.1151\tavg epoch acc = 0.9936\n","epoch 5\tavg epoch loss = 0.09689\tavg epoch acc = 0.995\n","epoch 6\tavg epoch loss = 0.08271\tavg epoch acc = 0.9959\n","epoch 7\tavg epoch loss = 0.07009\tavg epoch acc = 0.997\n","epoch 8\tavg epoch loss = 0.06554\tavg epoch acc = 0.9975\n","epoch 9\tavg epoch loss = 0.06043\tavg epoch acc = 0.9978\n","training took 34.78 s\n","Avg test loss = 0.0421\tAvg test acc = 0.989\n","{'lr': 0.1, 'batch_size': 64, 'decreasing_lr': 1}\n","epoch 0\tavg epoch loss = 2.298\tavg epoch acc = 0.4507\n","epoch 1\tavg epoch loss = 2.271\tavg epoch acc = 0.5094\n","epoch 2\tavg epoch loss = 2.26\tavg epoch acc = 0.5275\n","epoch 3\tavg epoch loss = 2.252\tavg epoch acc = 0.54\n","epoch 4\tavg epoch loss = 2.246\tavg epoch acc = 0.5493\n","epoch 5\tavg epoch loss = 2.242\tavg epoch acc = 0.5561\n","epoch 6\tavg epoch loss = 2.237\tavg epoch acc = 0.5619\n","epoch 7\tavg epoch loss = 2.234\tavg epoch acc = 0.5676\n","epoch 8\tavg epoch loss = 2.231\tavg epoch acc = 0.5713\n","epoch 9\tavg epoch loss = 2.228\tavg epoch acc = 0.575\n","training took 35.68 s\n","Avg test loss = 2.12\tAvg test acc = 0.579\n","epoch 0\tavg epoch loss = 2.3\tavg epoch acc = 0.4589\n","epoch 1\tavg epoch loss = 2.274\tavg epoch acc = 0.5417\n","epoch 2\tavg epoch loss = 2.263\tavg epoch acc = 0.5603\n","epoch 3\tavg epoch loss = 2.256\tavg epoch acc = 0.5707\n","epoch 4\tavg epoch loss = 2.25\tavg epoch acc = 0.5776\n","epoch 5\tavg epoch loss = 2.245\tavg epoch acc = 0.5828\n","epoch 6\tavg epoch loss = 2.241\tavg epoch acc = 0.5863\n","epoch 7\tavg epoch loss = 2.237\tavg epoch acc = 0.5894\n","epoch 8\tavg epoch loss = 2.234\tavg epoch acc = 0.5921\n","epoch 9\tavg epoch loss = 2.231\tavg epoch acc = 0.5947\n","training took 37.1 s\n","Avg test loss = 2.12\tAvg test acc = 0.596\n","epoch 0\tavg epoch loss = 2.296\tavg epoch acc = 0.4981\n","epoch 1\tavg epoch loss = 2.269\tavg epoch acc = 0.5238\n","epoch 2\tavg epoch loss = 2.258\tavg epoch acc = 0.5349\n","epoch 3\tavg epoch loss = 2.25\tavg epoch acc = 0.5432\n","epoch 4\tavg epoch loss = 2.244\tavg epoch acc = 0.5497\n","epoch 5\tavg epoch loss = 2.239\tavg epoch acc = 0.5553\n","epoch 6\tavg epoch loss = 2.235\tavg epoch acc = 0.5601\n","epoch 7\tavg epoch loss = 2.231\tavg epoch acc = 0.5649\n","epoch 8\tavg epoch loss = 2.228\tavg epoch acc = 0.5686\n","epoch 9\tavg epoch loss = 2.225\tavg epoch acc = 0.5715\n","training took 39.51 s\n","Avg test loss = 2.11\tAvg test acc = 0.568\n","{'lr': 0.1, 'batch_size': 128, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.7739\tavg epoch acc = 0.8861\n","epoch 1\tavg epoch loss = 0.304\tavg epoch acc = 0.9782\n","epoch 2\tavg epoch loss = 0.2259\tavg epoch acc = 0.9851\n","epoch 3\tavg epoch loss = 0.1849\tavg epoch acc = 0.9891\n","epoch 4\tavg epoch loss = 0.1575\tavg epoch acc = 0.9913\n","epoch 5\tavg epoch loss = 0.1368\tavg epoch acc = 0.9932\n","epoch 6\tavg epoch loss = 0.1208\tavg epoch acc = 0.9942\n","epoch 7\tavg epoch loss = 0.1068\tavg epoch acc = 0.9952\n","epoch 8\tavg epoch loss = 0.0953\tavg epoch acc = 0.996\n","epoch 9\tavg epoch loss = 0.08494\tavg epoch acc = 0.9967\n","training took 30.08 s\n","Avg test loss = 0.0448\tAvg test acc = 0.987\n","epoch 0\tavg epoch loss = 0.7846\tavg epoch acc = 0.8851\n","epoch 1\tavg epoch loss = 0.3123\tavg epoch acc = 0.9767\n","epoch 2\tavg epoch loss = 0.2313\tavg epoch acc = 0.9845\n","epoch 3\tavg epoch loss = 0.1875\tavg epoch acc = 0.9886\n","epoch 4\tavg epoch loss = 0.1591\tavg epoch acc = 0.9909\n","epoch 5\tavg epoch loss = 0.1386\tavg epoch acc = 0.9922\n","epoch 6\tavg epoch loss = 0.1224\tavg epoch acc = 0.9934\n","epoch 7\tavg epoch loss = 0.1089\tavg epoch acc = 0.9945\n","epoch 8\tavg epoch loss = 0.09794\tavg epoch acc = 0.9953\n","epoch 9\tavg epoch loss = 0.08773\tavg epoch acc = 0.9959\n","training took 30.16 s\n","Avg test loss = 0.0393\tAvg test acc = 0.988\n","epoch 0\tavg epoch loss = 0.7728\tavg epoch acc = 0.8844\n","epoch 1\tavg epoch loss = 0.3063\tavg epoch acc = 0.9772\n","epoch 2\tavg epoch loss = 0.2292\tavg epoch acc = 0.985\n","epoch 3\tavg epoch loss = 0.1882\tavg epoch acc = 0.9888\n","epoch 4\tavg epoch loss = 0.1597\tavg epoch acc = 0.9911\n","epoch 5\tavg epoch loss = 0.1394\tavg epoch acc = 0.9927\n","epoch 6\tavg epoch loss = 0.1229\tavg epoch acc = 0.9937\n","epoch 7\tavg epoch loss = 0.1092\tavg epoch acc = 0.9948\n","epoch 8\tavg epoch loss = 0.09795\tavg epoch acc = 0.9955\n","epoch 9\tavg epoch loss = 0.08869\tavg epoch acc = 0.9961\n","training took 30.01 s\n","Avg test loss = 0.0435\tAvg test acc = 0.986\n","{'lr': 0.1, 'batch_size': 128, 'decreasing_lr': 1}\n","epoch 0\tavg epoch loss = 2.31\tavg epoch acc = 0.4337\n","epoch 1\tavg epoch loss = 2.285\tavg epoch acc = 0.5058\n","epoch 2\tavg epoch loss = 2.275\tavg epoch acc = 0.5204\n","epoch 3\tavg epoch loss = 2.268\tavg epoch acc = 0.5294\n","epoch 4\tavg epoch loss = 2.262\tavg epoch acc = 0.5355\n","epoch 5\tavg epoch loss = 2.258\tavg epoch acc = 0.5411\n","epoch 6\tavg epoch loss = 2.254\tavg epoch acc = 0.5457\n","epoch 7\tavg epoch loss = 2.251\tavg epoch acc = 0.5494\n","epoch 8\tavg epoch loss = 2.248\tavg epoch acc = 0.552\n","epoch 9\tavg epoch loss = 2.245\tavg epoch acc = 0.5549\n","training took 30.18 s\n","Avg test loss = 2.14\tAvg test acc = 0.555\n","epoch 0\tavg epoch loss = 2.31\tavg epoch acc = 0.488\n","epoch 1\tavg epoch loss = 2.285\tavg epoch acc = 0.5506\n","epoch 2\tavg epoch loss = 2.275\tavg epoch acc = 0.5654\n","epoch 3\tavg epoch loss = 2.268\tavg epoch acc = 0.5745\n","epoch 4\tavg epoch loss = 2.262\tavg epoch acc = 0.5807\n","epoch 5\tavg epoch loss = 2.258\tavg epoch acc = 0.5859\n","epoch 6\tavg epoch loss = 2.254\tavg epoch acc = 0.5891\n","epoch 7\tavg epoch loss = 2.251\tavg epoch acc = 0.5927\n","epoch 8\tavg epoch loss = 2.248\tavg epoch acc = 0.5955\n","epoch 9\tavg epoch loss = 2.245\tavg epoch acc = 0.598\n","training took 28.03 s\n","Avg test loss = 2.14\tAvg test acc = 0.604\n","epoch 0\tavg epoch loss = 2.31\tavg epoch acc = 0.4532\n","epoch 1\tavg epoch loss = 2.285\tavg epoch acc = 0.5224\n","epoch 2\tavg epoch loss = 2.275\tavg epoch acc = 0.5368\n","epoch 3\tavg epoch loss = 2.268\tavg epoch acc = 0.5455\n","epoch 4\tavg epoch loss = 2.263\tavg epoch acc = 0.5518\n","epoch 5\tavg epoch loss = 2.258\tavg epoch acc = 0.5566\n","epoch 6\tavg epoch loss = 2.255\tavg epoch acc = 0.5613\n","epoch 7\tavg epoch loss = 2.252\tavg epoch acc = 0.5646\n","epoch 8\tavg epoch loss = 2.249\tavg epoch acc = 0.5676\n","epoch 9\tavg epoch loss = 2.246\tavg epoch acc = 0.5701\n","training took 27.65 s\n","Avg test loss = 2.14\tAvg test acc = 0.566\n"]}]},{"cell_type":"code","metadata":{"id":"ePsa3cis9uFZ"},"source":["with open('./res/prot_res_minibatch_new2.json', 'w', encoding ='utf8') as json_file:\n","    json.dump(results_minibatch_prot, json_file, indent=2)"],"id":"ePsa3cis9uFZ","execution_count":26,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["836\n","epoch 2\tavg epoch loss = 0.1503\tavg epoch acc = 0.9895\n","epoch 3\tavg epoch loss = 0.1217\tavg epoch acc = 0.9926\n","epoch 4\tavg epoch loss = 0.09619\tavg epoch acc = 0.9946\n","epoch 5\tavg epoch loss = 0.07807\tavg epoch acc = 0.9962\n","epoch 6\tavg epoch loss = 0.07831\tavg epoch acc = 0.9967\n","epoch 7\tavg epoch loss = 0.06665\tavg epoch acc = 0.9976\n","epoch 8\tavg epoch loss = 0.06522\tavg epoch acc = 0.9978\n","epoch 9\tavg epoch loss = 0.07598\tavg epoch acc = 0.9977\n","training took 34.54 s\n","Avg test loss = 0.0621\tAvg test acc = 0.987\n","epoch 0\tavg epoch loss = 0.5165\tavg epoch acc = 0.9319\n","epoch 1\tavg epoch loss = 0.2065\tavg epoch acc = 0.9846\n","epoch 2\tavg epoch loss = 0.1569\tavg epoch acc = 0.9894\n","epoch 3\tavg epoch loss = 0.12\tavg epoch acc = 0.9927\n","epoch 4\tavg epoch loss = 0.09895\tavg epoch acc = 0.9947\n","epoch 5\tavg epoch loss = 0.08195\tavg epoch acc = 0.9957\n","epoch 6\tavg epoch loss = 0.0751\tavg epoch acc = 0.9967\n","epoch 7\tavg epoch loss = 0.06954\tavg epoch acc = 0.9973\n","epoch 8\tavg epoch loss = 0.0766\tavg epoch acc = 0.9974\n","epoch 9\tavg epoch loss = 0.05776\tavg epoch acc = 0.998\n","training took 34.51 s\n","Avg test loss = 0.0733\tAvg test acc = 0.985\n","{'lr': 0.20619860095022213, 'batch_size': 96, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.598\tavg epoch acc = 0.9136\n","epoch 1\tavg epoch loss = 0.2382\tavg epoch acc = 0.9821\n","epoch 2\tavg epoch loss = 0.1763\tavg epoch acc = 0.9885\n","epoch 3\tavg epoch loss = 0.1369\tavg epoch acc = 0.9915\n","epoch 4\tavg epoch loss = 0.1121\tavg epoch acc = 0.9934\n","epoch 5\tavg epoch loss = 0.09448\tavg epoch acc = 0.995\n","epoch 6\tavg epoch loss = 0.09045\tavg epoch acc = 0.9956\n","epoch 7\tavg epoch loss = 0.07639\tavg epoch acc = 0.9966\n","epoch 8\tavg epoch loss = 0.06328\tavg epoch acc = 0.9973\n","epoch 9\tavg epoch loss = 0.05467\tavg epoch acc = 0.9978\n","training took 30.33 s\n","Avg test loss = 0.0436\tAvg test acc = 0.989\n","epoch 0\tavg epoch loss = 0.6019\tavg epoch acc = 0.9159\n","epoch 1\tavg epoch loss = 0.231\tavg epoch acc = 0.9831\n","epoch 2\tavg epoch loss = 0.1684\tavg epoch acc = 0.989\n","epoch 3\tavg epoch loss = 0.1343\tavg epoch acc = 0.9922\n","epoch 4\tavg epoch loss = 0.1108\tavg epoch acc = 0.994\n","epoch 5\tavg epoch loss = 0.09249\tavg epoch acc = 0.9955\n","epoch 6\tavg epoch loss = 0.08296\tavg epoch acc = 0.9961\n","epoch 7\tavg epoch loss = 0.0716\tavg epoch acc = 0.997\n","epoch 8\tavg epoch loss = 0.06498\tavg epoch acc = 0.9976\n","epoch 9\tavg epoch loss = 0.06731\tavg epoch acc = 0.9976\n","training took 30.67 s\n","Avg test loss = 0.0549\tAvg test acc = 0.985\n","epoch 0\tavg epoch loss = 0.623\tavg epoch acc = 0.9099\n","epoch 1\tavg epoch loss = 0.2369\tavg epoch acc = 0.9821\n","epoch 2\tavg epoch loss = 0.1703\tavg epoch acc = 0.9881\n","epoch 3\tavg epoch loss = 0.1367\tavg epoch acc = 0.9917\n","epoch 4\tavg epoch loss = 0.109\tavg epoch acc = 0.9941\n","epoch 5\tavg epoch loss = 0.09121\tavg epoch acc = 0.9957\n","epoch 6\tavg epoch loss = 0.07786\tavg epoch acc = 0.9966\n","epoch 7\tavg epoch loss = 0.07669\tavg epoch acc = 0.9967\n","epoch 8\tavg epoch loss = 0.06198\tavg epoch acc = 0.9978\n","epoch 9\tavg epoch loss = 0.06802\tavg epoch acc = 0.9974\n","training took 30.05 s\n","Avg test loss = 0.0435\tAvg test acc = 0.988\n","{'lr': 0.20619860095022213, 'batch_size': 128, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.7043\tavg epoch acc = 0.8914\n","epoch 1\tavg epoch loss = 0.2675\tavg epoch acc = 0.98\n","epoch 2\tavg epoch loss = 0.1976\tavg epoch acc = 0.9867\n","epoch 3\tavg epoch loss = 0.1593\tavg epoch acc = 0.9901\n","epoch 4\tavg epoch loss = 0.1311\tavg epoch acc = 0.9927\n","epoch 5\tavg epoch loss = 0.1108\tavg epoch acc = 0.9942\n","epoch 6\tavg epoch loss = 0.09385\tavg epoch acc = 0.9953\n","epoch 7\tavg epoch loss = 0.08069\tavg epoch acc = 0.9964\n","epoch 8\tavg epoch loss = 0.06993\tavg epoch acc = 0.9972\n","epoch 9\tavg epoch loss = 0.0654\tavg epoch acc = 0.9976\n","training took 26.59 s\n","Avg test loss = 0.0433\tAvg test acc = 0.988\n","epoch 0\tavg epoch loss = 0.6643\tavg epoch acc = 0.9006\n","epoch 1\tavg epoch loss = 0.2461\tavg epoch acc = 0.9819\n","epoch 2\tavg epoch loss = 0.179\tavg epoch acc = 0.9885\n","epoch 3\tavg epoch loss = 0.1441\tavg epoch acc = 0.9916\n","epoch 4\tavg epoch loss = 0.1197\tavg epoch acc = 0.9935\n","epoch 5\tavg epoch loss = 0.1055\tavg epoch acc = 0.9949\n","epoch 6\tavg epoch loss = 0.0949\tavg epoch acc = 0.9957\n","epoch 7\tavg epoch loss = 0.08224\tavg epoch acc = 0.9967\n","epoch 8\tavg epoch loss = 0.07208\tavg epoch acc = 0.9973\n","epoch 9\tavg epoch loss = 0.0618\tavg epoch acc = 0.9976\n","training took 26.52 s\n","Avg test loss = 0.0393\tAvg test acc = 0.989\n","epoch 0\tavg epoch loss = 0.6931\tavg epoch acc = 0.8955\n","epoch 1\tavg epoch loss = 0.2536\tavg epoch acc = 0.9805\n","epoch 2\tavg epoch loss = 0.1879\tavg epoch acc = 0.9876\n","epoch 3\tavg epoch loss = 0.1507\tavg epoch acc = 0.9908\n","epoch 4\tavg epoch loss = 0.1271\tavg epoch acc = 0.9932\n","epoch 5\tavg epoch loss = 0.1105\tavg epoch acc = 0.9946\n","epoch 6\tavg epoch loss = 0.09211\tavg epoch acc = 0.9957\n","epoch 7\tavg epoch loss = 0.09244\tavg epoch acc = 0.996\n","epoch 8\tavg epoch loss = 0.07549\tavg epoch acc = 0.997\n","epoch 9\tavg epoch loss = 0.06693\tavg epoch acc = 0.9976\n","training took 26.59 s\n","Avg test loss = 0.0421\tAvg test acc = 0.989\n","{'lr': 0.321471781635738, 'batch_size': 32, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.7565\tavg epoch acc = 0.9006\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.5117\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09663\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09663\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09663\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09663\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09663\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09663\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09663\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09663\n","training took 48.1 s\n","Avg test loss = nan\tAvg test acc = 0.103\n","epoch 0\tavg epoch loss = 0.6847\tavg epoch acc = 0.9076\n","epoch 1\tavg epoch loss = 0.6389\tavg epoch acc = 0.9447\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.7635\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.1008\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.1008\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.1008\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.1008\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.1008\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.1008\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.1008\n","training took 48.02 s\n","Avg test loss = nan\tAvg test acc = 0.0946\n","epoch 0\tavg epoch loss = 0.9679\tavg epoch acc = 0.8527\n","epoch 1\tavg epoch loss = 1.063\tavg epoch acc = 0.7959\n","epoch 2\tavg epoch loss = 0.7676\tavg epoch acc = 0.9249\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.1827\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09875\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09875\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09875\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09875\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09875\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09875\n","training took 48.12 s\n","Avg test loss = nan\tAvg test acc = 0.0987\n","{'lr': 0.321471781635738, 'batch_size': 64, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.586\tavg epoch acc = 0.9142\n","epoch 1\tavg epoch loss = 0.2317\tavg epoch acc = 0.9808\n","epoch 2\tavg epoch loss = 0.1825\tavg epoch acc = 0.9874\n","epoch 3\tavg epoch loss = 0.1645\tavg epoch acc = 0.989\n","epoch 4\tavg epoch loss = 0.1372\tavg epoch acc = 0.9919\n","epoch 5\tavg epoch loss = 0.1306\tavg epoch acc = 0.9933\n","epoch 6\tavg epoch loss = 0.1083\tavg epoch acc = 0.995\n","epoch 7\tavg epoch loss = 0.1254\tavg epoch acc = 0.9947\n","epoch 8\tavg epoch loss = 0.1007\tavg epoch acc = 0.9959\n","epoch 9\tavg epoch loss = 0.1178\tavg epoch acc = 0.9956\n","training took 34.55 s\n","Avg test loss = 0.12\tAvg test acc = 0.974\n","epoch 0\tavg epoch loss = 0.6034\tavg epoch acc = 0.9137\n","epoch 1\tavg epoch loss = 0.262\tavg epoch acc = 0.9771\n","epoch 2\tavg epoch loss = 0.1928\tavg epoch acc = 0.9853\n","epoch 3\tavg epoch loss = 0.1538\tavg epoch acc = 0.9897\n","epoch 4\tavg epoch loss = 0.1423\tavg epoch acc = 0.9915\n","epoch 5\tavg epoch loss = 0.2044\tavg epoch acc = 0.9829\n","epoch 6\tavg epoch loss = 0.1895\tavg epoch acc = 0.9896\n","epoch 7\tavg epoch loss = 0.1804\tavg epoch acc = 0.9903\n","epoch 8\tavg epoch loss = 0.1341\tavg epoch acc = 0.9934\n","epoch 9\tavg epoch loss = 0.1224\tavg epoch acc = 0.9945\n","training took 34.54 s\n","Avg test loss = 0.0547\tAvg test acc = 0.984\n","epoch 0\tavg epoch loss = 0.5772\tavg epoch acc = 0.9162\n","epoch 1\tavg epoch loss = 0.3308\tavg epoch acc = 0.9701\n","epoch 2\tavg epoch loss = 0.2101\tavg epoch acc = 0.9835\n","epoch 3\tavg epoch loss = 0.1681\tavg epoch acc = 0.9886\n","epoch 4\tavg epoch loss = 0.1596\tavg epoch acc = 0.9905\n","epoch 5\tavg epoch loss = 0.1328\tavg epoch acc = 0.9928\n","epoch 6\tavg epoch loss = 0.1584\tavg epoch acc = 0.992\n","epoch 7\tavg epoch loss = 0.1569\tavg epoch acc = 0.9924\n","epoch 8\tavg epoch loss = 0.1171\tavg epoch acc = 0.9949\n","epoch 9\tavg epoch loss = 0.17\tavg epoch acc = 0.9923\n","training took 34.53 s\n","Avg test loss = 0.109\tAvg test acc = 0.969\n","{'lr': 0.321471781635738, 'batch_size': 96, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.6404\tavg epoch acc = 0.9\n","epoch 1\tavg epoch loss = 0.2419\tavg epoch acc = 0.9807\n","epoch 2\tavg epoch loss = 0.1783\tavg epoch acc = 0.9869\n","epoch 3\tavg epoch loss = 0.1411\tavg epoch acc = 0.9908\n","epoch 4\tavg epoch loss = 0.1186\tavg epoch acc = 0.9931\n","epoch 5\tavg epoch loss = 0.1035\tavg epoch acc = 0.9944\n","epoch 6\tavg epoch loss = 0.09779\tavg epoch acc = 0.9951\n","epoch 7\tavg epoch loss = 0.08413\tavg epoch acc = 0.9963\n","epoch 8\tavg epoch loss = 0.0759\tavg epoch acc = 0.9968\n","epoch 9\tavg epoch loss = 0.07197\tavg epoch acc = 0.9973\n","training took 30.03 s\n","Avg test loss = 0.0678\tAvg test acc = 0.981\n","epoch 0\tavg epoch loss = 0.6835\tavg epoch acc = 0.8895\n","epoch 1\tavg epoch loss = 0.2513\tavg epoch acc = 0.9776\n","epoch 2\tavg epoch loss = 0.1765\tavg epoch acc = 0.986\n","epoch 3\tavg epoch loss = 0.1429\tavg epoch acc = 0.9893\n","epoch 4\tavg epoch loss = 0.134\tavg epoch acc = 0.9903\n","epoch 5\tavg epoch loss = 0.1033\tavg epoch acc = 0.9933\n","epoch 6\tavg epoch loss = 0.09031\tavg epoch acc = 0.995\n","epoch 7\tavg epoch loss = 0.1276\tavg epoch acc = 0.9927\n","epoch 8\tavg epoch loss = 0.1242\tavg epoch acc = 0.9934\n","epoch 9\tavg epoch loss = 0.08405\tavg epoch acc = 0.9959\n","training took 30.12 s\n","Avg test loss = 0.0539\tAvg test acc = 0.986\n","epoch 0\tavg epoch loss = 0.6776\tavg epoch acc = 0.8935\n","epoch 1\tavg epoch loss = 0.244\tavg epoch acc = 0.9786\n","epoch 2\tavg epoch loss = 0.1826\tavg epoch acc = 0.9853\n","epoch 3\tavg epoch loss = 0.1643\tavg epoch acc = 0.9879\n","epoch 4\tavg epoch loss = 0.1332\tavg epoch acc = 0.9914\n","epoch 5\tavg epoch loss = 0.1163\tavg epoch acc = 0.9932\n","epoch 6\tavg epoch loss = 0.09859\tavg epoch acc = 0.9948\n","epoch 7\tavg epoch loss = 0.09152\tavg epoch acc = 0.9957\n","epoch 8\tavg epoch loss = 0.9525\tavg epoch acc = 0.7092\n","epoch 9\tavg epoch loss = 2.308\tavg epoch acc = 0.1174\n","training took 30.05 s\n","Avg test loss = 2.3\tAvg test acc = 0.104\n","{'lr': 0.321471781635738, 'batch_size': 128, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.6821\tavg epoch acc = 0.8876\n","epoch 1\tavg epoch loss = 0.2655\tavg epoch acc = 0.9776\n","epoch 2\tavg epoch loss = 0.1771\tavg epoch acc = 0.9866\n","epoch 3\tavg epoch loss = 0.1337\tavg epoch acc = 0.9908\n","epoch 4\tavg epoch loss = 0.1077\tavg epoch acc = 0.9934\n","epoch 5\tavg epoch loss = 0.08986\tavg epoch acc = 0.9947\n","epoch 6\tavg epoch loss = 0.09557\tavg epoch acc = 0.995\n","epoch 7\tavg epoch loss = 0.08043\tavg epoch acc = 0.9962\n","epoch 8\tavg epoch loss = 0.07386\tavg epoch acc = 0.9966\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.1224\n","training took 26.53 s\n","Avg test loss = nan\tAvg test acc = 0.0972\n","epoch 0\tavg epoch loss = 0.7999\tavg epoch acc = 0.8586\n","epoch 1\tavg epoch loss = 0.2773\tavg epoch acc = 0.9765\n","epoch 2\tavg epoch loss = 0.1886\tavg epoch acc = 0.9854\n","epoch 3\tavg epoch loss = 0.1533\tavg epoch acc = 0.989\n","epoch 4\tavg epoch loss = 0.1259\tavg epoch acc = 0.9919\n","epoch 5\tavg epoch loss = 0.1089\tavg epoch acc = 0.9936\n","epoch 6\tavg epoch loss = 0.09699\tavg epoch acc = 0.9947\n","epoch 7\tavg epoch loss = 0.08877\tavg epoch acc = 0.9958\n","epoch 8\tavg epoch loss = 0.0763\tavg epoch acc = 0.9964\n","epoch 9\tavg epoch loss = 0.06649\tavg epoch acc = 0.997\n","training took 26.58 s\n","Avg test loss = 0.0414\tAvg test acc = 0.988\n","epoch 0\tavg epoch loss = 0.7931\tavg epoch acc = 0.8659\n","epoch 1\tavg epoch loss = 0.2764\tavg epoch acc = 0.9765\n","epoch 2\tavg epoch loss = 0.1917\tavg epoch acc = 0.9848\n","epoch 3\tavg epoch loss = 0.1553\tavg epoch acc = 0.9884\n","epoch 4\tavg epoch loss = 0.1359\tavg epoch acc = 0.9908\n","epoch 5\tavg epoch loss = 0.1191\tavg epoch acc = 0.9927\n","epoch 6\tavg epoch loss = 0.1015\tavg epoch acc = 0.9941\n","epoch 7\tavg epoch loss = 0.09111\tavg epoch acc = 0.9952\n","epoch 8\tavg epoch loss = 0.09619\tavg epoch acc = 0.9954\n","epoch 9\tavg epoch loss = 0.09501\tavg epoch acc = 0.9956\n","training took 26.6 s\n","Avg test loss = 0.0983\tAvg test acc = 0.972\n","{'lr': 0.5011872336272722, 'batch_size': 32, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 2.305\tavg epoch acc = 0.1188\n","epoch 1\tavg epoch loss = 2.259\tavg epoch acc = 0.1938\n","epoch 2\tavg epoch loss = 2.12\tavg epoch acc = 0.3268\n","epoch 3\tavg epoch loss = 2.309\tavg epoch acc = 0.1034\n","epoch 4\tavg epoch loss = 2.305\tavg epoch acc = 0.1052\n","epoch 5\tavg epoch loss = 2.305\tavg epoch acc = 0.1053\n","epoch 6\tavg epoch loss = 2.305\tavg epoch acc = 0.1053\n","epoch 7\tavg epoch loss = 2.305\tavg epoch acc = 0.1053\n","epoch 8\tavg epoch loss = 2.305\tavg epoch acc = 0.1053\n","epoch 9\tavg epoch loss = 2.305\tavg epoch acc = 0.1053\n","training took 47.96 s\n","Avg test loss = 2.3\tAvg test acc = 0.101\n","epoch 0\tavg epoch loss = 2.196\tavg epoch acc = 0.2213\n","epoch 1\tavg epoch loss = 2.305\tavg epoch acc = 0.1079\n","epoch 2\tavg epoch loss = 2.305\tavg epoch acc = 0.1061\n","epoch 3\tavg epoch loss = 2.305\tavg epoch acc = 0.1062\n","epoch 4\tavg epoch loss = 2.305\tavg epoch acc = 0.1062\n","epoch 5\tavg epoch loss = 2.305\tavg epoch acc = 0.1062\n","epoch 6\tavg epoch loss = 2.305\tavg epoch acc = 0.1062\n","epoch 7\tavg epoch loss = 2.305\tavg epoch acc = 0.1062\n","epoch 8\tavg epoch loss = 2.305\tavg epoch acc = 0.1062\n","epoch 9\tavg epoch loss = 2.305\tavg epoch acc = 0.1062\n","training took 48.0 s\n","Avg test loss = 2.3\tAvg test acc = 0.103\n","epoch 0\tavg epoch loss = 2.306\tavg epoch acc = 0.108\n","epoch 1\tavg epoch loss = 2.286\tavg epoch acc = 0.159\n","epoch 2\tavg epoch loss = 2.28\tavg epoch acc = 0.1582\n","epoch 3\tavg epoch loss = 2.267\tavg epoch acc = 0.1749\n","epoch 4\tavg epoch loss = 2.305\tavg epoch acc = 0.1066\n","epoch 5\tavg epoch loss = 2.305\tavg epoch acc = 0.1069\n","epoch 6\tavg epoch loss = 2.305\tavg epoch acc = 0.1069\n","epoch 7\tavg epoch loss = 2.305\tavg epoch acc = 0.1069\n","epoch 8\tavg epoch loss = 2.305\tavg epoch acc = 0.1069\n","epoch 9\tavg epoch loss = 2.305\tavg epoch acc = 0.1069\n","training took 47.98 s\n","Avg test loss = 2.3\tAvg test acc = 0.102\n","{'lr': 0.5011872336272722, 'batch_size': 64, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 1.483\tavg epoch acc = 0.6688\n","epoch 1\tavg epoch loss = 2.06\tavg epoch acc = 0.3242\n","epoch 2\tavg epoch loss = 2.306\tavg epoch acc = 0.1105\n","epoch 3\tavg epoch loss = 2.304\tavg epoch acc = 0.1108\n","epoch 4\tavg epoch loss = 2.303\tavg epoch acc = 0.111\n","epoch 5\tavg epoch loss = 2.303\tavg epoch acc = 0.1118\n","epoch 6\tavg epoch loss = 2.303\tavg epoch acc = 0.1124\n","epoch 7\tavg epoch loss = 2.303\tavg epoch acc = 0.1126\n","epoch 8\tavg epoch loss = 2.303\tavg epoch acc = 0.1126\n","epoch 9\tavg epoch loss = 2.303\tavg epoch acc = 0.1126\n","training took 34.41 s\n","Avg test loss = 2.3\tAvg test acc = 0.104\n","epoch 0\tavg epoch loss = 154.6\tavg epoch acc = 0.7763\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.0978\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.0979\n","training took 34.42 s\n","Avg test loss = nan\tAvg test acc = 0.1\n","epoch 0\tavg epoch loss = 2.243\tavg epoch acc = 0.1908\n","epoch 1\tavg epoch loss = 2.304\tavg epoch acc = 0.1083\n","epoch 2\tavg epoch loss = 2.303\tavg epoch acc = 0.1096\n","epoch 3\tavg epoch loss = 2.303\tavg epoch acc = 0.1102\n","epoch 4\tavg epoch loss = 2.185\tavg epoch acc = 0.2352\n","epoch 5\tavg epoch loss = 2.304\tavg epoch acc = 0.1071\n","epoch 6\tavg epoch loss = 2.304\tavg epoch acc = 0.11\n","epoch 7\tavg epoch loss = 2.019\tavg epoch acc = 0.3239\n","epoch 8\tavg epoch loss = 1.989\tavg epoch acc = 0.3463\n","epoch 9\tavg epoch loss = 2.307\tavg epoch acc = 0.1057\n","training took 34.48 s\n","Avg test loss = 2.3\tAvg test acc = 0.113\n","{'lr': 0.5011872336272722, 'batch_size': 96, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 0.8459\tavg epoch acc = 0.8562\n","epoch 1\tavg epoch loss = 0.5463\tavg epoch acc = 0.9418\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.2828\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09978\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09978\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09978\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09978\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09978\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09978\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09978\n","training took 29.98 s\n","Avg test loss = nan\tAvg test acc = 0.0967\n","epoch 0\tavg epoch loss = 1.03\tavg epoch acc = 0.8096\n","epoch 1\tavg epoch loss = 0.4576\tavg epoch acc = 0.9555\n","epoch 2\tavg epoch loss = 10.15\tavg epoch acc = 0.1174\n","epoch 3\tavg epoch loss = 2.302\tavg epoch acc = 0.1111\n","epoch 4\tavg epoch loss = 2.302\tavg epoch acc = 0.1112\n","epoch 5\tavg epoch loss = 2.302\tavg epoch acc = 0.1112\n","epoch 6\tavg epoch loss = 2.302\tavg epoch acc = 0.1112\n","epoch 7\tavg epoch loss = 2.302\tavg epoch acc = 0.1112\n","epoch 8\tavg epoch loss = 2.302\tavg epoch acc = 0.1112\n","epoch 9\tavg epoch loss = 2.302\tavg epoch acc = 0.1112\n","training took 30.03 s\n","Avg test loss = 2.3\tAvg test acc = 0.101\n","epoch 0\tavg epoch loss = 1.139\tavg epoch acc = 0.7594\n","epoch 1\tavg epoch loss = nan\tavg epoch acc = 0.4836\n","epoch 2\tavg epoch loss = nan\tavg epoch acc = 0.09738\n","epoch 3\tavg epoch loss = nan\tavg epoch acc = 0.09738\n","epoch 4\tavg epoch loss = nan\tavg epoch acc = 0.09738\n","epoch 5\tavg epoch loss = nan\tavg epoch acc = 0.09738\n","epoch 6\tavg epoch loss = nan\tavg epoch acc = 0.09738\n","epoch 7\tavg epoch loss = nan\tavg epoch acc = 0.09738\n","epoch 8\tavg epoch loss = nan\tavg epoch acc = 0.09738\n","epoch 9\tavg epoch loss = nan\tavg epoch acc = 0.09738\n","training took 29.94 s\n","Avg test loss = nan\tAvg test acc = 0.101\n","{'lr': 0.5011872336272722, 'batch_size': 128, 'decreasing_lr': 0}\n","epoch 0\tavg epoch loss = 1.399\tavg epoch acc = 0.6599\n","epoch 1\tavg epoch loss = 0.4379\tavg epoch acc = 0.9526\n","epoch 2\tavg epoch loss = 2.047\tavg epoch acc = 0.3592\n","epoch 3\tavg epoch loss = 2.31\tavg epoch acc = 0.1094\n","epoch 4\tavg epoch loss = 2.303\tavg epoch acc = 0.1095\n","epoch 5\tavg epoch loss = 2.303\tavg epoch acc = 0.1092\n","epoch 6\tavg epoch loss = 2.303\tavg epoch acc = 0.109\n","epoch 7\tavg epoch loss = 2.303\tavg epoch acc = 0.109\n","epoch 8\tavg epoch loss = 2.303\tavg epoch acc = 0.1093\n","epoch 9\tavg epoch loss = 2.303\tavg epoch acc = 0.1097\n","training took 26.46 s\n","Avg test loss = 2.3\tAvg test acc = 0.113\n","epoch 0\tavg epoch loss = 1.173\tavg epoch acc = 0.7509\n","epoch 1\tavg epoch loss = 0.5566\tavg epoch acc = 0.9342\n","epoch 2\tavg epoch loss = 0.3191\tavg epoch acc = 0.9702\n","epoch 3\tavg epoch loss = 2.381\tavg epoch acc = 0.1215\n","epoch 4\tavg epoch loss = 2.303\tavg epoch acc = 0.1067\n","epoch 5\tavg epoch loss = 2.303\tavg epoch acc = 0.1073\n","epoch 6\tavg epoch loss = 2.303\tavg epoch acc = 0.108\n","epoch 7\tavg epoch loss = 2.303\tavg epoch acc = 0.1086\n","epoch 8\tavg epoch loss = 2.303\tavg epoch acc = 0.1093\n","epoch 9\tavg epoch loss = 2.303\tavg epoch acc = 0.1096\n","training took 26.49 s\n","Avg test loss = 2.3\tAvg test acc = 0.101\n","epoch 0\tavg epoch loss = 0.9365\tavg epoch acc = 0.827\n","epoch 1\tavg epoch loss = 0.4223\tavg epoch acc = 0.9572\n","epoch 2\tavg epoch loss = 0.4479\tavg epoch acc = 0.9544\n","epoch 3\tavg epoch loss = 0.2697\tavg epoch acc = 0.9789\n","epoch 4\tavg epoch loss = 0.255\tavg epoch acc = 0.9822\n","epoch 5\tavg epoch loss = 0.1812\tavg epoch acc = 0.9884\n","epoch 6\tavg epoch loss = 0.1469\tavg epoch acc = 0.9914\n","epoch 7\tavg epoch loss = 0.127\tavg epoch acc = 0.9935\n","epoch 8\tavg epoch loss = 0.1242\tavg epoch acc = 0.9943\n","epoch 9\tavg epoch loss = 0.1202\tavg epoch acc = 0.9944\n","training took 26.59 s\n","Avg test loss = 0.25\tAvg test acc = 0.947\n"]}],"source":["search_grid_mini  = {\n","        'lr': np.logspace(-3,-0.3,num=15),\n","        'batch_size': [32, 64, 96, 128],\n","        'decreasing_lr': [0],\n","    }\n","if prot_hyperparameter_tune:\n","    results_minibatch_prot = tune_optimizer(\n","        model=Net().to(device),\n","        optim_fun=MiniBatchOptimizer,\n","        xtrain=train_dataset.data,\n","        ytrain=train_dataset.targets,\n","        search_grid=search_grid_mini,\n","        nfolds=3,\n","        func=protected_training,\n","        **training_config\n","    )\n","\n","else:\n","    results_minibatch_prot = optimizers[MiniBatchOptimizer]"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["with open('./res/prot_res_minibatch_extensive.json', 'w', encoding ='utf8') as json_file:\n","    json.dump(results_minibatch_prot, json_file, indent=2)"]},{"cell_type":"markdown","metadata":{"id":"pretty-hotel"},"source":["## Train & Attack robust models"],"id":"pretty-hotel"},{"cell_type":"code","metadata":{"id":"modular-commander"},"source":["from adversary import protected_training"],"id":"modular-commander","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"coordinated-letters","outputId":"140d3e39-aff4-4c2b-fe46-f19b87d6183b"},"source":["# Logging data structures\n","data_robust = list()\n","data_robust_attack = list()\n","\n","# Training / attack config\n","n_try = 3\n","batch_log_interval = -1\n","epsilons = np.arange(0, 0.6, 0.05)\n","\n","for optimizer, optimizer_params in optimizers.items():\n","    print(f'--- {optimizer.__name__}')\n","    # --------- SETUP OPTIMIZER\n","    optimizer_params = optimizer_params.copy()\n","    # Instantiate data loaders with selected batch size\n","    batch_size = int(optimizer_params.pop('batch_size'))\n","    train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)\n","\n","    # --------- Train & Attack several times per optimizer\n","    for n in range(1, n_try + 1):\n","        net = Net().to(device)\n","        optimizer_instance = optimizer(net.parameters(), **optimizer_params)\n","        # --------- TRAIN MODEL\n","        loss_train, acc_train = protected_training(\n","            model=net,\n","            dataset=train_loader,\n","            optim=optimizer_instance,\n","            batch_log_interval=batch_log_interval,\n","            **training_config\n","        )\n","        # --------- TEST MODEL\n","        loss_test, acc_test = testing(\n","            model=net,\n","            dataset=test_loader,\n","            **test_config\n","        )\n","        # Log\n","        data_robust.append({\n","            'optimizer': str(optimizer_instance),\n","            'n': n,\n","            'loss_train': loss_train,\n","            'acc_train': acc_train,\n","            'loss_test': loss_test,\n","            'acc_test': acc_test\n","        })\n","\n","        # --------- ATTACK MODEL\n","        print(f'Launching attacks', end=' ')\n","        for eps in epsilons:\n","            print('', end='.')\n","            loss_attack, acc_attack = attack(\n","                model=net,\n","                test_loader=test_loader,\n","                epsilon=eps,\n","                verbose=False,\n","                **test_config\n","            )\n","            # Log\n","            data_robust_attack.append({\n","                'optimizer': str(optimizer_instance),\n","                'n': n,\n","                'epsilon': eps,\n","                'loss': loss_attack,\n","                'acc': acc_attack\n","            })\n","\n","        print()"],"id":"coordinated-letters","execution_count":null,"outputs":[]},{"cell_type":"raw","metadata":{"id":"blocked-child","tags":[]},"source":["# non-loopy train\n","robust_networks = dict()\n","data_robust = list()\n","batch_log_interval = 0\n","epsilon = 0.25\n","\n","for optimizer, optimizer_params in prot_optimizers.items():\n","    name = optimizer_names[optimizer]\n","    print(f'--- {name}')\n","    # Instantiate model\n","    net = Net().to(device)\n","    # Instantiate optimizer\n","    optimizer_params = optimizer_params.copy()\n","    batch_size = int(optimizer_params.pop('batch_size'))\n","    metric_test = optimizer_params.pop('metric_test')\n","    metric_test_std = optimizer_params.pop('metric_test_std')\n","    optimizer_instance = optimizer(net.parameters(), **optimizer_params)\n","    # Instantiate data loaders\n","    train_loader, test_loader = build_data_loaders(train_dataset, test_dataset, batch_size)\n","    # Train robust model\n","    losses, metrics = protected_training(\n","        model=net,\n","        dataset=train_loader,\n","        optim=optimizer_instance,\n","        batch_log_interval=batch_log_interval,\n","        **training_config\n","    )\n","    # Log\n","    data_robust.append({\n","        'optimizer': name, \n","        'loss': losses,\n","        'acc': metrics\n","    })\n","    # Save robust net\n","    robust_networks[optimizer] = net"],"id":"blocked-child"},{"cell_type":"raw","metadata":{"tags":[],"id":"phantom-strategy"},"source":["# NON LOOPY ATTACK ON ROBUST MODELS\n","data_robust_attack = list()\n","epsilons = np.arange(0, 0.6, 0.05)\n","\n","# Attack all models\n","for optimizer, network in robust_networks.items():\n","    name = optimizer_names[optimizer]\n","    print(f'\\n{name:<10}', end='')\n","    \n","    for eps in epsilons:\n","        print('', end='.')\n","        # Attack\n","        loss_attack, acc_attack = attack(\n","            model=network,\n","            test_loader=test_loader,\n","            epsilon=eps,\n","            verbose=False,\n","            **test_config\n","        )\n","        # Log\n","        data_robust_attack.append({\n","            'optimizer': name,\n","            'epsilon': eps,\n","            'loss': loss_attack,\n","            'acc': acc_attack\n","        })"],"id":"phantom-strategy"},{"cell_type":"markdown","metadata":{"id":"institutional-partnership"},"source":["### Training curves"],"id":"institutional-partnership"},{"cell_type":"code","metadata":{"id":"adequate-dispute","outputId":"a690d323-6724-40bb-d180-fc70cac1f889"},"source":["df_naive = pd.DataFrame(data_robust).sort_values(['optimizer', 'n'])\n","# Average training loss per epoch\n","df_naive.loss_train = df_naive.loss_train.apply(lambda s: np.mean(s, axis=1))\n","\n","colors = {'AdamOptimizer': 'r', 'MiniBatchOptimizer': 'b', 'NesterovOptimizer': 'm'}\n","for _, row in df_naive.iterrows():\n","    plt.plot(range(1, training_config['epochs'] + 1), row.loss_train, '-o', label=row.optimizer, color=colors[row.optimizer])\n","\n","plt.grid(alpha=.6)\n","plt.legend();\n","plt.yscale('log')\n","plt.xlabel('Epoch'); plt.ylabel('Training Loss')\n","plt.title('Training curves of naive models');"],"id":"adequate-dispute","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"continued-nowhere"},"source":["### Attack plots"],"id":"continued-nowhere"},{"cell_type":"code","metadata":{"id":"portable-billion","outputId":"4899cd23-2960-4eb4-9680-feeb7cc21375"},"source":["df_robust = pd.DataFrame(data_robust_attack).sort_values(['optimizer', 'epsilon'])\n","# Raw accuracy in function of attack stength\n","sns.lineplot(x='epsilon', y='acc', data=df, hue='optimizer', marker='o')\n","plt.grid(alpha=.6)\n","plt.ylabel('Accuracy');\n","plt.title('Attack on robust models');"],"id":"portable-billion","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"raising-barrel"},"source":["df_robust.T.to_json('res/log_attack_robust.json', indent=2)"],"id":"raising-barrel","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"headed-hamburg"},"source":["# Comparative analysis"],"id":"headed-hamburg"},{"cell_type":"code","metadata":{"id":"informational-toilet","outputId":"42807108-e91d-4657-8197-ccdf9a7a5360"},"source":["%%capture --no-stderr\n","# Whether to generate fancy plots for the report. Warning: this lengthens image rendering time\n","# This requires the `ipypublish` library\n","fancy_plots = True\n","\n","if fancy_plots:\n","    from ipypublish import nb_setup\n","    plt = nb_setup.setup_matplotlib()\n","    # Override with seaborn defaults\n","    sns.set(style='whitegrid')"],"id":"informational-toilet","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"artificial-quarter"},"source":["from pathlib import Path\n","\n","fig_path = Path('fig')\n","if not fig_path.exists():\n","    fig_path.mkdir()"],"id":"artificial-quarter","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"monetary-registrar"},"source":["df_naive = pd.read_json('res/log_attack_naive.json').T.drop(columns=['loss', 'n'])\n","df_robust = pd.read_json('res/log_attack_robust.json').T\n","# For some reason this has to be done after parsing from json\n","df_naive.epsilon = df_naive.epsilon.astype(float)\n","df_naive.acc = df_naive.acc.astype(float)\n","df_robust.acc = df_robust.acc.astype(float)\n","df_robust.epsilon = df_robust.epsilon.astype(float)"],"id":"monetary-registrar","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dated-consciousness","outputId":"cd8b6584-82a8-4789-f31b-7d4c2e27c001"},"source":["_, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=True)\n","\n","plt.axes(ax[0])\n","sns.lineplot(x='epsilon', y='acc', data=df_naive, hue='optimizer', marker='o', ci='sd')\n","plt.yscale('log')\n","plt.ylabel('Accuracy');\n","plt.title('Attack on naive models');\n","plt.xlabel('Attack strength $\\epsilon$')\n","\n","plt.axes(ax[1])\n","sns.lineplot(x='epsilon', y='acc', data=df_robust, hue='optimizer', marker='o', ci='sd')\n","plt.grid(alpha=.6)\n","plt.ylabel('Accuracy');\n","plt.title('Attack on robust models')\n","plt.xlabel('Attack strength $\\epsilon$')\n","plt.tight_layout();\n","sns.despine()\n","plt.savefig('fig/attacks.pdf', bbox_inches='tight')"],"id":"dated-consciousness","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"western-album","outputId":"8a594e6c-f613-4cc8-90b4-bfb994157824"},"source":["df_naive[df_naive.epsilon==0]"],"id":"western-album","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"silent-staff","outputId":"c690a802-5a76-49ef-f35c-3832d8c6ab0e"},"source":["df_robust[df_robust.epsilon == 0]"],"id":"silent-staff","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"burning-dylan","outputId":"891c9410-b68e-415f-c302-b6535f4b0427"},"source":["df_robust"],"id":"burning-dylan","execution_count":null,"outputs":[]}]}