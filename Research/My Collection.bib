@article{Goodfellow2015,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:media/storage/Semester 2/Opt. ML/Project/REPO/optml_project/Research/EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--11},
title = {{Explaining and harnessing adversarial examples}},
year = {2015}
}
@article{Carlini2018,
abstract = {We construct targeted audio adversarial examples on automatic speech recognition. Given any audio waveform, we can produce another that is over 99.9% similar, but transcribes as any phrase we choose (recognizing up to 50 characters per second of audio). We apply our white-box iterative optimization-based attack to Mozilla's implementation DeepSpeech end-to-end, and show it has a 100% success rate. The feasibility of this attack introduce a new domain to study adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1801.01944},
author = {Carlini, Nicholas and Wagner, David},
doi = {10.1109/SPW.2018.00009},
eprint = {1801.01944},
file = {:media/storage/Semester 2/Opt. ML/Project/REPO/optml_project/Research/Audio Adversarial Examples\: Targeted Attacks on Speech-to-Text.pdf:pdf},
isbn = {9780769563497},
journal = {Proceedings - 2018 IEEE Symposium on Security and Privacy Workshops, SPW 2018},
keywords = {Adversarial example,Neural network},
pages = {1--7},
title = {{Audio adversarial examples: Targeted attacks on speech-to-text}},
year = {2018}
}
@article{Carlini2019,
abstract = {Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect. We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.},
archivePrefix = {arXiv},
arxivId = {1902.06705},
author = {Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
eprint = {1902.06705},
file = {:media/storage/Semester 2/Opt. ML/Project/REPO/optml_project/Research/On Evaluating Adversarial Robustness.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--24},
title = {{On Evaluating Adversarial Robustness}},
year = {2019}
}
@article{Andriushchenko2020,
abstract = {A recent line of work focused on making adversarial training computationally efficient for deep learning models. In particular, Wong et al. [46] showed that `∞-adversarial training with fast gradient sign method (FGSM) can fail due to a phenomenon called catastrophic overfitting, when the model quickly loses its robustness over a single epoch of training. We show that adding a random step to FGSM, as proposed in [46], does not prevent catastrophic overfitting, and that randomness is not important per se — its main role being simply to reduce the magnitude of the perturbation. Moreover, we show that catastrophic overfitting is not inherent to deep and overparametrized networks, but can occur in a single-layer convolutional network with a few filters. In an extreme case, even a single filter can make the network highly non-linear locally, which is the main reason why FGSM training fails. Based on this observation, we propose a new regularization method, GradAlign, that prevents catastrophic overfitting by explicitly maximizing the gradient alignment inside the perturbation set and improves the quality of the FGSM solution. As a result, GradAlign allows to successfully apply FGSM training also for larger `∞-perturbations and reduce the gap to multi-step adversarial training. The code of our experiments is available at https://github.com/tml-epfl/understanding-fast-adv-training.},
archivePrefix = {arXiv},
arxivId = {2007.02617},
author = {Andriushchenko, Maksym and Flammarion, Nicolas},
eprint = {2007.02617},
file = {:media/storage/Semester 2/Opt. ML/Project/REPO/optml_project/Research/Understanding and Improving Fast Adversarial Training.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {c},
title = {{Understanding and Improving Fast Adversarial Training}},
year = {2020}
}
