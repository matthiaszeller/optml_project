from torch._C import StringType
import torchvision
import numpy as np
from time import time
from typing import Optional, Callable
from typing import Iterable, Callable

import torch
from torch.functional import Tensor
from torch.nn import CrossEntropyLoss, Module
from torch.optim import Optimizer

from data_utils import get_mnist, build_data_loaders
from net import Net
from training import testing
from training import training, accuracy
import matplotlib.pyplot as plt


## Taken from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html 
# and Lab 10 â€“ Adversarial Robustness(https://colab.research.google.com/drive/1w697nylLw72aFcBEKu7j3yCm6RdpzOi6#scrollTo=eoE7_FDHHkat)

def fgsm(image: Tensor, grad_torch: Tensor, epsilon: float):
    """
    Creates an adverserial version of an inputted image
    """
    # Based on the grad generated by pytorch, get the sign
    grad_sign = grad_torch.sign()
    # Create an adverserial example, by perturbing the image in the direction of the gradient 
    adverserial_image = image + epsilon*grad_sign
    adverserial_image = torch.clamp(adverserial_image, 0, 1)
    # Sent back an adverserial image
    return adverserial_image



def attack(model: Module, loss_fun: Module, test_loader: Iterable, epsilon: float, device):
  """
  Attack a trained neural net 
  """
  metrics = []
  losses = []
  # Loop over test set with our dataloaders
  for _, (x, y)in enumerate(test_loader, 1):
    x, y = x.to(device), y.to(device)

    x.requires_grad = True

    # Forward pass
    yhat = model(x)
    loss = loss_fun(yhat, y)

    # Zero all existing gradients
    model.zero_grad()
    loss.backward()

    # Generate an adverserial version of the test data
    x_adverserial = fgsm(x, x.grad, epsilon)

    # Re-classify the perturbed batch
    yhat_adv = model(x_adverserial)
    adversarial_accuracy = accuracy(yhat_adv, y)

    metrics.append(adversarial_accuracy)
    losses.append(loss.item())

  # Assuming unchanged Batch
  average_accuracy = sum(metrics) / len(metrics)  

  print("Epsilon: {:.2f}\tTest Accuracy = {:.3f}".format(epsilon, average_accuracy))

  return losses, average_accuracy


def protect(model: Module, optim: Optimizer,loss_fun: Module, train_loader: Iterable, 
            test_loader: Iterable, device, epochs: int = 10, epsilon: float = 0.25):
    """
    Protects a model with a chosen optimiser against FGSM.
    """    
    t = time()
    for epoch in range(epochs):
       
        model.train()  # Train an epoch
        for _, (x, y) in enumerate(train_loader, 1):
            x, y = x.to(device), y.to(device)

            # Forward pass for adversarial perturbations
            x.requires_grad = True
            yhat = model(x)

            loss = loss_fun(yhat, y)
            model.zero_grad()
            loss.backward()
            x_adverserial = fgsm(x, x.grad, epsilon)
            
            # Evaluate the network (forward pass)
            yhat_adv = model(x_adverserial)
            loss = loss_fun(yhat_adv, y)
            
            # Compute the gradient
            optim.zero_grad()
            loss.backward()

            # Update the parameters of the model with a gradient step
            # Due to how we specified our Optimizers, this is generic
            optim.step()

        # Test the quality on the test set
        model.eval()
        metrics = []
        for _, (x, y) in enumerate(test_loader, 1):
            x, y = x.to(device), y.to(device)

            # Evaluate the network (forward pass)
            yhat_adv = model(x)
            metrics.append(accuracy(yhat_adv, y))
        
        print("Epoch {:.2f} | Test accuracy: {:.5f}".format(epoch, sum(metrics)/len(metrics)))
    t = time() - t
    print(f'training took {t:.4} s')
    return model