from time import time
from typing import Iterable, Callable
import numpy as np
import torch
from torch.functional import Tensor
from torch.nn import Module
from torch.optim import Optimizer

# TODO remove
from training import accuracy


## Source: https://adversarial-ml-tutorial.org/adversarial_examples/

def fgsm_alt(model, loss_fun, x, y, epsilon):
    """ Construct FGSM adversarial examples on the examples X"""
    delta = torch.zeros_like(x, requires_grad=True)
    loss = loss_fun((model(x + delta), y))
    loss.backward()
    return epsilon * delta.grad.detach().sign()

def fgsm_attack(model: Module, loss_fun: Module, metric_fun: Callable, test_loader: Iterable, epsilon: float, device):
    # TODO add accuracy (and remove import)
    """
    Attack a trained neural net with alternate FGSM
    """
    metrics = []
    losses = []
    # Loop over test set with our dataloaders
    for x, y in test_loader:
        x, y = x.to(device), y.to(device)

        # Generate an adverserial version of the test data
        x_adverserial = x+fgsm_alt(model, x, y, epsilon)

        # Re-classify the perturbed batch
        yhat_adv = model(x_adverserial)

        adversarial_accuracy = metric_fun(yhat_adv, y)
        loss = loss_fun(yhat_adv, y)
        
        metrics.append(adversarial_accuracy)
        losses.append(loss.item())
    # Assuming unchanged Batch
    average_accuracy = sum(metrics) / len(metrics)

    print("Epsilon: {:.2f}\tTest Accuracy = {:.3f}".format(epsilon, average_accuracy))

    return losses, average_accuracy

# acc, ex = fgsm_attack(robust_net, criterion, accuracy, prot_test_loader, eps, device=device)

def projected_gd(model, loss_fun, x, y, epsilon, alpha, num_iter):
    """ Construct FGSM adversarial examples on the examples X"""
    delta = torch.zeros_like(x, requires_grad=True)
    for t in range(num_iter):
        loss = loss_fun(model(x + delta), y)
        loss.backward()
        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)
        delta.grad.zero_()
    return delta.detach()

def projected_attack(model: Module, loss_fun: Module, metric_fun: Callable, test_loader: Iterable, epsilon=0.1, alpha=1e-2, num_iter=40, device):
    # TODO add accuracy (and remove import)
    """
    Attack a trained neural net with projected GD
    """
    metrics = []
    losses = []
    # Loop over test set with our dataloaders
    for x, y in test_loader:
        x, y = x.to(device), y.to(device)

        # Generate an adverserial version of the test data
        x_adverserial = x+projected_gd(model, x, y, epsilon, alpha, num_iter)

        # Re-classify the perturbed batch
        yhat_adv = model(x_adverserial)

        adversarial_accuracy = metric_fun(yhat_adv, y)
        loss = loss_fun(yhat_adv, y)
        
        metrics.append(adversarial_accuracy)
        losses.append(loss.item())
    # Assuming unchanged Batch
    average_accuracy = sum(metrics) / len(metrics)

    print("Epsilon: {:.2f}\tTest Accuracy = {:.3f}".format(epsilon, average_accuracy))

    return losses, average_accuracy

#print("Net Naive with PGD:", projected_attack(robust_net, criterion, accuracy, prot_test_loader, eps, alpha, num_iter device=device)


## Taken from https://pytorch.org/tutorials/beginner/fgsm_tutorial.html 
# and Lab 10 â€“ Adversarial Robustness(https://colab.research.google.com/drive/1w697nylLw72aFcBEKu7j3yCm6RdpzOi6#scrollTo=eoE7_FDHHkat)


def fgsm(image: Tensor, grad_torch: Tensor, epsilon: float):
    """
    Creates an adverserial version of an inputted image
    """
    # Based on the grad generated by pytorch, get the sign
    grad_sign = grad_torch.sign()
    # Create an adverserial example, by perturbing the image in the direction of the gradient 
    adverserial_image = image + epsilon * grad_sign
    adverserial_image = torch.clamp(adverserial_image, 0, 1)
    # Sent back an adverserial image
    return adverserial_image


def attack(model: Module, loss_fun: Module, metric_fun: Callable, test_loader: Iterable, epsilon: float, device):
    # TODO add accuracy (and remove import)
    """
    Attack a trained neural net
    """
    metrics = []
    losses = []
    # Loop over test set with our dataloaders
    for _, (x, y) in enumerate(test_loader, 1):
        x, y = x.to(device), y.to(device)

        x.requires_grad = True

        # Forward pass
        yhat = model(x)
        loss = loss_fun(yhat, y)

        # Zero all existing gradients
        model.zero_grad()
        loss.backward()

        # Generate an adverserial version of the test data
        x_adverserial = fgsm(x, x.grad, epsilon)

        # Re-classify the perturbed batch
        yhat_adv = model(x_adverserial)
        adversarial_accuracy = metric_fun(yhat_adv, y)

        metrics.append(adversarial_accuracy)
        losses.append(loss.item())

    # Assuming unchanged Batch
    average_accuracy = sum(metrics) / len(metrics)

    print("Epsilon: {:.2f}\tTest Accuracy = {:.3f}".format(epsilon, average_accuracy))

    return losses, average_accuracy


def protected_training(model: Module, dataset: Iterable, optim: Optimizer, loss_fun: Module, metric_fun: Callable = None,
            epochs: int = 10, device=None, epsilon: float = 0.25):
    """
    Protects a model with a chosen optimiser against FGSM.
    """
    losses_epoch = []
    metrics_epoch = []
    t = time()

    for epoch in range(epochs):
        losses = []
        metrics = []
        model.train()  # Train an epoch
        for _, (x, y) in enumerate(dataset, 1):
            x, y = x.to(device), y.to(device)

            # Forward pass for adversarial perturbations
            x.requires_grad = True
            yhat = model(x)

            loss = loss_fun(yhat, y)
            model.zero_grad()
            loss.backward()
            x_adverserial = fgsm(x, x.grad, epsilon)

            # Evaluate the network (forward pass)
            yhat_adv = model(x_adverserial)
            loss = loss_fun(yhat_adv, y)
            metric = metric_fun(yhat, y)
            # Compute the gradient
            optim.zero_grad()
            loss.backward()

            # Update the parameters of the model with a gradient step
            # Due to how we specified our Optimizers, this is generic
            optim.step()

            metrics.append(metric)
            losses.append(loss.item())

        losses_epoch.append(losses)
        metrics_epoch.append(metrics)
        print_metric = '' if metric_fun is None else f'\tavg epoch acc = {np.mean(metrics):.4}'
        print(f'Epoch {epoch}\tavg epoch Loss = {np.mean(losses):.4}{print_metric}')
    t = time() - t
    print(f'training took {t:.4} s')
    if metric_fun is None:
        return losses_epoch
    return losses_epoch, metrics_epoch

